{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f0af45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6161d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"from concurrent.futures import ThreadPoolExecutor\\nfrom typing import (\\n    Dict,\\n    List,\\n    Set,\\n    Union,\\n    Any,\\n    Optional,\\n    ClassVar,\\n    AsyncGenerator,\\n    Literal,\\n)\\nfrom abc import ABC, abstractmethod\\nimport regex as re\\nfrom transformers import PreTrainedModel, PreTrainedTokenizer\\nfrom tiktoken import Encoding\\nimport numpy as np\\nfrom dataclasses import dataclass, field\\nimport warnings\\nimport asyncio\\nimport aiohttp\\nimport torch\\nfrom enum import Enum\\nfrom itertools import chain\\n\\nfrom uuid import UUID, uuid4\\n\\nTokenIds = List[int]\\nTokens = Dict[int, str]\\nTokenDistribution = Dict[str, int]\\nSelectedTokens = Set[int]\\nTokenConstraint = Union[None, SelectedTokens, str]\\n\\n\\ndef transformers_tokens(tokenizer: PreTrainedTokenizer) -> Tokens:\\n    tokens = {\\n        token_id: tokenizer.decode(token_id)\\n        for _, token_id in tokenizer.get_vocab().items()\\n    }\\n    return tokens\\n\\n\\ndef openai_tokens(tokenizer: Encoding) -> Tokens:\\n    vocab_len = len(tokenizer.token_byte_values())\\n    tokens = {i: tokenizer.decode([i]) for i in range(vocab_len - 1)}\\n    for i in range(vocab_len, tokenizer.max_token_value):\\n        tokens[i] = f\\\"<|special_{i}|>\\\"\\n    return tokens\";\n",
       "                var nbb_formatted_code = \"from concurrent.futures import ThreadPoolExecutor\\nfrom typing import (\\n    Dict,\\n    List,\\n    Set,\\n    Union,\\n    Any,\\n    Optional,\\n    ClassVar,\\n    AsyncGenerator,\\n    Literal,\\n)\\nfrom abc import ABC, abstractmethod\\nimport regex as re\\nfrom transformers import PreTrainedModel, PreTrainedTokenizer\\nfrom tiktoken import Encoding\\nimport numpy as np\\nfrom dataclasses import dataclass, field\\nimport warnings\\nimport asyncio\\nimport aiohttp\\nimport torch\\nfrom enum import Enum\\nfrom itertools import chain\\n\\nfrom uuid import UUID, uuid4\\n\\nTokenIds = List[int]\\nTokens = Dict[int, str]\\nTokenDistribution = Dict[str, int]\\nSelectedTokens = Set[int]\\nTokenConstraint = Union[None, SelectedTokens, str]\\n\\n\\ndef transformers_tokens(tokenizer: PreTrainedTokenizer) -> Tokens:\\n    tokens = {\\n        token_id: tokenizer.decode(token_id)\\n        for _, token_id in tokenizer.get_vocab().items()\\n    }\\n    return tokens\\n\\n\\ndef openai_tokens(tokenizer: Encoding) -> Tokens:\\n    vocab_len = len(tokenizer.token_byte_values())\\n    tokens = {i: tokenizer.decode([i]) for i in range(vocab_len - 1)}\\n    for i in range(vocab_len, tokenizer.max_token_value):\\n        tokens[i] = f\\\"<|special_{i}|>\\\"\\n    return tokens\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import (\n",
    "    Dict,\n",
    "    List,\n",
    "    Set,\n",
    "    Union,\n",
    "    Any,\n",
    "    Optional,\n",
    "    ClassVar,\n",
    "    AsyncGenerator,\n",
    "    Literal,\n",
    ")\n",
    "from abc import ABC, abstractmethod\n",
    "import regex as re\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "from tiktoken import Encoding\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "import warnings\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import torch\n",
    "from enum import Enum\n",
    "from itertools import chain\n",
    "import regex as re\n",
    "from typing import List, Dict\n",
    "from uuid import UUID, uuid4\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "TokenIds = List[int]\n",
    "Tokens = Dict[int, str]\n",
    "TokenDistribution = Dict[str, int]\n",
    "SelectedTokens = Set[int]\n",
    "TokenConstraint = Union[None, SelectedTokens, str]\n",
    "\n",
    "\n",
    "def transformers_tokens(tokenizer: PreTrainedTokenizer) -> Tokens:\n",
    "    tokens = {\n",
    "        token_id: tokenizer.decode(token_id)\n",
    "        for _, token_id in tokenizer.get_vocab().items()\n",
    "    }\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def openai_tokens(tokenizer: Encoding) -> Tokens:\n",
    "    vocab_len = len(tokenizer.token_byte_values())\n",
    "    tokens = {i: tokenizer.decode([i]) for i in range(vocab_len - 1)}\n",
    "    for i in range(vocab_len, tokenizer.max_token_value):\n",
    "        tokens[i] = f\"<|special_{i}|>\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "681ed61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex==2023.5.5\r\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 32;\n",
       "                var nbb_unformatted_code = \"!pip freeze | grep reg\";\n",
       "                var nbb_formatted_code = \"!pip freeze | grep reg\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip freeze | grep reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a8b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiktoken==0.4.0\n",
    "transformers==4.30.2\n",
    "regex==2023.5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e57a424e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"class DecodingStrategy(str, Enum):\\n    GREEDY = \\\"GREEDY\\\"\\n    SAMPLE = \\\"SAMPLE\\\"\\n\\n\\n@dataclass\\nclass Decoder:\\n    temperature: float = 0.7\\n    top_p: float = 0.95\\n    strategy: DecodingStrategy = DecodingStrategy.GREEDY\";\n",
       "                var nbb_formatted_code = \"class DecodingStrategy(str, Enum):\\n    GREEDY = \\\"GREEDY\\\"\\n    SAMPLE = \\\"SAMPLE\\\"\\n\\n\\n@dataclass\\nclass Decoder:\\n    temperature: float = 0.7\\n    top_p: float = 0.95\\n    strategy: DecodingStrategy = DecodingStrategy.GREEDY\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class DecodingStrategy(str, Enum):\n",
    "    GREEDY = \"GREEDY\"\n",
    "    SAMPLE = \"SAMPLE\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Decoder:\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.95\n",
    "    strategy: DecodingStrategy = DecodingStrategy.GREEDY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd6e4b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"class Model(ABC):\\n    tokens: Tokens\\n    supported_decodings: ClassVar[Set[DecodingStrategy]]\\n    max_total_tokens: int = 512\\n\\n    @abstractmethod\\n    async def generate(\\n        self,\\n        text: str,\\n        max_tokens: int = 1,\\n        selected_tokens: Optional[Set[int]] = None,\\n        decoder: Optional[Decoder] = None,\\n        timeout: float = 10.0,\\n    ) -> AsyncGenerator[str, None]:\\n        \\\"\\\"\\\"\\n        Generate text using the Huggingface model.\\n\\n        Args:\\n            text: The text to generate from.\\n            max_length: The maximum length of the generated text.\\n            selected_tokens: A set of tokens that should be excluded from the generated text.\\n            decoder: A parameterized description of how to select tokens from the distribution\\n            timeout: The timeout for the generation process.\\n\\n        Returns:\\n            An iterator of generated text.\\n        \\\"\\\"\\\"\\n\\n    async def sample(\\n        self,\\n        text: str,\\n        selected_tokens: Optional[SelectedTokens] = None,\\n        decoder: Optional[Decoder] = None,\\n        timeout: float = 10.0,\\n    ) -> str:\\n        \\\"\\\"\\\"Sample from the language model given the input text and the selected tokens to constrain the sampling.\\n\\n        Args:\\n            text (str): The input text to the language model.\\n            selected_tokens (Optional[Set[int]]): The set of token ids to constrain the sampling. Defaults to None.\\n\\n        Returns:\\n            str: The generated text from the language model.\\n        \\\"\\\"\\\"\\n        return await anext(\\n            self.generate(\\n                text=text,\\n                max_tokens=1,\\n                selected_tokens=selected_tokens,\\n                decoder=decoder,\\n                timeout=timeout,\\n            )\\n        )\\n\\n    @abstractmethod\\n    def encode(self, text: str) -> TokenIds:\\n        \\\"\\\"\\\"Encode the input text as token ids.\\n\\n        Args:\\n            text (str): The input text to encode.\\n\\n        Returns:\\n            TokenIds: The encoded token ids.\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def decode(self, ids: TokenIds) -> str:\\n        \\\"\\\"\\\"Decode the token ids into text.\\n\\n        Args:\\n            ids (TokenIds): The token ids to decode.\\n\\n        Returns:\\n            str: The decoded text.\\n        \\\"\\\"\\\"\\n\\n    @property\\n    def vocab_size(self) -> int:\\n        \\\"\\\"\\\"Get the vocabulary size of the language model.\\\"\\\"\\\"\\n        return len(self.tokens)\\n\\n    @property\\n    @abstractmethod\\n    def eos_token_id(self) -> int:\\n        \\\"\\\"\\\"Get the token id of the end of sequence (eos) token.\\\"\\\"\\\"\\n\\n    @property\\n    @abstractmethod\\n    def bos_token_id(self) -> int:\\n        \\\"\\\"\\\"Get the token id of the beginning of sequence (bos) token.\\\"\\\"\\\"\";\n",
       "                var nbb_formatted_code = \"class Model(ABC):\\n    tokens: Tokens\\n    supported_decodings: ClassVar[Set[DecodingStrategy]]\\n    max_total_tokens: int = 512\\n\\n    @abstractmethod\\n    async def generate(\\n        self,\\n        text: str,\\n        max_tokens: int = 1,\\n        selected_tokens: Optional[Set[int]] = None,\\n        decoder: Optional[Decoder] = None,\\n        timeout: float = 10.0,\\n    ) -> AsyncGenerator[str, None]:\\n        \\\"\\\"\\\"\\n        Generate text using the Huggingface model.\\n\\n        Args:\\n            text: The text to generate from.\\n            max_length: The maximum length of the generated text.\\n            selected_tokens: A set of tokens that should be excluded from the generated text.\\n            decoder: A parameterized description of how to select tokens from the distribution\\n            timeout: The timeout for the generation process.\\n\\n        Returns:\\n            An iterator of generated text.\\n        \\\"\\\"\\\"\\n\\n    async def sample(\\n        self,\\n        text: str,\\n        selected_tokens: Optional[SelectedTokens] = None,\\n        decoder: Optional[Decoder] = None,\\n        timeout: float = 10.0,\\n    ) -> str:\\n        \\\"\\\"\\\"Sample from the language model given the input text and the selected tokens to constrain the sampling.\\n\\n        Args:\\n            text (str): The input text to the language model.\\n            selected_tokens (Optional[Set[int]]): The set of token ids to constrain the sampling. Defaults to None.\\n\\n        Returns:\\n            str: The generated text from the language model.\\n        \\\"\\\"\\\"\\n        return await anext(\\n            self.generate(\\n                text=text,\\n                max_tokens=1,\\n                selected_tokens=selected_tokens,\\n                decoder=decoder,\\n                timeout=timeout,\\n            )\\n        )\\n\\n    @abstractmethod\\n    def encode(self, text: str) -> TokenIds:\\n        \\\"\\\"\\\"Encode the input text as token ids.\\n\\n        Args:\\n            text (str): The input text to encode.\\n\\n        Returns:\\n            TokenIds: The encoded token ids.\\n        \\\"\\\"\\\"\\n\\n    @abstractmethod\\n    def decode(self, ids: TokenIds) -> str:\\n        \\\"\\\"\\\"Decode the token ids into text.\\n\\n        Args:\\n            ids (TokenIds): The token ids to decode.\\n\\n        Returns:\\n            str: The decoded text.\\n        \\\"\\\"\\\"\\n\\n    @property\\n    def vocab_size(self) -> int:\\n        \\\"\\\"\\\"Get the vocabulary size of the language model.\\\"\\\"\\\"\\n        return len(self.tokens)\\n\\n    @property\\n    @abstractmethod\\n    def eos_token_id(self) -> int:\\n        \\\"\\\"\\\"Get the token id of the end of sequence (eos) token.\\\"\\\"\\\"\\n\\n    @property\\n    @abstractmethod\\n    def bos_token_id(self) -> int:\\n        \\\"\\\"\\\"Get the token id of the beginning of sequence (bos) token.\\\"\\\"\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Model(ABC):\n",
    "    tokens: Tokens\n",
    "    supported_decodings: ClassVar[Set[DecodingStrategy]]\n",
    "    max_total_tokens: int = 512\n",
    "\n",
    "    @abstractmethod\n",
    "    async def generate(\n",
    "        self,\n",
    "        text: str,\n",
    "        max_tokens: int = 1,\n",
    "        selected_tokens: Optional[Set[int]] = None,\n",
    "        decoder: Optional[Decoder] = None,\n",
    "        timeout: float = 10.0,\n",
    "    ) -> AsyncGenerator[str, None]:\n",
    "        \"\"\"\n",
    "        Generate text using the Huggingface model.\n",
    "\n",
    "        Args:\n",
    "            text: The text to generate from.\n",
    "            max_length: The maximum length of the generated text.\n",
    "            selected_tokens: A set of tokens that should be excluded from the generated text.\n",
    "            decoder: A parameterized description of how to select tokens from the distribution\n",
    "            timeout: The timeout for the generation process.\n",
    "\n",
    "        Returns:\n",
    "            An iterator of generated text.\n",
    "        \"\"\"\n",
    "\n",
    "    async def sample(\n",
    "        self,\n",
    "        text: str,\n",
    "        selected_tokens: Optional[SelectedTokens] = None,\n",
    "        decoder: Optional[Decoder] = None,\n",
    "        timeout: float = 10.0,\n",
    "    ) -> str:\n",
    "        \"\"\"Sample from the language model given the input text and the selected tokens to constrain the sampling.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to the language model.\n",
    "            selected_tokens (Optional[Set[int]]): The set of token ids to constrain the sampling. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated text from the language model.\n",
    "        \"\"\"\n",
    "        return await anext(\n",
    "            self.generate(\n",
    "                text=text,\n",
    "                max_tokens=1,\n",
    "                selected_tokens=selected_tokens,\n",
    "                decoder=decoder,\n",
    "                timeout=timeout,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def encode(self, text: str) -> TokenIds:\n",
    "        \"\"\"Encode the input text as token ids.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to encode.\n",
    "\n",
    "        Returns:\n",
    "            TokenIds: The encoded token ids.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def decode(self, ids: TokenIds) -> str:\n",
    "        \"\"\"Decode the token ids into text.\n",
    "\n",
    "        Args:\n",
    "            ids (TokenIds): The token ids to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded text.\n",
    "        \"\"\"\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"Get the vocabulary size of the language model.\"\"\"\n",
    "        return len(self.tokens)\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def eos_token_id(self) -> int:\n",
    "        \"\"\"Get the token id of the end of sequence (eos) token.\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def bos_token_id(self) -> int:\n",
    "        \"\"\"Get the token id of the beginning of sequence (bos) token.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5692dbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"@dataclass\\nclass Huggingface(Model):\\n    model_name: Optional[str] = None\\n    model: Optional[PreTrainedModel] = None\\n    tokenizer: Optional[PreTrainedTokenizer] = None\\n    chunk_size: int = 64\\n    supported_decodings: Set[DecodingStrategy] = frozenset(\\n        (\\n            DecodingStrategy.GREEDY,\\n            DecodingStrategy.SAMPLE,\\n        )\\n    )\\n\\n    def __post_init__(self):\\n        if self.model_name is None and self.model is None and self.tokenizer is None:\\n            raise ValueError(\\n                \\\"must specify either `model_name` or both `model` and `tokenizer`\\\"\\n            )\\n        if (self.model is not None and self.tokenizer is None) or (\\n            self.model is None and self.tokenizer is not None\\n        ):\\n            raise ValueError(\\n                \\\"must specify either `model_name` or both `model` and `tokenizer`\\\"\\n            )\\n        self.model = self.model or AutoModelForCausalLM.from_pretrained(self.model_name)\\n        self.tokenizer = self.tokenizer or AutoTokenizer.from_pretrained(\\n            self.model_name\\n        )\\n        self.tokens = transformers_tokens(self.tokenizer)\\n        self._completion_buffer = {}\\n        if self.chunk_size < 1:\\n            raise ValueError(f\\\"`chunksize` must be positive, got {self.chunksize}.\\\")\\n\\n    def encode(self, text: str) -> TokenIds:\\n        return self.tokenizer.encode(text)\\n\\n    def decode(self, ids: TokenIds) -> str:\\n        return self.tokenizer.decode(ids)\\n\\n    @property\\n    def eos_token_id(self) -> int:\\n        return self.tokenizer.eos_token_id\\n\\n    @property\\n    def bos_token_id(self) -> int:\\n        return self.tokenizer.bos_token_id\\n\\n    def _logit_processor(self, selected_tokens: Optional[SelectedTokens] = None):\\n        logits_processor = []\\n        if selected_tokens is not None:\\n\\n            def _logits_processor(input_ids, scores):\\n                mask = np.ones_like(scores) * -1e10\\n                for token_id in selected_tokens:\\n                    mask[:, token_id] = 0\\n                scores = scores + mask\\n                return scores\\n\\n            logits_processor.append(_logits_processor)\\n        return logits_processor\\n\\n    async def generate(\\n        self,\\n        text: str,\\n        max_tokens: int = 1,\\n        selected_tokens: Optional[SelectedTokens] = None,\\n        decoder: Optional[Decoder] = None,\\n        timeout: float = 10.0,\\n    ) -> AsyncGenerator[str, None]:\\n        decoder = decoder or Decoder()\\n        if decoder.strategy not in self.supported_decodings:\\n            raise ValueError(\\n                f\\\"Unsupported decoding strategy for Huggingface model `{decoder.strategy}`.\\\"\\n            )\\n        temperature = decoder.temperature\\n        top_p = decoder.top_p\\n        addtl = {}\\n\\n        if decoder.strategy == DecodingStrategy.SAMPLE:\\n            addtl[\\\"do_sample\\\"] = True\\n\\n        gen_kwargs = dict(temperature=temperature, top_p=top_p, **addtl)\\n\\n        n_gen = 0\\n        prompt_token_ids = self.tokenizer.encode(text)\\n        while n_gen < max_tokens:\\n            max_new_tokens = min(self.chunk_size, max_tokens - n_gen)\\n            output = await asyncio.to_thread(\\n                self.model.generate,\\n                input_ids=torch.tensor(prompt_token_ids)\\n                .unsqueeze(0)\\n                .to(self.model.device),\\n                max_new_tokens=max_new_tokens,\\n                logits_processor=self._logit_processor(selected_tokens),\\n                pad_token_id=self.tokenizer.eos_token_id,\\n                **gen_kwargs,\\n            )\\n            new_token_ids = output[0, len(prompt_token_ids) :].detach().cpu().tolist()\\n            prompt_token_ids += new_token_ids\\n            tok_str = self.tokenizer.decode(new_token_ids, skip_special_tokens=True)\\n            text += tok_str\\n            n_gen += max_new_tokens\\n            yield tok_str\";\n",
       "                var nbb_formatted_code = \"@dataclass\\nclass Huggingface(Model):\\n    model_name: Optional[str] = None\\n    model: Optional[PreTrainedModel] = None\\n    tokenizer: Optional[PreTrainedTokenizer] = None\\n    chunk_size: int = 64\\n    supported_decodings: Set[DecodingStrategy] = frozenset(\\n        (\\n            DecodingStrategy.GREEDY,\\n            DecodingStrategy.SAMPLE,\\n        )\\n    )\\n\\n    def __post_init__(self):\\n        if self.model_name is None and self.model is None and self.tokenizer is None:\\n            raise ValueError(\\n                \\\"must specify either `model_name` or both `model` and `tokenizer`\\\"\\n            )\\n        if (self.model is not None and self.tokenizer is None) or (\\n            self.model is None and self.tokenizer is not None\\n        ):\\n            raise ValueError(\\n                \\\"must specify either `model_name` or both `model` and `tokenizer`\\\"\\n            )\\n        self.model = self.model or AutoModelForCausalLM.from_pretrained(self.model_name)\\n        self.tokenizer = self.tokenizer or AutoTokenizer.from_pretrained(\\n            self.model_name\\n        )\\n        self.tokens = transformers_tokens(self.tokenizer)\\n        self._completion_buffer = {}\\n        if self.chunk_size < 1:\\n            raise ValueError(f\\\"`chunksize` must be positive, got {self.chunksize}.\\\")\\n\\n    def encode(self, text: str) -> TokenIds:\\n        return self.tokenizer.encode(text)\\n\\n    def decode(self, ids: TokenIds) -> str:\\n        return self.tokenizer.decode(ids)\\n\\n    @property\\n    def eos_token_id(self) -> int:\\n        return self.tokenizer.eos_token_id\\n\\n    @property\\n    def bos_token_id(self) -> int:\\n        return self.tokenizer.bos_token_id\\n\\n    def _logit_processor(self, selected_tokens: Optional[SelectedTokens] = None):\\n        logits_processor = []\\n        if selected_tokens is not None:\\n\\n            def _logits_processor(input_ids, scores):\\n                mask = np.ones_like(scores) * -1e10\\n                for token_id in selected_tokens:\\n                    mask[:, token_id] = 0\\n                scores = scores + mask\\n                return scores\\n\\n            logits_processor.append(_logits_processor)\\n        return logits_processor\\n\\n    async def generate(\\n        self,\\n        text: str,\\n        max_tokens: int = 1,\\n        selected_tokens: Optional[SelectedTokens] = None,\\n        decoder: Optional[Decoder] = None,\\n        timeout: float = 10.0,\\n    ) -> AsyncGenerator[str, None]:\\n        decoder = decoder or Decoder()\\n        if decoder.strategy not in self.supported_decodings:\\n            raise ValueError(\\n                f\\\"Unsupported decoding strategy for Huggingface model `{decoder.strategy}`.\\\"\\n            )\\n        temperature = decoder.temperature\\n        top_p = decoder.top_p\\n        addtl = {}\\n\\n        if decoder.strategy == DecodingStrategy.SAMPLE:\\n            addtl[\\\"do_sample\\\"] = True\\n\\n        gen_kwargs = dict(temperature=temperature, top_p=top_p, **addtl)\\n\\n        n_gen = 0\\n        prompt_token_ids = self.tokenizer.encode(text)\\n        while n_gen < max_tokens:\\n            max_new_tokens = min(self.chunk_size, max_tokens - n_gen)\\n            output = await asyncio.to_thread(\\n                self.model.generate,\\n                input_ids=torch.tensor(prompt_token_ids)\\n                .unsqueeze(0)\\n                .to(self.model.device),\\n                max_new_tokens=max_new_tokens,\\n                logits_processor=self._logit_processor(selected_tokens),\\n                pad_token_id=self.tokenizer.eos_token_id,\\n                **gen_kwargs,\\n            )\\n            new_token_ids = output[0, len(prompt_token_ids) :].detach().cpu().tolist()\\n            prompt_token_ids += new_token_ids\\n            tok_str = self.tokenizer.decode(new_token_ids, skip_special_tokens=True)\\n            text += tok_str\\n            n_gen += max_new_tokens\\n            yield tok_str\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Huggingface(Model):\n",
    "    model_name: Optional[str] = None\n",
    "    model: Optional[PreTrainedModel] = None\n",
    "    tokenizer: Optional[PreTrainedTokenizer] = None\n",
    "    chunk_size: int = 64\n",
    "    supported_decodings: Set[DecodingStrategy] = frozenset(\n",
    "        (\n",
    "            DecodingStrategy.GREEDY,\n",
    "            DecodingStrategy.SAMPLE,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.model_name is None and self.model is None and self.tokenizer is None:\n",
    "            raise ValueError(\n",
    "                \"must specify either `model_name` or both `model` and `tokenizer`\"\n",
    "            )\n",
    "        if (self.model is not None and self.tokenizer is None) or (\n",
    "            self.model is None and self.tokenizer is not None\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"must specify either `model_name` or both `model` and `tokenizer`\"\n",
    "            )\n",
    "        self.model = self.model or AutoModelForCausalLM.from_pretrained(self.model_name)\n",
    "        self.tokenizer = self.tokenizer or AutoTokenizer.from_pretrained(\n",
    "            self.model_name\n",
    "        )\n",
    "        self.tokens = transformers_tokens(self.tokenizer)\n",
    "        self._completion_buffer = {}\n",
    "        if self.chunk_size < 1:\n",
    "            raise ValueError(f\"`chunksize` must be positive, got {self.chunksize}.\")\n",
    "\n",
    "    def encode(self, text: str) -> TokenIds:\n",
    "        return self.tokenizer.encode(text)\n",
    "\n",
    "    def decode(self, ids: TokenIds) -> str:\n",
    "        return self.tokenizer.decode(ids)\n",
    "\n",
    "    @property\n",
    "    def eos_token_id(self) -> int:\n",
    "        return self.tokenizer.eos_token_id\n",
    "\n",
    "    @property\n",
    "    def bos_token_id(self) -> int:\n",
    "        return self.tokenizer.bos_token_id\n",
    "\n",
    "    def _logit_processor(self, selected_tokens: Optional[SelectedTokens] = None):\n",
    "        logits_processor = []\n",
    "        if selected_tokens is not None:\n",
    "\n",
    "            def _logits_processor(input_ids, scores):\n",
    "                mask = np.ones_like(scores) * -1e10\n",
    "                for token_id in selected_tokens:\n",
    "                    mask[:, token_id] = 0\n",
    "                scores = scores + mask\n",
    "                return scores\n",
    "\n",
    "            logits_processor.append(_logits_processor)\n",
    "        return logits_processor\n",
    "\n",
    "    async def generate(\n",
    "        self,\n",
    "        text: str,\n",
    "        max_tokens: int = 1,\n",
    "        selected_tokens: Optional[SelectedTokens] = None,\n",
    "        decoder: Optional[Decoder] = None,\n",
    "        timeout: float = 10.0,\n",
    "    ) -> AsyncGenerator[str, None]:\n",
    "        decoder = decoder or Decoder()\n",
    "        if decoder.strategy not in self.supported_decodings:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported decoding strategy for Huggingface model `{decoder.strategy}`.\"\n",
    "            )\n",
    "        temperature = decoder.temperature\n",
    "        top_p = decoder.top_p\n",
    "        addtl = {}\n",
    "\n",
    "        if decoder.strategy == DecodingStrategy.SAMPLE:\n",
    "            addtl[\"do_sample\"] = True\n",
    "\n",
    "        gen_kwargs = dict(temperature=temperature, top_p=top_p, **addtl)\n",
    "\n",
    "        n_gen = 0\n",
    "        prompt_token_ids = self.tokenizer.encode(text)\n",
    "        while n_gen < max_tokens:\n",
    "            max_new_tokens = min(self.chunk_size, max_tokens - n_gen)\n",
    "            output = await asyncio.to_thread(\n",
    "                self.model.generate,\n",
    "                input_ids=torch.tensor(prompt_token_ids)\n",
    "                .unsqueeze(0)\n",
    "                .to(self.model.device),\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                logits_processor=self._logit_processor(selected_tokens),\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                **gen_kwargs,\n",
    "            )\n",
    "            new_token_ids = output[0, len(prompt_token_ids) :].detach().cpu().tolist()\n",
    "            prompt_token_ids += new_token_ids\n",
    "            tok_str = self.tokenizer.decode(new_token_ids, skip_special_tokens=True)\n",
    "            text += tok_str\n",
    "            n_gen += max_new_tokens\n",
    "            yield tok_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e2d4d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"class Constraint(ABC):\\n    @abstractmethod\\n    def constrain_tokens(\\n        self, base_text: str, completion_text: str, model: \\\"Model\\\"\\n    ) -> TokenConstraint:\\n        \\\"\\\"\\\"Constrain the token ids that can be sampled from the model's vocabulary.\\n\\n        Args:\\n            base_text (str): The text to which the completion_text should be appended.\\n            completion_text (str): The text to be completed.\\n            model (Model): The language model to be used.\\n\\n        Returns:\\n            None: If no restrictions are to be applied and the full vocabulary can be used.\\n            set: The set of valid token ids that can be sampled.\\n            str: If the constraint is complete and the str is the finished value, which may not be what was passed as the completion text.\\n        \\\"\\\"\\\"\\n\\n    def __or__(self, other):\\n        return OrConstraint([self, other])\\n\\n    def __and__(self, other):\\n        return AndConstraint([self, other])\\n\\n\\n@dataclass\\nclass NotConstraint(Constraint):\\n    \\\"\\\"\\\"Invert a token id constraint.\\n\\n    Attributes:\\n        constraint (Constraint): The constraint to negate.\\n    \\\"\\\"\\\"\\n\\n    constraint: Constraint\\n\\n    def constrain_tokens(\\n        self, base_text: str, completion_text: str, model: \\\"Model\\\"\\n    ) -> TokenConstraint:\\n        selected_tokens = self.constraint.constrain_tokens(\\n            base_text, completion_text, model\\n        )\\n        if selected_tokens is None or isinstance(selected_tokens, str):\\n            return selected_tokens\\n        return {tok for tok in model.tokens if tok not in selected_tokens}\\n\\n\\n@dataclass\\nclass AndConstraint(Constraint):\\n    \\\"\\\"\\\"Constrain token ids that can be sampled by applying multiple constraints.\\n\\n    Attributes:\\n        constraints (List[Constraint]): The list of constraints to apply.\\n    \\\"\\\"\\\"\\n\\n    constraints: List[Constraint]\\n\\n    def constrain_tokens(\\n        self, base_text: str, completion_text: str, model: \\\"Model\\\"\\n    ) -> Union[None, Set[int], str]:\\n        ret = None\\n        completions = []\\n        for constraint in self.constraints:\\n            completions = []\\n            selected_tokens = constraint.constrain_tokens(\\n                base_text, completion_text, model\\n            )\\n            if selected_tokens is None:\\n                # Do nothing because all tokens are valid\\n                pass\\n            if isinstance(selected_tokens, str):\\n                completions.append(selected_tokens)\\n            if isinstance(selected_tokens, set):\\n                ret = ret & selected_tokens if ret is not None else selected_tokens\\n        if len(completions) == len(self.constraints):\\n            if len(set(completions)) != 1:\\n                raise ValueError(\\n                    f\\\"Got different completions for constraints `{self}`. Completions: `{set(completions)}`\\\"\\n                )\\n            return completions[0]\\n        return ret\\n\\n\\n@dataclass\\nclass OrConstraint(Constraint):\\n    \\\"\\\"\\\"Constrain token ids that can be sampled by applying multiple constraints.\\n\\n    Attributes:\\n        constraints (List[Constraint]): The list of constraints to apply.\\n    \\\"\\\"\\\"\\n\\n    constraints: List[Constraint]\\n\\n    def constrain_tokens(\\n        self, base_text: str, completion_text: str, model: \\\"Model\\\"\\n    ) -> Union[None, Set[int], str]:\\n        ret = set()\\n        for constraint in self.constraints:\\n            selected_tokens = constraint.constrain_tokens(\\n                base_text, completion_text, model\\n            )\\n            if selected_tokens is None:\\n                # One allows everything so overall the or does\\n                return None\\n            if isinstance(selected_tokens, str):\\n                return selected_tokens\\n            if isinstance(selected_tokens, set):\\n                ret |= selected_tokens\\n        return ret\\n\\n\\n@dataclass\\nclass RegexConstraint(Constraint):\\n    \\\"\\\"\\\"Constrain token ids that can be sampled based on a regex pattern.\\n\\n    Attributes:\\n        pattern (str): The regex pattern to match.\\n\\n    Notes:\\n        Based on https://github.com/r2d4/rellm\\n    \\\"\\\"\\\"\\n\\n    pattern: str\\n\\n    def __post_init__(self):\\n        self._pattern = re.compile(self.pattern)\\n\\n    def _is_valid_token(\\n        self, token_id: int, partial_completion: str, model: \\\"Model\\\"\\n    ) -> bool:\\n        decoded_token = model.tokens[token_id]\\n        return self._pattern.fullmatch(partial_completion + decoded_token, partial=True)\\n\\n    def constrain_tokens(\\n        self, base_text: str, completion_text: str, model: \\\"Model\\\"\\n    ) -> TokenConstraint:\\n        m = self._pattern.match(completion_text)\\n        if m and m.start() == 0:\\n            return completion_text\\n\\n        with ThreadPoolExecutor():\\n            valid_token_ids = set(\\n                filter(\\n                    lambda token_id: self._is_valid_token(\\n                        token_id, completion_text, model\\n                    ),\\n                    model.tokens.keys(),\\n                )\\n            )\\n\\n        return valid_token_ids\\n\\n\\n@dataclass\\nclass StopsConstraint(Constraint):\\n    \\\"\\\"\\\"Constrain token ids that can be sampled based on a regex pattern.\\n\\n    Attributes:\\n        stop (str): The string after which to stop.\\n        include (bool): Whether to include the stop string in the completion or not.\\n    \\\"\\\"\\\"\\n\\n    stop: str\\n    include: bool = True\\n\\n    def __post_init__(self):\\n        end = stop\\n        if not include:\\n            end = f\\\"(?={stop})\\\"\\n        self._re_constraint = RegexConstraint(\\\".*?\\\" + end)\\n\\n    def constrain_tokens(\\n        self, base_text: str, completion_text: str, model: Model\\n    ) -> TokenConstraint:\\n        return self._re_constraint(base_text, completion_text, model)\\n\\n\\n@dataclass\\nclass OptionsConstraint(Constraint):\\n    \\\"\\\"\\\"\\n    Options constraint constrains output based on a list of string options\\n    \\\"\\\"\\\"\\n\\n    options: Set[str]\\n    short_circuit: bool = (\\n        True  # early return when available options based on completed text are <=1\\n    )\\n\\n    def _is_valid_token(\\n        self, token_id: int, partial_completion: str, model: Model\\n    ) -> bool:\\n        decoded_token = model.tokens[token_id]\\n        return any(\\n            option.startswith(partial_completion + decoded_token)\\n            for option in self.options\\n        )\\n\\n    def constrain_tokens(\\n        self, base_text: str, completion_text: str, model: Model\\n    ) -> TokenConstraint:\\n        if completion_text in self.options:\\n            return completion_text\\n\\n        if completion_text and self.short_circuit:\\n            limited_options = set()\\n            for option in self.options:\\n                if option.startswith(completion_text):\\n                    limited_options.add(option)\\n                    if len(limited_options) > 1:\\n                        break\\n            if len(limited_options) == 0:\\n                return {}\\n            if len(limited_options) == 1:\\n                return limited_options.pop()\\n\\n        with ThreadPoolExecutor():\\n            valid_token_ids = set(\\n                filter(\\n                    lambda token_id: self._is_valid_token(\\n                        token_id, completion_text, model\\n                    ),\\n                    model.tokens.keys(),\\n                )\\n            )\\n\\n        return valid_token_ids\";\n",
       "                var nbb_formatted_code = \"class Constraint(ABC):\\n    @abstractmethod\\n    def constrain_tokens(\\n        self, base_text: str, completion_text: str, model: \\\"Model\\\"\\n    ) -> TokenConstraint:\\n        \\\"\\\"\\\"Constrain the token ids that can be sampled from the model's vocabulary.\\n\\n        Args:\\n            base_text (str): The text to which the completion_text should be appended.\\n            completion_text (str): The text to be completed.\\n            model (Model): The language model to be used.\\n\\n        Returns:\\n            None: If no restrictions are to be applied and the full vocabulary can be used.\\n            set: The set of valid token ids that can be sampled.\\n            str: If the constraint is complete and the str is the finished value, which may not be what was passed as the completion text.\\n        \\\"\\\"\\\"\\n\\n    def __or__(self, other):\\n        return OrConstraint([self, other])\\n\\n    def __and__(self, other):\\n        return AndConstraint([self, other])\\n\\n\\n@dataclass\\nclass NotConstraint(Constraint):\\n    \\\"\\\"\\\"Invert a token id constraint.\\n\\n    Attributes:\\n        constraint (Constraint): The constraint to negate.\\n    \\\"\\\"\\\"\\n\\n    constraint: Constraint\\n\\n    def constrain_tokens(\\n        self, base_text: str, completion_text: str, model: \\\"Model\\\"\\n    ) -> TokenConstraint:\\n        selected_tokens = self.constraint.constrain_tokens(\\n            base_text, completion_text, model\\n        )\\n        if selected_tokens is None or isinstance(selected_tokens, str):\\n            return selected_tokens\\n        return {tok for tok in model.tokens if tok not in selected_tokens}\\n\\n\\n@dataclass\\nclass AndConstraint(Constraint):\\n    \\\"\\\"\\\"Constrain token ids that can be sampled by applying multiple constraints.\\n\\n    Attributes:\\n        constraints (List[Constraint]): The list of constraints to apply.\\n    \\\"\\\"\\\"\\n\\n    constraints: List[Constraint]\\n\\n    def constrain_tokens(\\n        self, base_text: str, completion_text: str, model: \\\"Model\\\"\\n    ) -> Union[None, Set[int], str]:\\n        ret = None\\n        completions = []\\n        for constraint in self.constraints:\\n            completions = []\\n            selected_tokens = constraint.constrain_tokens(\\n                base_text, completion_text, model\\n            )\\n            if selected_tokens is None:\\n                # Do nothing because all tokens are valid\\n                pass\\n            if isinstance(selected_tokens, str):\\n                completions.append(selected_tokens)\\n            if isinstance(selected_tokens, set):\\n                ret = ret & selected_tokens if ret is not None else selected_tokens\\n        if len(completions) == len(self.constraints):\\n            if len(set(completions)) != 1:\\n                raise ValueError(\\n                    f\\\"Got different completions for constraints `{self}`. Completions: `{set(completions)}`\\\"\\n                )\\n            return completions[0]\\n        return ret\\n\\n\\n@dataclass\\nclass OrConstraint(Constraint):\\n    \\\"\\\"\\\"Constrain token ids that can be sampled by applying multiple constraints.\\n\\n    Attributes:\\n        constraints (List[Constraint]): The list of constraints to apply.\\n    \\\"\\\"\\\"\\n\\n    constraints: List[Constraint]\\n\\n    def constrain_tokens(\\n        self, base_text: str, completion_text: str, model: \\\"Model\\\"\\n    ) -> Union[None, Set[int], str]:\\n        ret = set()\\n        for constraint in self.constraints:\\n            selected_tokens = constraint.constrain_tokens(\\n                base_text, completion_text, model\\n            )\\n            if selected_tokens is None:\\n                # One allows everything so overall the or does\\n                return None\\n            if isinstance(selected_tokens, str):\\n                return selected_tokens\\n            if isinstance(selected_tokens, set):\\n                ret |= selected_tokens\\n        return ret\\n\\n\\n@dataclass\\nclass RegexConstraint(Constraint):\\n    \\\"\\\"\\\"Constrain token ids that can be sampled based on a regex pattern.\\n\\n    Attributes:\\n        pattern (str): The regex pattern to match.\\n\\n    Notes:\\n        Based on https://github.com/r2d4/rellm\\n    \\\"\\\"\\\"\\n\\n    pattern: str\\n\\n    def __post_init__(self):\\n        self._pattern = re.compile(self.pattern)\\n\\n    def _is_valid_token(\\n        self, token_id: int, partial_completion: str, model: \\\"Model\\\"\\n    ) -> bool:\\n        decoded_token = model.tokens[token_id]\\n        return self._pattern.fullmatch(partial_completion + decoded_token, partial=True)\\n\\n    def constrain_tokens(\\n        self, base_text: str, completion_text: str, model: \\\"Model\\\"\\n    ) -> TokenConstraint:\\n        m = self._pattern.match(completion_text)\\n        if m and m.start() == 0:\\n            return completion_text\\n\\n        with ThreadPoolExecutor():\\n            valid_token_ids = set(\\n                filter(\\n                    lambda token_id: self._is_valid_token(\\n                        token_id, completion_text, model\\n                    ),\\n                    model.tokens.keys(),\\n                )\\n            )\\n\\n        return valid_token_ids\\n\\n\\n@dataclass\\nclass StopsConstraint(Constraint):\\n    \\\"\\\"\\\"Constrain token ids that can be sampled based on a regex pattern.\\n\\n    Attributes:\\n        stop (str): The string after which to stop.\\n        include (bool): Whether to include the stop string in the completion or not.\\n    \\\"\\\"\\\"\\n\\n    stop: str\\n    include: bool = True\\n\\n    def __post_init__(self):\\n        end = stop\\n        if not include:\\n            end = f\\\"(?={stop})\\\"\\n        self._re_constraint = RegexConstraint(\\\".*?\\\" + end)\\n\\n    def constrain_tokens(\\n        self, base_text: str, completion_text: str, model: Model\\n    ) -> TokenConstraint:\\n        return self._re_constraint(base_text, completion_text, model)\\n\\n\\n@dataclass\\nclass OptionsConstraint(Constraint):\\n    \\\"\\\"\\\"\\n    Options constraint constrains output based on a list of string options\\n    \\\"\\\"\\\"\\n\\n    options: Set[str]\\n    short_circuit: bool = (\\n        True  # early return when available options based on completed text are <=1\\n    )\\n\\n    def _is_valid_token(\\n        self, token_id: int, partial_completion: str, model: Model\\n    ) -> bool:\\n        decoded_token = model.tokens[token_id]\\n        return any(\\n            option.startswith(partial_completion + decoded_token)\\n            for option in self.options\\n        )\\n\\n    def constrain_tokens(\\n        self, base_text: str, completion_text: str, model: Model\\n    ) -> TokenConstraint:\\n        if completion_text in self.options:\\n            return completion_text\\n\\n        if completion_text and self.short_circuit:\\n            limited_options = set()\\n            for option in self.options:\\n                if option.startswith(completion_text):\\n                    limited_options.add(option)\\n                    if len(limited_options) > 1:\\n                        break\\n            if len(limited_options) == 0:\\n                return {}\\n            if len(limited_options) == 1:\\n                return limited_options.pop()\\n\\n        with ThreadPoolExecutor():\\n            valid_token_ids = set(\\n                filter(\\n                    lambda token_id: self._is_valid_token(\\n                        token_id, completion_text, model\\n                    ),\\n                    model.tokens.keys(),\\n                )\\n            )\\n\\n        return valid_token_ids\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Constraint(ABC):\n",
    "    @abstractmethod\n",
    "    def constrain_tokens(\n",
    "        self, base_text: str, completion_text: str, model: \"Model\"\n",
    "    ) -> TokenConstraint:\n",
    "        \"\"\"Constrain the token ids that can be sampled from the model's vocabulary.\n",
    "\n",
    "        Args:\n",
    "            base_text (str): The text to which the completion_text should be appended.\n",
    "            completion_text (str): The text to be completed.\n",
    "            model (Model): The language model to be used.\n",
    "\n",
    "        Returns:\n",
    "            None: If no restrictions are to be applied and the full vocabulary can be used.\n",
    "            set: The set of valid token ids that can be sampled.\n",
    "            str: If the constraint is complete and the str is the finished value, which may not be what was passed as the completion text.\n",
    "        \"\"\"\n",
    "\n",
    "    def __or__(self, other):\n",
    "        return OrConstraint([self, other])\n",
    "\n",
    "    def __and__(self, other):\n",
    "        return AndConstraint([self, other])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NotConstraint(Constraint):\n",
    "    \"\"\"Invert a token id constraint.\n",
    "\n",
    "    Attributes:\n",
    "        constraint (Constraint): The constraint to negate.\n",
    "    \"\"\"\n",
    "\n",
    "    constraint: Constraint\n",
    "\n",
    "    def constrain_tokens(\n",
    "        self, base_text: str, completion_text: str, model: \"Model\"\n",
    "    ) -> TokenConstraint:\n",
    "        selected_tokens = self.constraint.constrain_tokens(\n",
    "            base_text, completion_text, model\n",
    "        )\n",
    "        if selected_tokens is None or isinstance(selected_tokens, str):\n",
    "            return selected_tokens\n",
    "        return {tok for tok in model.tokens if tok not in selected_tokens}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AndConstraint(Constraint):\n",
    "    \"\"\"Constrain token ids that can be sampled by applying multiple constraints.\n",
    "\n",
    "    Attributes:\n",
    "        constraints (List[Constraint]): The list of constraints to apply.\n",
    "    \"\"\"\n",
    "\n",
    "    constraints: List[Constraint]\n",
    "\n",
    "    def constrain_tokens(\n",
    "        self, base_text: str, completion_text: str, model: \"Model\"\n",
    "    ) -> Union[None, Set[int], str]:\n",
    "        ret = None\n",
    "        completions = []\n",
    "        for constraint in self.constraints:\n",
    "            completions = []\n",
    "            selected_tokens = constraint.constrain_tokens(\n",
    "                base_text, completion_text, model\n",
    "            )\n",
    "            if selected_tokens is None:\n",
    "                # Do nothing because all tokens are valid\n",
    "                pass\n",
    "            if isinstance(selected_tokens, str):\n",
    "                completions.append(selected_tokens)\n",
    "            if isinstance(selected_tokens, set):\n",
    "                ret = ret & selected_tokens if ret is not None else selected_tokens\n",
    "        if len(completions) == len(self.constraints):\n",
    "            if len(set(completions)) != 1:\n",
    "                raise ValueError(\n",
    "                    f\"Got different completions for constraints `{self}`. Completions: `{set(completions)}`\"\n",
    "                )\n",
    "            return completions[0]\n",
    "        return ret\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OrConstraint(Constraint):\n",
    "    \"\"\"Constrain token ids that can be sampled by applying multiple constraints.\n",
    "\n",
    "    Attributes:\n",
    "        constraints (List[Constraint]): The list of constraints to apply.\n",
    "    \"\"\"\n",
    "\n",
    "    constraints: List[Constraint]\n",
    "\n",
    "    def constrain_tokens(\n",
    "        self, base_text: str, completion_text: str, model: \"Model\"\n",
    "    ) -> Union[None, Set[int], str]:\n",
    "        ret = set()\n",
    "        for constraint in self.constraints:\n",
    "            selected_tokens = constraint.constrain_tokens(\n",
    "                base_text, completion_text, model\n",
    "            )\n",
    "            if selected_tokens is None:\n",
    "                # One allows everything so overall the or does\n",
    "                return None\n",
    "            if isinstance(selected_tokens, str):\n",
    "                return selected_tokens\n",
    "            if isinstance(selected_tokens, set):\n",
    "                ret |= selected_tokens\n",
    "        return ret\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RegexConstraint(Constraint):\n",
    "    \"\"\"Constrain token ids that can be sampled based on a regex pattern.\n",
    "\n",
    "    Attributes:\n",
    "        pattern (str): The regex pattern to match.\n",
    "\n",
    "    Notes:\n",
    "        Based on https://github.com/r2d4/rellm\n",
    "    \"\"\"\n",
    "\n",
    "    pattern: str\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self._pattern = re.compile(self.pattern)\n",
    "\n",
    "    def _is_valid_token(\n",
    "        self, token_id: int, partial_completion: str, model: \"Model\"\n",
    "    ) -> bool:\n",
    "        decoded_token = model.tokens[token_id]\n",
    "        return self._pattern.fullmatch(partial_completion + decoded_token, partial=True)\n",
    "\n",
    "    def constrain_tokens(\n",
    "        self, base_text: str, completion_text: str, model: \"Model\"\n",
    "    ) -> TokenConstraint:\n",
    "        m = self._pattern.match(completion_text)\n",
    "        if m and m.start() == 0:\n",
    "            return completion_text\n",
    "\n",
    "        with ThreadPoolExecutor():\n",
    "            valid_token_ids = set(\n",
    "                filter(\n",
    "                    lambda token_id: self._is_valid_token(\n",
    "                        token_id, completion_text, model\n",
    "                    ),\n",
    "                    model.tokens.keys(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return valid_token_ids\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StopsConstraint(Constraint):\n",
    "    \"\"\"Constrain token ids that can be sampled based on a regex pattern.\n",
    "\n",
    "    Attributes:\n",
    "        stop (str): The string after which to stop.\n",
    "        include (bool): Whether to include the stop string in the completion or not.\n",
    "    \"\"\"\n",
    "\n",
    "    stop: str\n",
    "    include: bool = True\n",
    "\n",
    "    def __post_init__(self):\n",
    "        end = stop\n",
    "        if not include:\n",
    "            end = f\"(?={stop})\"\n",
    "        self._re_constraint = RegexConstraint(\".*?\" + end)\n",
    "\n",
    "    def constrain_tokens(\n",
    "        self, base_text: str, completion_text: str, model: Model\n",
    "    ) -> TokenConstraint:\n",
    "        return self._re_constraint(base_text, completion_text, model)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OptionsConstraint(Constraint):\n",
    "    \"\"\"\n",
    "    Options constraint constrains output based on a list of string options\n",
    "    \"\"\"\n",
    "\n",
    "    options: Set[str]\n",
    "    short_circuit: bool = (\n",
    "        True  # early return when available options based on completed text are <=1\n",
    "    )\n",
    "\n",
    "    def _is_valid_token(\n",
    "        self, token_id: int, partial_completion: str, model: Model\n",
    "    ) -> bool:\n",
    "        decoded_token = model.tokens[token_id]\n",
    "        return any(\n",
    "            option.startswith(partial_completion + decoded_token)\n",
    "            for option in self.options\n",
    "        )\n",
    "\n",
    "    def constrain_tokens(\n",
    "        self, base_text: str, completion_text: str, model: Model\n",
    "    ) -> TokenConstraint:\n",
    "        if completion_text in self.options:\n",
    "            return completion_text\n",
    "\n",
    "        if completion_text and self.short_circuit:\n",
    "            limited_options = set()\n",
    "            for option in self.options:\n",
    "                if option.startswith(completion_text):\n",
    "                    limited_options.add(option)\n",
    "                    if len(limited_options) > 1:\n",
    "                        break\n",
    "            if len(limited_options) == 0:\n",
    "                return {}\n",
    "            if len(limited_options) == 1:\n",
    "                return limited_options.pop()\n",
    "\n",
    "        with ThreadPoolExecutor():\n",
    "            valid_token_ids = set(\n",
    "                filter(\n",
    "                    lambda token_id: self._is_valid_token(\n",
    "                        token_id, completion_text, model\n",
    "                    ),\n",
    "                    model.tokens.keys(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return valid_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dfda67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"class Completion(str):\\n    \\\"\\\"\\\"A completion string from a prompt\\n\\n    Args:\\n        text (str): the generated string\\n        start (int): the start index of the completion in the prompt it came from\\n        stop (int): the stop index of the completion in the prompt it came from\\n\\n    Returns:\\n        Completion (str)\\n    \\\"\\\"\\\"\\n\\n    def __new__(cls, text: str, start: int, stop: int):\\n        if isinstance(text, Completion):\\n            return text\\n        obj = str.__new__(cls, text)\\n        obj.start = start\\n        obj.stop = stop\\n        return obj\\n\\n    def __repr__(self) -> str:\\n        return f\\\"Completion(text = '{self}', start = {self.start}, stop = {self.stop})\\\"\";\n",
       "                var nbb_formatted_code = \"class Completion(str):\\n    \\\"\\\"\\\"A completion string from a prompt\\n\\n    Args:\\n        text (str): the generated string\\n        start (int): the start index of the completion in the prompt it came from\\n        stop (int): the stop index of the completion in the prompt it came from\\n\\n    Returns:\\n        Completion (str)\\n    \\\"\\\"\\\"\\n\\n    def __new__(cls, text: str, start: int, stop: int):\\n        if isinstance(text, Completion):\\n            return text\\n        obj = str.__new__(cls, text)\\n        obj.start = start\\n        obj.stop = stop\\n        return obj\\n\\n    def __repr__(self) -> str:\\n        return f\\\"Completion(text = '{self}', start = {self.start}, stop = {self.stop})\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Completion(str):\n",
    "    \"\"\"A completion string from a prompt\n",
    "\n",
    "    Args:\n",
    "        text (str): the generated string\n",
    "        start (int): the start index of the completion in the prompt it came from\n",
    "        stop (int): the stop index of the completion in the prompt it came from\n",
    "\n",
    "    Returns:\n",
    "        Completion (str)\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(cls, text: str, start: int, stop: int):\n",
    "        if isinstance(text, Completion):\n",
    "            return text\n",
    "        obj = str.__new__(cls, text)\n",
    "        obj.start = start\n",
    "        obj.stop = stop\n",
    "        return obj\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Completion(text = '{self}', start = {self.start}, stop = {self.stop})\"\n",
    "\n",
    "class Completions:\n",
    "    def __init__(self):\n",
    "        self._completions = []\n",
    "        self._named_completions = {}\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Completions({self._completions}, {self._named_completions})\"\n",
    "\n",
    "    def add(self, completion, name=None):\n",
    "        if name is not None:\n",
    "            self._named_completions[name] = completion\n",
    "        else:\n",
    "            self._completions.append(completion)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, int):\n",
    "            return self._completions[key]\n",
    "        else:\n",
    "            return self._named_completions[key]\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in self._named_completions:\n",
    "            return self._named_completions[name]\n",
    "        else:\n",
    "            raise AttributeError(f\"'Completions' object has no attribute '{name}'\")\n",
    "\n",
    "    def __or__(self, other):\n",
    "        if isinstance(other, Completions):\n",
    "            combined = Completions()\n",
    "            combined._completions = self._completions + other._completions\n",
    "            combined._named_completions = {\n",
    "                **self._named_completions,\n",
    "                **other._named_completions,\n",
    "            }\n",
    "            return combined\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                f\"unsupported operand type(s) for |: 'Completions' and '{type(other).__name__}'\"\n",
    "            )\n",
    "\n",
    "class Prompt(str):\n",
    "    \"\"\"A Prompt is a piece of text a model can generate off of\n",
    "\n",
    "    Args:\n",
    "        prompt (str): the string representing the current completion of the Prompt\n",
    "\n",
    "    Returns:\n",
    "        Prompt (str)\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(cls, prompt: str, completions: Optional[Completions] = None):\n",
    "        if isinstance(prompt, Completion):\n",
    "            return prompt\n",
    "        obj = str.__new__(cls, prompt)\n",
    "        obj.prompt = prompt\n",
    "        obj.completions = completions or Completions()\n",
    "        return obj\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Prompt('{self.prompt}')\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.prompt\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, str):\n",
    "            return Prompt(self.prompt + other, self.completions)\n",
    "        elif isinstance(other, Prompt):\n",
    "            return Prompt(\n",
    "                self.prompt + other.prompt, self.completions | other.completions\n",
    "            )\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                f\"Cannot concatenate Prompt object with object of type {type(other)}\"\n",
    "            )\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        if isinstance(other, str):\n",
    "            return Prompt(other + self.prompt, self.completions)\n",
    "        elif isinstance(other, Prompt):\n",
    "            return Prompt(\n",
    "                other.prompt + self.prompt, self.completions | other.completions\n",
    "            )\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                f\"Cannot concatenate object of type {type(other)} with Prompt object\"\n",
    "            )\n",
    "\n",
    "    def token_length(self, model: Model) -> int:\n",
    "        return len(model.encode(self.prompt))\n",
    "\n",
    "    async def complete(\n",
    "        self,\n",
    "        model: Model,\n",
    "        constraint: Optional[Constraint] = None,\n",
    "        name: Optional[str] = None,\n",
    "        max_tokens: Optional[int] = None,\n",
    "        decoder: Optional[Decoder] = None,\n",
    "        stream_queue: Optional[asyncio.Queue] = None,\n",
    "        timeout: float = 10.0,\n",
    "        truncate: bool = False,\n",
    "    ):\n",
    "        text = self.prompt\n",
    "        prompt_tokens = model.encode(self.prompt)\n",
    "        token_limit = min(\n",
    "            max_tokens or float(\"inf\"), model.max_total_tokens - len(prompt_tokens)\n",
    "        )\n",
    "        if (\n",
    "            truncate\n",
    "            and (len(prompt_tokens) + (max_tokens or 0)) >= model.max_total_tokens\n",
    "        ):\n",
    "            warnings.warn(\n",
    "                f\"Prompt plus `max_tokens` more than model `max_total_tokens` of {model.max_total_tokens}.\"\n",
    "                \"Truncating from right.\"\n",
    "            )\n",
    "            text = model.decode(\n",
    "                prompt_tokens[-(model.max_total_tokens - token_limit) :]\n",
    "            )\n",
    "\n",
    "        if max_tokens is not None and max_tokens > token_limit:\n",
    "            warnings.warn(\n",
    "                f\"Requested `max_tokens` of {max_tokens} \"\n",
    "                f\"greater than remaining token limit {token_limit} \"\n",
    "                f\"from model {str(model)[:10]}...) which has \"\n",
    "                f\"`max_total_tokens` {model.max_total_tokens}. \"\n",
    "                f\"will limit `max_tokens` to {token_limit}.\"\n",
    "            )\n",
    "        if constraint is None:\n",
    "            generated = \"\"\n",
    "            async for tok in model.generate(\n",
    "                text, max_tokens=max_tokens, decoder=decoder, timeout=timeout\n",
    "            ):\n",
    "                if stream_queue:\n",
    "                    await stream_queue.put(tok)\n",
    "                generated += tok\n",
    "            if stream_queue:\n",
    "                await stream_queue.put(None)\n",
    "            return self + generated\n",
    "\n",
    "        token_count = 0\n",
    "        partial_completion = \"\"\n",
    "        prompt_plus_completion = text[:]\n",
    "        while token_count < token_limit:\n",
    "            selected_token_ids = constraint.constrain_tokens(\n",
    "                text, partial_completion, model\n",
    "            )\n",
    "            selected_token_ids = (\n",
    "                None\n",
    "                if len(selected_token_ids) > model.vocab_size\n",
    "                else selected_token_ids\n",
    "            )\n",
    "            if isinstance(selected_token_ids, set) and len(selected_token_ids) == 0:\n",
    "                warnings.warn(\n",
    "                    f\"Empty token mask encountered with Constraint `{constraint}`. Ending completion.\"\n",
    "                )\n",
    "                break\n",
    "            if isinstance(selected_token_ids, str):\n",
    "                partial_completion = selected_token_ids\n",
    "                break\n",
    "            generation = await model.sample(\n",
    "                prompt_plus_completion,\n",
    "                selected_tokens=selected_token_ids,\n",
    "                decoder=decoder,\n",
    "                timeout=timeout,\n",
    "            )\n",
    "            if model.encode(generation)[-1] == model.eos_token_id:\n",
    "                break\n",
    "            if stream_queue:\n",
    "                await stream_queue.put(tok)\n",
    "            partial_completion += generation\n",
    "            prompt_plus_completion = self.prompt + partial_completion\n",
    "            token_count += 1\n",
    "        if stream_queue:\n",
    "            await stream_queue.put(None)\n",
    "\n",
    "        ret = self + partial_completion\n",
    "        ret.completions.add(\n",
    "            Completion(\n",
    "                partial_completion,\n",
    "                len(self.prompt),\n",
    "                len(self.prompt) + len(partial_completion),\n",
    "            ),\n",
    "            name,\n",
    "        )\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39eb527e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"from transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\\"gpt2\\\")\\ntokenizer = AutoTokenizer.from_pretrained(\\\"gpt2\\\")\";\n",
       "                var nbb_formatted_code = \"from transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\\"gpt2\\\")\\ntokenizer = AutoTokenizer.from_pretrained(\\\"gpt2\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0a4434f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 89;\n",
       "                var nbb_unformatted_code = \"hf = Huggingface(model=model, tokenizer=tokenizer, chunk_size=3)\\n\\n# async for t in hf.generate(\\\"Hello,\\\", 10):\\n#     print(t)\\n\\n# await hf.sample(\\\"Hello,\\\")\";\n",
       "                var nbb_formatted_code = \"hf = Huggingface(model=model, tokenizer=tokenizer, chunk_size=3)\\n\\n# async for t in hf.generate(\\\"Hello,\\\", 10):\\n#     print(t)\\n\\n# await hf.sample(\\\"Hello,\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf = Huggingface(model=model, tokenizer=tokenizer, chunk_size=3)\n",
    "\n",
    "# async for t in hf.generate(\"Hello,\", 10):\n",
    "#     print(t)\n",
    "\n",
    "# await hf.sample(\"Hello,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c9ee09f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 124;\n",
       "                var nbb_unformatted_code = \"prompt = Prompt(\\\"There are (3-2) = \\\")\";\n",
       "                var nbb_formatted_code = \"prompt = Prompt(\\\"There are (3-2) = \\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = Prompt(\"There are (3-2) = \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "21126118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 125;\n",
       "                var nbb_unformatted_code = \"constraint = NotConstraint(OptionsConstraint(set(\\\"789\\\")))\";\n",
       "                var nbb_formatted_code = \"constraint = NotConstraint(OptionsConstraint(set(\\\"789\\\")))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "constraint = NotConstraint(OptionsConstraint(set(\"789\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cee411e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NotConstraint(constraint=OptionsConstraint(options={'7', '9', '8'}, short_circuit=True))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 126;\n",
       "                var nbb_unformatted_code = \"constraint\";\n",
       "                var nbb_formatted_code = \"constraint\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bbd551f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 127;\n",
       "                var nbb_unformatted_code = \"prompt = await prompt.complete(\\n    model=hf,\\n    constraint=constraint,\\n    name=\\\"num_piggies\\\",\\n    max_tokens=10,\\n    decoder=Decoder(temperature=0.5, top_p=0.1, strategy=DecodingStrategy.SAMPLE),\\n)\";\n",
       "                var nbb_formatted_code = \"prompt = await prompt.complete(\\n    model=hf,\\n    constraint=constraint,\\n    name=\\\"num_piggies\\\",\\n    max_tokens=10,\\n    decoder=Decoder(temperature=0.5, top_p=0.1, strategy=DecodingStrategy.SAMPLE),\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = await prompt.complete(\n",
    "    model=hf,\n",
    "    constraint=constraint,\n",
    "    name=\"num_piggies\",\n",
    "    max_tokens=10,\n",
    "    decoder=Decoder(temperature=0.5, top_p=0.1, strategy=DecodingStrategy.SAMPLE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "61134dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt('There are (3-2) =   1-1 =   1-1 =')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 128;\n",
       "                var nbb_unformatted_code = \"prompt\";\n",
       "                var nbb_formatted_code = \"prompt\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e9103e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"prompt = await Prompt(\\\"There are (3-2) = \\\").complete(\\n    model=hf,\\n    constraint=RegexConstraint(pattern=r\\\"one|15|three\\\"),\\n    name=\\\"num_piggies\\\",\\n)\";\n",
       "                var nbb_formatted_code = \"prompt = await Prompt(\\\"There are (3-2) = \\\").complete(\\n    model=hf,\\n    constraint=RegexConstraint(pattern=r\\\"one|15|three\\\"),\\n    name=\\\"num_piggies\\\",\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = await Prompt(\"There are (3-2) = \").complete(\n",
    "    model=hf,\n",
    "    constraint=RegexConstraint(pattern=r\"one|15|three\"),\n",
    "    name=\"num_piggies\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7b456f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt('There are (3-2) = 15 piggys')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"prompt + \\\" piggys\\\"\";\n",
       "                var nbb_formatted_code = \"prompt + \\\" piggys\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt + \" piggys\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29a14a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"int(prompt.completions.num_piggies)\";\n",
       "                var nbb_formatted_code = \"int(prompt.completions.num_piggies)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "int(prompt.completions.num_piggies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00cd0e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"import regex as re\\nfrom typing import List, Dict\\n\\n\\ndef split_tags(\\n    text: str,\\n    tag_start: str = \\\"%\\\",\\n    tag_end: str = \\\"%\\\",\\n    default_role: str = \\\"assistant\\\",\\n    roles: List[str] = [\\\"system\\\", \\\"user\\\", \\\"assistant\\\"],\\n) -> List[Dict[str, str]]:\\n    \\\"\\\"\\\"\\n    Splits a text string into a list of messages based on tags.\\n\\n    Args:\\n        text (str): The input text to split into messages.\\n        tag_start (str, optional): The start delimiter for tags. Defaults to '%'.\\n        tag_end (str, optional): The end delimiter for tags. Defaults to '%'.\\n        default_role (str, optional): The default role to use for untagged messages. Defaults to 'assistant'.\\n        roles (List[str], optional): The list of valid roles for tagged messages. Defaults to ['system', 'user', 'assistant'].\\n\\n    Returns:\\n        List[Dict[str, str]]: A list of messages, where each message is a dictionary with keys 'role' and 'content'.\\n\\n    Raises:\\n        Exception: If an end tag is found with no start tag, or if an unknown or mismatched tag is found.\\n\\n    Examples:\\n        >>> split_tags('\\\\nYou are a friendly bot\\\\n%/system%\\\\n%user%Can you help me calculate stuff?%/user%\\\\nYes, how may I help you?\\\\n%user%\\\\nI want to know the square root of 10%/user%\\\\n%assistant%\\\\nSure the square root of 10 is ...\\\\n%/assistant%')\\n        [{'role': 'assistant', 'content': 'You are a friendly bot\\\\n'}, {'role': 'system', 'content': ''}, {'role': 'user', 'content': 'Can you help me calculate stuff?'}, {'role': 'assistant', 'content': 'Yes, how may I help you?\\\\n'}, {'role': 'user', 'co...\\n    \\\"\\\"\\\"\\n    text = str(text)\\n    current = None\\n    messages = []\\n\\n    while text:\\n        # first we check to see if the text is untagged\\n        match_role = None\\n        match = re.search(\\n            rf\\\"(?P<content>\\\\s*.*?)\\\\s*(?P<tag>{tag_start}/?(?P<role>.*?){tag_end}\\\\s*|$)\\\",\\n            text,\\n        )\\n        if match:\\n            content = match.group(\\\"content\\\")\\n            if match.group(\\\"tag\\\").startswith(f\\\"{tag_start}/\\\"):\\n                raise Exception(f\\\"Found end tag with no start `{match.group('tag')}`.\\\")\\n            if match.group(\\\"role\\\") is not None and match.group(\\\"role\\\") not in roles:\\n                raise Exception(f\\\"Unknown role `{match.group('role')}`.\\\")\\n            if content.strip():\\n                messages.append({\\\"role\\\": default_role, \\\"content\\\": content.strip()})\\n            text = text[match.span(\\\"tag\\\")[1] :]\\n            match_role = match.group(\\\"role\\\")\\n        if not text:\\n            break\\n        # now that we have defaulted any untagged text, we can handle the next tagged portion\\n        match = re.search(\\n            rf\\\"\\\\s*.*?(?P<tag>{tag_start}/?(?P<role>.*?){tag_end}\\\\s*)\\\", text\\n        )\\n        content = text[: match.span(\\\"tag\\\")[0]]\\n        if (match_role is not None and match.group(\\\"role\\\") != match_role) or (\\n            not match.group(\\\"tag\\\").startswith(f\\\"{tag_start}/\\\")\\n        ):\\n            raise Exception(\\n                f\\\"Unclosed tag `{match_role}`. Found `{match.group('role')}`.\\\"\\n            )\\n        messages.append({\\\"role\\\": match_role, \\\"content\\\": content.strip()})\\n        text = text[match.span(\\\"tag\\\")[1] :]\\n    return messages\\n\\n\\ndef strip_tags(\\n    prompt: Prompt,\\n    tag_start: str = \\\"%\\\",\\n    tag_end: str = \\\"%\\\",\\n    roles_seps: Dict[str, str] = {\\n        \\\"system\\\": \\\"\\\",\\n        \\\"user\\\": \\\"User: \\\",\\n        \\\"assistant\\\": \\\"Assistant: \\\",\\n    },\\n    sep: str = \\\"\\\\n\\\",\\n) -> Prompt:\\n    messages = split_tags(\\n        text=prompt, tag_start=tag_start, tag_end=tag_end, roles=list(roles_seps.keys())\\n    )\\n    return Prompt(\\n        sep.join(\\n            (roles_seps[message[\\\"role\\\"]] + message[\\\"content\\\"] for message in messages)\\n        ),\\n        prompt.completions,\\n    )\";\n",
       "                var nbb_formatted_code = \"import regex as re\\nfrom typing import List, Dict\\n\\n\\ndef split_tags(\\n    text: str,\\n    tag_start: str = \\\"%\\\",\\n    tag_end: str = \\\"%\\\",\\n    default_role: str = \\\"assistant\\\",\\n    roles: List[str] = [\\\"system\\\", \\\"user\\\", \\\"assistant\\\"],\\n) -> List[Dict[str, str]]:\\n    \\\"\\\"\\\"\\n    Splits a text string into a list of messages based on tags.\\n\\n    Args:\\n        text (str): The input text to split into messages.\\n        tag_start (str, optional): The start delimiter for tags. Defaults to '%'.\\n        tag_end (str, optional): The end delimiter for tags. Defaults to '%'.\\n        default_role (str, optional): The default role to use for untagged messages. Defaults to 'assistant'.\\n        roles (List[str], optional): The list of valid roles for tagged messages. Defaults to ['system', 'user', 'assistant'].\\n\\n    Returns:\\n        List[Dict[str, str]]: A list of messages, where each message is a dictionary with keys 'role' and 'content'.\\n\\n    Raises:\\n        Exception: If an end tag is found with no start tag, or if an unknown or mismatched tag is found.\\n\\n    Examples:\\n        >>> split_tags('\\\\nYou are a friendly bot\\\\n%/system%\\\\n%user%Can you help me calculate stuff?%/user%\\\\nYes, how may I help you?\\\\n%user%\\\\nI want to know the square root of 10%/user%\\\\n%assistant%\\\\nSure the square root of 10 is ...\\\\n%/assistant%')\\n        [{'role': 'assistant', 'content': 'You are a friendly bot\\\\n'}, {'role': 'system', 'content': ''}, {'role': 'user', 'content': 'Can you help me calculate stuff?'}, {'role': 'assistant', 'content': 'Yes, how may I help you?\\\\n'}, {'role': 'user', 'co...\\n    \\\"\\\"\\\"\\n    text = str(text)\\n    current = None\\n    messages = []\\n\\n    while text:\\n        # first we check to see if the text is untagged\\n        match_role = None\\n        match = re.search(\\n            rf\\\"(?P<content>\\\\s*.*?)\\\\s*(?P<tag>{tag_start}/?(?P<role>.*?){tag_end}\\\\s*|$)\\\",\\n            text,\\n        )\\n        if match:\\n            content = match.group(\\\"content\\\")\\n            if match.group(\\\"tag\\\").startswith(f\\\"{tag_start}/\\\"):\\n                raise Exception(f\\\"Found end tag with no start `{match.group('tag')}`.\\\")\\n            if match.group(\\\"role\\\") is not None and match.group(\\\"role\\\") not in roles:\\n                raise Exception(f\\\"Unknown role `{match.group('role')}`.\\\")\\n            if content.strip():\\n                messages.append({\\\"role\\\": default_role, \\\"content\\\": content.strip()})\\n            text = text[match.span(\\\"tag\\\")[1] :]\\n            match_role = match.group(\\\"role\\\")\\n        if not text:\\n            break\\n        # now that we have defaulted any untagged text, we can handle the next tagged portion\\n        match = re.search(\\n            rf\\\"\\\\s*.*?(?P<tag>{tag_start}/?(?P<role>.*?){tag_end}\\\\s*)\\\", text\\n        )\\n        content = text[: match.span(\\\"tag\\\")[0]]\\n        if (match_role is not None and match.group(\\\"role\\\") != match_role) or (\\n            not match.group(\\\"tag\\\").startswith(f\\\"{tag_start}/\\\")\\n        ):\\n            raise Exception(\\n                f\\\"Unclosed tag `{match_role}`. Found `{match.group('role')}`.\\\"\\n            )\\n        messages.append({\\\"role\\\": match_role, \\\"content\\\": content.strip()})\\n        text = text[match.span(\\\"tag\\\")[1] :]\\n    return messages\\n\\n\\ndef strip_tags(\\n    prompt: Prompt,\\n    tag_start: str = \\\"%\\\",\\n    tag_end: str = \\\"%\\\",\\n    roles_seps: Dict[str, str] = {\\n        \\\"system\\\": \\\"\\\",\\n        \\\"user\\\": \\\"User: \\\",\\n        \\\"assistant\\\": \\\"Assistant: \\\",\\n    },\\n    sep: str = \\\"\\\\n\\\",\\n) -> Prompt:\\n    messages = split_tags(\\n        text=prompt, tag_start=tag_start, tag_end=tag_end, roles=list(roles_seps.keys())\\n    )\\n    return Prompt(\\n        sep.join(\\n            (roles_seps[message[\\\"role\\\"]] + message[\\\"content\\\"] for message in messages)\\n        ),\\n        prompt.completions,\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def split_tags(\n",
    "    text: str,\n",
    "    tag_start: str = \"%\",\n",
    "    tag_end: str = \"%\",\n",
    "    default_role: str = \"assistant\",\n",
    "    roles: List[str] = [\"system\", \"user\", \"assistant\"],\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Splits a text string into a list of messages based on tags.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to split into messages.\n",
    "        tag_start (str, optional): The start delimiter for tags. Defaults to '%'.\n",
    "        tag_end (str, optional): The end delimiter for tags. Defaults to '%'.\n",
    "        default_role (str, optional): The default role to use for untagged messages. Defaults to 'assistant'.\n",
    "        roles (List[str], optional): The list of valid roles for tagged messages. Defaults to ['system', 'user', 'assistant'].\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of messages, where each message is a dictionary with keys 'role' and 'content'.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an end tag is found with no start tag, or if an unknown or mismatched tag is found.\n",
    "\n",
    "    Examples:\n",
    "        >>> split_tags('\\nYou are a friendly bot\\n%/system%\\n%user%Can you help me calculate stuff?%/user%\\nYes, how may I help you?\\n%user%\\nI want to know the square root of 10%/user%\\n%assistant%\\nSure the square root of 10 is ...\\n%/assistant%')\n",
    "        [{'role': 'assistant', 'content': 'You are a friendly bot\\n'}, {'role': 'system', 'content': ''}, {'role': 'user', 'content': 'Can you help me calculate stuff?'}, {'role': 'assistant', 'content': 'Yes, how may I help you?\\n'}, {'role': 'user', 'co...\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    current = None\n",
    "    messages = []\n",
    "\n",
    "    while text:\n",
    "        # first we check to see if the text is untagged\n",
    "        match_role = None\n",
    "        match = re.search(\n",
    "            rf\"(?P<content>\\s*.*?)\\s*(?P<tag>{tag_start}/?(?P<role>.*?){tag_end}\\s*|$)\",\n",
    "            text,\n",
    "        )\n",
    "        if match:\n",
    "            content = match.group(\"content\")\n",
    "            if match.group(\"tag\").startswith(f\"{tag_start}/\"):\n",
    "                raise Exception(f\"Found end tag with no start `{match.group('tag')}`.\")\n",
    "            if match.group(\"role\") is not None and match.group(\"role\") not in roles:\n",
    "                raise Exception(f\"Unknown role `{match.group('role')}`.\")\n",
    "            if content.strip():\n",
    "                messages.append({\"role\": default_role, \"content\": content.strip()})\n",
    "            text = text[match.span(\"tag\")[1] :]\n",
    "            match_role = match.group(\"role\")\n",
    "        if not text:\n",
    "            break\n",
    "        # now that we have defaulted any untagged text, we can handle the next tagged portion\n",
    "        match = re.search(\n",
    "            rf\"\\s*.*?(?P<tag>{tag_start}/?(?P<role>.*?){tag_end}\\s*)\", text\n",
    "        )\n",
    "        content = text[: match.span(\"tag\")[0]]\n",
    "        if (match_role is not None and match.group(\"role\") != match_role) or (\n",
    "            not match.group(\"tag\").startswith(f\"{tag_start}/\")\n",
    "        ):\n",
    "            raise Exception(\n",
    "                f\"Unclosed tag `{match_role}`. Found `{match.group('role')}`.\"\n",
    "            )\n",
    "        messages.append({\"role\": match_role, \"content\": content.strip()})\n",
    "        text = text[match.span(\"tag\")[1] :]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def strip_tags(\n",
    "    prompt: Prompt,\n",
    "    tag_start: str = \"%\",\n",
    "    tag_end: str = \"%\",\n",
    "    roles_seps: Dict[str, str] = {\n",
    "        \"system\": \"\",\n",
    "        \"user\": \"User: \",\n",
    "        \"assistant\": \"Assistant: \",\n",
    "    },\n",
    "    sep: str = \"\\n\",\n",
    ") -> Prompt:\n",
    "    messages = split_tags(\n",
    "        text=prompt, tag_start=tag_start, tag_end=tag_end, roles=list(roles_seps.keys())\n",
    "    )\n",
    "    return Prompt(\n",
    "        sep.join(\n",
    "            (roles_seps[message[\"role\"]] + message[\"content\"] for message in messages)\n",
    "        ),\n",
    "        prompt.completions,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9008f167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"import openai\\nimport tiktoken\\n\\n\\n@dataclass\\nclass OpenAIChat(Model):\\n    model_name: str = \\\"gpt-3.5-turbo\\\"\\n    supported_decodings: Set[DecodingStrategy] = frozenset(\\n        (\\n            DecodingStrategy.GREEDY,\\n            DecodingStrategy.SAMPLE,\\n        )\\n    )\\n    role_tag_start: str = \\\"%\\\"\\n    role_tag_end: str = \\\"%\\\"\\n    default_role: str = \\\"assistant\\\"\\n    allowed_roles: Set[str] = field(\\n        default_factory=lambda: {\\\"system\\\", \\\"user\\\", \\\"assistant\\\"}\\n    )\\n    max_retries: int = 10\\n    retry_sleep_time: float = 1.0\\n    max_token_selection: int = 300\\n\\n    def __post_init__(self):\\n        self._tokenizer = tiktoken.encoding_for_model(self.model_name)\\n        self.tokens = openai_tokens(self._tokenizer)\\n\\n    def encode(self, text: str) -> TokenIds:\\n        return self._tokenizer.encode(text)\\n\\n    def decode(self, ids: TokenIds) -> str:\\n        return self._tokenizer.decode(ids)\\n\\n    @property\\n    def eos_token_id(self) -> int:\\n        return self._tokenizer.eot_token\\n\\n    @property\\n    def bos_token_id(self) -> int:\\n        return self._tokenizer.encode_single_token(\\\"<|endofprompt|>\\\")\\n\\n    async def _generate(\\n        self,\\n        text: str,\\n        max_tokens: int = 1,\\n        selected_tokens: Optional[SelectedTokens] = None,\\n        decoder: Optional[Decoder] = None,\\n        timeout: float = 10.0,\\n    ) -> AsyncGenerator[str, None]:\\n        decoder = decoder or Decoder()\\n        if decoder.strategy not in self.supported_decodings:\\n            raise ValueError(\\n                f\\\"Unsupported decoding strategy for {self.__class__} model `{decoder.strategy}`.\\\"\\n            )\\n        messages = split_tags(\\n            text,\\n            self.role_tag_start,\\n            self.role_tag_end,\\n            self.default_role,\\n            self.allowed_roles,\\n        )\\n\\n        temperature = decoder.temperature\\n        top_p = decoder.top_p\\n        if decoder.strategy == DecodingStrategy.GREEDY:\\n            # try to make the sampling as deterministic as possible\\n            # to select only the one top token\\n            top_p = 0.01  # select only n tokens to get over .01, should virually always be a single token\\n            temperature = 0.0\\n\\n        selected_tokens = selected_tokens or []\\n        payload = {\\n            \\\"messages\\\": messages,\\n            \\\"logit_bias\\\": {str(token): 100 for token in selected_tokens},\\n            \\\"model\\\": self.model_name,\\n            \\\"max_tokens\\\": max_tokens,\\n            \\\"temperature\\\": temperature,\\n            \\\"top_p\\\": top_p,\\n        }\\n\\n        async with aiohttp.ClientSession() as session:\\n            openai.aiosession.set(session)\\n            completion_stream = await openai.ChatCompletion.acreate(\\n                **payload, stream=True\\n            )\\n            async for chat_completion in completion_stream:\\n                yield chat_completion\\n\\n    async def generate(\\n        self,\\n        text: str,\\n        max_tokens: int = 1,\\n        selected_tokens: Optional[SelectedTokens] = None,\\n        decoder: Optional[Decoder] = None,\\n        timeout: float = 10.0,\\n    ) -> AsyncGenerator[str, None]:\\n        if len(selected_tokens) > self.max_token_selection:\\n            warnings.warn(\\n                f\\\"Trying to mask {len(selected_tokens)} tokens which \\\"\\n                f\\\"is more than {self.max_token_selection} mask limit \\\"\\n                f\\\"of {self}. Consider stricter constraints. Will select\\\"\\n                \\\"lowest token ids up to this limit.\\\"\\n            )\\n            selected_tokens = list(selected_tokens)[: self.max_token_selection]\\n\\n        def result_handler(response):\\n            delta = response[\\\"choices\\\"][0][\\\"delta\\\"]\\n            return (\\n                \\\"\\\" if not \\\"content\\\" in delta else delta[\\\"content\\\"],  # content\\n                \\\"finish_reason\\\" in delta\\n                and delta[\\\"finish_reason\\\"] is not None,  # complete generation\\n            )\\n\\n        error = False\\n        retries = 0\\n        for retries in range(self.max_retries):\\n            async for chat_completion in self._generate(\\n                text=text,\\n                max_tokens=max_tokens,\\n                selected_tokens=selected_tokens,\\n                decoder=decoder,\\n                timeout=timeout,\\n            ):\\n                if \\\"error\\\" in chat_completion.keys():\\n                    message = chat_completion[\\\"error\\\"][\\\"message\\\"]\\n                    retry = retries < self.max_retries\\n                    retries += 1\\n                    warnings.warn(\\n                        \\\"OpenAI Chat Completion API raised an error: \\\\n\\\"\\n                        f\\\"MESSAGE: {message}\\\\n\\\"\\n                        f\\\"RETRYING {retries}\\\"\\n                        if retry\\n                        else \\\"\\\"\\n                    )\\n                    error = True\\n                    break\\n                else:\\n                    error = False\\n                    content, done = result_handler(chat_completion)\\n                    text += content\\n                    if content:\\n                        yield content\\n                    if done:\\n                        break\\n            if not error:\\n                break\";\n",
       "                var nbb_formatted_code = \"import openai\\nimport tiktoken\\n\\n\\n@dataclass\\nclass OpenAIChat(Model):\\n    model_name: str = \\\"gpt-3.5-turbo\\\"\\n    supported_decodings: Set[DecodingStrategy] = frozenset(\\n        (\\n            DecodingStrategy.GREEDY,\\n            DecodingStrategy.SAMPLE,\\n        )\\n    )\\n    role_tag_start: str = \\\"%\\\"\\n    role_tag_end: str = \\\"%\\\"\\n    default_role: str = \\\"assistant\\\"\\n    allowed_roles: Set[str] = field(\\n        default_factory=lambda: {\\\"system\\\", \\\"user\\\", \\\"assistant\\\"}\\n    )\\n    max_retries: int = 10\\n    retry_sleep_time: float = 1.0\\n    max_token_selection: int = 300\\n\\n    def __post_init__(self):\\n        self._tokenizer = tiktoken.encoding_for_model(self.model_name)\\n        self.tokens = openai_tokens(self._tokenizer)\\n\\n    def encode(self, text: str) -> TokenIds:\\n        return self._tokenizer.encode(text)\\n\\n    def decode(self, ids: TokenIds) -> str:\\n        return self._tokenizer.decode(ids)\\n\\n    @property\\n    def eos_token_id(self) -> int:\\n        return self._tokenizer.eot_token\\n\\n    @property\\n    def bos_token_id(self) -> int:\\n        return self._tokenizer.encode_single_token(\\\"<|endofprompt|>\\\")\\n\\n    async def _generate(\\n        self,\\n        text: str,\\n        max_tokens: int = 1,\\n        selected_tokens: Optional[SelectedTokens] = None,\\n        decoder: Optional[Decoder] = None,\\n        timeout: float = 10.0,\\n    ) -> AsyncGenerator[str, None]:\\n        decoder = decoder or Decoder()\\n        if decoder.strategy not in self.supported_decodings:\\n            raise ValueError(\\n                f\\\"Unsupported decoding strategy for {self.__class__} model `{decoder.strategy}`.\\\"\\n            )\\n        messages = split_tags(\\n            text,\\n            self.role_tag_start,\\n            self.role_tag_end,\\n            self.default_role,\\n            self.allowed_roles,\\n        )\\n\\n        temperature = decoder.temperature\\n        top_p = decoder.top_p\\n        if decoder.strategy == DecodingStrategy.GREEDY:\\n            # try to make the sampling as deterministic as possible\\n            # to select only the one top token\\n            top_p = 0.01  # select only n tokens to get over .01, should virually always be a single token\\n            temperature = 0.0\\n\\n        selected_tokens = selected_tokens or []\\n        payload = {\\n            \\\"messages\\\": messages,\\n            \\\"logit_bias\\\": {str(token): 100 for token in selected_tokens},\\n            \\\"model\\\": self.model_name,\\n            \\\"max_tokens\\\": max_tokens,\\n            \\\"temperature\\\": temperature,\\n            \\\"top_p\\\": top_p,\\n        }\\n\\n        async with aiohttp.ClientSession() as session:\\n            openai.aiosession.set(session)\\n            completion_stream = await openai.ChatCompletion.acreate(\\n                **payload, stream=True\\n            )\\n            async for chat_completion in completion_stream:\\n                yield chat_completion\\n\\n    async def generate(\\n        self,\\n        text: str,\\n        max_tokens: int = 1,\\n        selected_tokens: Optional[SelectedTokens] = None,\\n        decoder: Optional[Decoder] = None,\\n        timeout: float = 10.0,\\n    ) -> AsyncGenerator[str, None]:\\n        if len(selected_tokens) > self.max_token_selection:\\n            warnings.warn(\\n                f\\\"Trying to mask {len(selected_tokens)} tokens which \\\"\\n                f\\\"is more than {self.max_token_selection} mask limit \\\"\\n                f\\\"of {self}. Consider stricter constraints. Will select\\\"\\n                \\\"lowest token ids up to this limit.\\\"\\n            )\\n            selected_tokens = list(selected_tokens)[: self.max_token_selection]\\n\\n        def result_handler(response):\\n            delta = response[\\\"choices\\\"][0][\\\"delta\\\"]\\n            return (\\n                \\\"\\\" if not \\\"content\\\" in delta else delta[\\\"content\\\"],  # content\\n                \\\"finish_reason\\\" in delta\\n                and delta[\\\"finish_reason\\\"] is not None,  # complete generation\\n            )\\n\\n        error = False\\n        retries = 0\\n        for retries in range(self.max_retries):\\n            async for chat_completion in self._generate(\\n                text=text,\\n                max_tokens=max_tokens,\\n                selected_tokens=selected_tokens,\\n                decoder=decoder,\\n                timeout=timeout,\\n            ):\\n                if \\\"error\\\" in chat_completion.keys():\\n                    message = chat_completion[\\\"error\\\"][\\\"message\\\"]\\n                    retry = retries < self.max_retries\\n                    retries += 1\\n                    warnings.warn(\\n                        \\\"OpenAI Chat Completion API raised an error: \\\\n\\\"\\n                        f\\\"MESSAGE: {message}\\\\n\\\"\\n                        f\\\"RETRYING {retries}\\\"\\n                        if retry\\n                        else \\\"\\\"\\n                    )\\n                    error = True\\n                    break\\n                else:\\n                    error = False\\n                    content, done = result_handler(chat_completion)\\n                    text += content\\n                    if content:\\n                        yield content\\n                    if done:\\n                        break\\n            if not error:\\n                break\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class OpenAIChat(Model):\n",
    "    model_name: str = \"gpt-3.5-turbo\"\n",
    "    supported_decodings: Set[DecodingStrategy] = frozenset(\n",
    "        (\n",
    "            DecodingStrategy.GREEDY,\n",
    "            DecodingStrategy.SAMPLE,\n",
    "        )\n",
    "    )\n",
    "    role_tag_start: str = \"%\"\n",
    "    role_tag_end: str = \"%\"\n",
    "    default_role: str = \"assistant\"\n",
    "    allowed_roles: Set[str] = field(\n",
    "        default_factory=lambda: {\"system\", \"user\", \"assistant\"}\n",
    "    )\n",
    "    max_retries: int = 10\n",
    "    retry_sleep_time: float = 1.0\n",
    "    max_token_selection: int = 300\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self._tokenizer = tiktoken.encoding_for_model(self.model_name)\n",
    "        self.tokens = openai_tokens(self._tokenizer)\n",
    "\n",
    "    def encode(self, text: str) -> TokenIds:\n",
    "        return self._tokenizer.encode(text)\n",
    "\n",
    "    def decode(self, ids: TokenIds) -> str:\n",
    "        return self._tokenizer.decode(ids)\n",
    "\n",
    "    @property\n",
    "    def eos_token_id(self) -> int:\n",
    "        return self._tokenizer.eot_token\n",
    "\n",
    "    @property\n",
    "    def bos_token_id(self) -> int:\n",
    "        return self._tokenizer.encode_single_token(\"<|endofprompt|>\")\n",
    "\n",
    "    async def _generate(\n",
    "        self,\n",
    "        text: str,\n",
    "        max_tokens: int = 1,\n",
    "        selected_tokens: Optional[SelectedTokens] = None,\n",
    "        decoder: Optional[Decoder] = None,\n",
    "        timeout: float = 10.0,\n",
    "    ) -> AsyncGenerator[str, None]:\n",
    "        decoder = decoder or Decoder()\n",
    "        if decoder.strategy not in self.supported_decodings:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported decoding strategy for {self.__class__} model `{decoder.strategy}`.\"\n",
    "            )\n",
    "        messages = split_tags(\n",
    "            text,\n",
    "            self.role_tag_start,\n",
    "            self.role_tag_end,\n",
    "            self.default_role,\n",
    "            self.allowed_roles,\n",
    "        )\n",
    "\n",
    "        temperature = decoder.temperature\n",
    "        top_p = decoder.top_p\n",
    "        if decoder.strategy == DecodingStrategy.GREEDY:\n",
    "            # try to make the sampling as deterministic as possible\n",
    "            # to select only the one top token\n",
    "            top_p = 0.01  # select only n tokens to get over .01, should virually always be a single token\n",
    "            temperature = 0.0\n",
    "\n",
    "        selected_tokens = selected_tokens or []\n",
    "        payload = {\n",
    "            \"messages\": messages,\n",
    "            \"logit_bias\": {str(token): 100 for token in selected_tokens},\n",
    "            \"model\": self.model_name,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "        }\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            openai.aiosession.set(session)\n",
    "            completion_stream = await openai.ChatCompletion.acreate(\n",
    "                **payload, stream=True\n",
    "            )\n",
    "            async for chat_completion in completion_stream:\n",
    "                yield chat_completion\n",
    "\n",
    "    async def generate(\n",
    "        self,\n",
    "        text: str,\n",
    "        max_tokens: int = 1,\n",
    "        selected_tokens: Optional[SelectedTokens] = None,\n",
    "        decoder: Optional[Decoder] = None,\n",
    "        timeout: float = 10.0,\n",
    "    ) -> AsyncGenerator[str, None]:\n",
    "        if len(selected_tokens) > self.max_token_selection:\n",
    "            warnings.warn(\n",
    "                f\"Trying to mask {len(selected_tokens)} tokens which \"\n",
    "                f\"is more than {self.max_token_selection} mask limit \"\n",
    "                f\"of {self}. Consider stricter constraints. Will select\"\n",
    "                \"lowest token ids up to this limit.\"\n",
    "            )\n",
    "            selected_tokens = list(selected_tokens)[: self.max_token_selection]\n",
    "\n",
    "        def result_handler(response):\n",
    "            delta = response[\"choices\"][0][\"delta\"]\n",
    "            return (\n",
    "                \"\" if not \"content\" in delta else delta[\"content\"],  # content\n",
    "                \"finish_reason\" in delta\n",
    "                and delta[\"finish_reason\"] is not None,  # complete generation\n",
    "            )\n",
    "\n",
    "        error = False\n",
    "        retries = 0\n",
    "        for retries in range(self.max_retries):\n",
    "            async for chat_completion in self._generate(\n",
    "                text=text,\n",
    "                max_tokens=max_tokens,\n",
    "                selected_tokens=selected_tokens,\n",
    "                decoder=decoder,\n",
    "                timeout=timeout,\n",
    "            ):\n",
    "                if \"error\" in chat_completion.keys():\n",
    "                    message = chat_completion[\"error\"][\"message\"]\n",
    "                    retry = retries < self.max_retries\n",
    "                    retries += 1\n",
    "                    warnings.warn(\n",
    "                        \"OpenAI Chat Completion API raised an error: \\n\"\n",
    "                        f\"MESSAGE: {message}\\n\"\n",
    "                        f\"RETRYING {retries}\"\n",
    "                        if retry\n",
    "                        else \"\"\n",
    "                    )\n",
    "                    error = True\n",
    "                    break\n",
    "                else:\n",
    "                    error = False\n",
    "                    content, done = result_handler(chat_completion)\n",
    "                    text += content\n",
    "                    if content:\n",
    "                        yield content\n",
    "                    if done:\n",
    "                        break\n",
    "            if not error:\n",
    "                break\n",
    "\n",
    "@dataclass\n",
    "class OpenAICompletion(Model):\n",
    "    model_name: str = \"text-ada-001\"\n",
    "    supported_decodings: Set[DecodingStrategy] = frozenset(\n",
    "        (\n",
    "            DecodingStrategy.GREEDY,\n",
    "            DecodingStrategy.SAMPLE,\n",
    "        )\n",
    "    )\n",
    "    role_tag_start: str = \"%\"\n",
    "    role_tag_end: str = \"%\"\n",
    "    default_role: str = \"assistant\"\n",
    "    allowed_roles: Set[str] = field(\n",
    "        default_factory=lambda: {\"system\", \"user\", \"assistant\"}\n",
    "    )\n",
    "    max_retries: int = 10\n",
    "    retry_sleep_time: float = 1.0\n",
    "    max_token_selection: int = 300\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self._tokenizer = tiktoken.encoding_for_model(self.model_name)\n",
    "        self.tokens = openai_tokens(self._tokenizer)\n",
    "\n",
    "    def encode(self, text: str) -> TokenIds:\n",
    "        return self._tokenizer.encode(text)\n",
    "\n",
    "    def decode(self, ids: TokenIds) -> str:\n",
    "        return self._tokenizer.decode(ids)\n",
    "\n",
    "    @property\n",
    "    def eos_token_id(self) -> int:\n",
    "        return self._tokenizer.eot_token\n",
    "\n",
    "    @property\n",
    "    def bos_token_id(self) -> int:\n",
    "        return self._tokenizer.encode_single_token(\"<|endofprompt|>\")\n",
    "\n",
    "    async def _generate(\n",
    "        self,\n",
    "        text: str,\n",
    "        max_tokens: int = 1,\n",
    "        selected_tokens: Optional[SelectedTokens] = None,\n",
    "        decoder: Optional[Decoder] = None,\n",
    "        timeout: float = 10.0,\n",
    "    ) -> AsyncGenerator[str, None]:\n",
    "        decoder = decoder or Decoder()\n",
    "        if decoder.strategy not in self.supported_decodings:\n",
    "            raise ValueError(\n",
    "                f\"Unsupported decoding strategy for {self.__class__} model `{decoder.strategy}`.\"\n",
    "            )\n",
    "\n",
    "        temperature = decoder.temperature\n",
    "        top_p = decoder.top_p\n",
    "        if decoder.strategy == DecodingStrategy.GREEDY:\n",
    "            # try to make the sampling as deterministic as possible\n",
    "            # to select only the one top token\n",
    "            top_p = 0.01  # select only n tokens to get over .01, should virually always be a single token\n",
    "            temperature = 0.0\n",
    "\n",
    "        selected_tokens = selected_tokens or []\n",
    "        payload = {\n",
    "            \"prompt\": text,\n",
    "            \"logit_bias\": {str(token): 100 for token in selected_tokens},\n",
    "            \"model\": self.model_name,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "        }\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            openai.aiosession.set(session)\n",
    "            completion_stream = await openai.Completion.acreate(**payload, stream=True)\n",
    "\n",
    "            async for completion in completion_stream:\n",
    "                yield completion\n",
    "\n",
    "    async def generate(\n",
    "        self,\n",
    "        text: str,\n",
    "        max_tokens: int = 1,\n",
    "        selected_tokens: Optional[SelectedTokens] = None,\n",
    "        decoder: Optional[Decoder] = None,\n",
    "        timeout: float = 10.0,\n",
    "    ) -> AsyncGenerator[str, None]:\n",
    "        if len(selected_tokens) > self.max_token_selection:\n",
    "            warnings.warn(\n",
    "                f\"Trying to mask {len(selected_tokens)} tokens which \"\n",
    "                f\"is more than {self.max_token_selection} mask limit \"\n",
    "                f\"of {self}. Consider stricter constraints. Will select\"\n",
    "                \"lowest token ids up to this limit.\"\n",
    "            )\n",
    "            selected_tokens = list(selected_tokens)[: self.max_token_selection]\n",
    "\n",
    "        def result_handler(response):\n",
    "            delta = response.choices[0]\n",
    "            return (\n",
    "                \"\" if not \"text\" in delta else delta[\"text\"],  # content\n",
    "                \"finish_reason\" in delta\n",
    "                and delta[\"finish_reason\"] is not None,  # complete generation\n",
    "            )\n",
    "\n",
    "        error = False\n",
    "        retries = 0\n",
    "        for retries in range(self.max_retries):\n",
    "            async for completion in self._generate(\n",
    "                text=text,\n",
    "                max_tokens=max_tokens,\n",
    "                selected_tokens=selected_tokens,\n",
    "                decoder=decoder,\n",
    "                timeout=timeout,\n",
    "            ):\n",
    "                if \"error\" in completion.keys():\n",
    "                    message = completion[\"error\"][\"message\"]\n",
    "                    retry = retries < self.max_retries\n",
    "                    retries += 1\n",
    "                    warnings.warn(\n",
    "                        \"OpenAI Completion API raised an error: \\n\"\n",
    "                        f\"MESSAGE: {message}\\n\"\n",
    "                        f\"RETRYING {retries}\"\n",
    "                        if retry\n",
    "                        else \"\"\n",
    "                    )\n",
    "                    error = True\n",
    "                    break\n",
    "                else:\n",
    "                    error = False\n",
    "                    content, done = result_handler(completion)\n",
    "                    text += content\n",
    "                    if content:\n",
    "                        yield content\n",
    "                    if done:\n",
    "                        break\n",
    "            if not error:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6792d7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"openai.api_key = \\\"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\\\"\";\n",
       "                var nbb_formatted_code = \"openai.api_key = \\\"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\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8495a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"# oai = OpenAIChat()\\noai = OpenAICompletion()\";\n",
       "                var nbb_formatted_code = \"# oai = OpenAIChat()\\noai = OpenAICompletion()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# oai = OpenAIChat()\n",
    "oai = OpenAICompletion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c53918b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/0mv20zzj0ld26z8h0ngg3wn00000gn/T/ipykernel_95361/911037778.py:88: UserWarning: Trying to mask 995 tokens which is more than 300 mask limit of OpenAICompletion(model_name='text-ada-001', supported_decodings=frozenset({<DecodingStrategy.GREEDY: 'GREEDY'>, <DecodingStrategy.SAMPLE: 'SAMPLE'>}), role_tag_start='%', role_tag_end='%', default_role='assistant', allowed_roles={'system', 'assistant', 'user'}, max_retries=10, retry_sleep_time=1.0, max_token_selection=300). Consider stricter constraints. Will selectlowest token ids up to this limit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/var/folders/vl/0mv20zzj0ld26z8h0ngg3wn00000gn/T/ipykernel_95361/911037778.py\u001b[0m(76)\u001b[0;36m_generate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     74 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     75 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 76 \u001b[0;31m            \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcompletion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompletion_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     77 \u001b[0;31m                \u001b[0;32myield\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     78 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m/var/folders/vl/0mv20zzj0ld26z8h0ngg3wn00000gn/T/ipykernel_95361/911037778.py\u001b[0m(77)\u001b[0;36m_generate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     75 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     76 \u001b[0;31m            \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcompletion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompletion_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 77 \u001b[0;31m                \u001b[0;32myield\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     78 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     79 \u001b[0;31m    async def generate(\n",
      "\u001b[0m\n",
      "ipdb> completion\n",
      "<OpenAIObject text_completion id=cmpl-7ZSxtT2K5aL8SIKsWxXoio9SFjPfG at 0x2983bc220> JSON: {\n",
      "  \"id\": \"cmpl-7ZSxtT2K5aL8SIKsWxXoio9SFjPfG\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1688687177,\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"-\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"text-ada-001\"\n",
      "}\n",
      "ipdb> c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vl/0mv20zzj0ld26z8h0ngg3wn00000gn/T/ipykernel_95361/911037778.py:88: UserWarning: Trying to mask 994 tokens which is more than 300 mask limit of OpenAICompletion(model_name='text-ada-001', supported_decodings=frozenset({<DecodingStrategy.GREEDY: 'GREEDY'>, <DecodingStrategy.SAMPLE: 'SAMPLE'>}), role_tag_start='%', role_tag_end='%', default_role='assistant', allowed_roles={'system', 'assistant', 'user'}, max_retries=10, retry_sleep_time=1.0, max_token_selection=300). Consider stricter constraints. Will selectlowest token ids up to this limit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/var/folders/vl/0mv20zzj0ld26z8h0ngg3wn00000gn/T/ipykernel_95361/911037778.py\u001b[0m(76)\u001b[0;36m_generate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     74 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     75 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 76 \u001b[0;31m            \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcompletion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompletion_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     77 \u001b[0;31m                \u001b[0;32myield\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     78 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> n\n",
      "> \u001b[0;32m/var/folders/vl/0mv20zzj0ld26z8h0ngg3wn00000gn/T/ipykernel_95361/911037778.py\u001b[0m(77)\u001b[0;36m_generate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     75 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     76 \u001b[0;31m            \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcompletion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompletion_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 77 \u001b[0;31m                \u001b[0;32myield\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     78 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     79 \u001b[0;31m    async def generate(\n",
      "\u001b[0m\n",
      "ipdb> completion\n",
      "<OpenAIObject text_completion id=cmpl-7ZSy4l3iMdFFH6MJnGylRTWv9EY6R at 0x2983bd030> JSON: {\n",
      "  \"id\": \"cmpl-7ZSy4l3iMdFFH6MJnGylRTWv9EY6R\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1688687188,\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"36\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\"\n",
      "    }\n",
      "  ],\n",
      "  \"model\": \"text-ada-001\"\n",
      "}\n",
      "ipdb> c\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"prompt = await Prompt(\\n    \\\"We need to count some pigs. I have 48 and slaughtered 23, so now I have \\\"\\n).complete(\\n    model=oai,\\n    constraint=RegexConstraint(pattern=r\\\"-?[0-9]+\\\"),\\n    name=\\\"num_piggies\\\",\\n)\";\n",
       "                var nbb_formatted_code = \"prompt = await Prompt(\\n    \\\"We need to count some pigs. I have 48 and slaughtered 23, so now I have \\\"\\n).complete(\\n    model=oai,\\n    constraint=RegexConstraint(pattern=r\\\"-?[0-9]+\\\"),\\n    name=\\\"num_piggies\\\",\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = await Prompt(\n",
    "    \"We need to count some pigs. I have 48 and slaughtered 23, so now I have \"\n",
    ").complete(\n",
    "    model=oai,\n",
    "    constraint=RegexConstraint(pattern=r\"-?[0-9]+\"),\n",
    "    name=\"num_piggies\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "22c86d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 138;\n",
       "                var nbb_unformatted_code = \"isinstance(prompt, str)\";\n",
       "                var nbb_formatted_code = \"isinstance(prompt, str)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "isinstance(prompt, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "af0d33d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 144;\n",
       "                var nbb_unformatted_code = \"prompt+=\\\"%user%yoyoyoy%/user%\\\"\";\n",
       "                var nbb_formatted_code = \"prompt += \\\"%user%yoyoyoy%/user%\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt += \"%user%yoyoyoy%/user%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bdc7cadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 121;\n",
       "                var nbb_unformatted_code = \"%pdb\";\n",
       "                var nbb_formatted_code = \"%pdb\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3783d445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt('We need to count some pigs. I have 48 and slaughtered 23, so now I have -36')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 29;\n",
       "                var nbb_unformatted_code = \"prompt\";\n",
       "                var nbb_formatted_code = \"prompt\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2a0bb623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 154;\n",
       "                var nbb_unformatted_code = \"%pdb\";\n",
       "                var nbb_formatted_code = \"%pdb\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7a52ba5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt('Assistant: There are (3-2) =   1-1 =   1-1 =\n",
       "User: yoyoyoy')"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 162;\n",
       "                var nbb_unformatted_code = \"strip_tags(prompt)\";\n",
       "                var nbb_formatted_code = \"strip_tags(prompt)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "strip_tags(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bcfdffc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(strip_tags(prompt))                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">strip_tags</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">78</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">75 │   </span>sep: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span> = <span style=\"color: #808000; text-decoration-color: #808000\">\"\\n\"</span>,                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">76 </span>) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>:                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">77 │   </span>messages = split_tags(text, tag_start, tag_end, roles)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>78 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">sum</span>(                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">79 │   │   </span>(                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">80 │   │   │   </span>message[<span style=\"color: #808000; text-decoration-color: #808000\">\"content\"</span>] + (<span style=\"color: #808000; text-decoration-color: #808000\">\"\\n\"</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> i != (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(messages) - <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"\"</span>)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">81 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i, message <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">enumerate</span>(messages)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">sum</span><span style=\"font-weight: bold\">()</span> can't sum strings <span style=\"font-weight: bold\">[</span>use <span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.join</span><span style=\"font-weight: bold\">(</span>seq<span style=\"font-weight: bold\">)</span> instead<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 \u001b[96mtype\u001b[0m(strip_tags(prompt))                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mstrip_tags\u001b[0m:\u001b[94m78\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m75 \u001b[0m\u001b[2m│   \u001b[0msep: \u001b[96mstr\u001b[0m = \u001b[33m\"\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m,                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m76 \u001b[0m) -> \u001b[96mstr\u001b[0m:                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m77 \u001b[0m\u001b[2m│   \u001b[0mmessages = split_tags(text, tag_start, tag_end, roles)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m78 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96msum\u001b[0m(                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m79 \u001b[0m\u001b[2m│   │   \u001b[0m(                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m80 \u001b[0m\u001b[2m│   │   │   \u001b[0mmessage[\u001b[33m\"\u001b[0m\u001b[33mcontent\u001b[0m\u001b[33m\"\u001b[0m] + (\u001b[33m\"\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m \u001b[94mif\u001b[0m i != (\u001b[96mlen\u001b[0m(messages) - \u001b[94m1\u001b[0m) \u001b[94melse\u001b[0m \u001b[33m\"\u001b[0m\u001b[33m\"\u001b[0m)                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m81 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m i, message \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(messages)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35msum\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m can't sum strings \u001b[1m[\u001b[0muse \u001b[32m''\u001b[0m\u001b[1;35m.join\u001b[0m\u001b[1m(\u001b[0mseq\u001b[1m)\u001b[0m instead\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 132;\n",
       "                var nbb_unformatted_code = \"type(strip_tags(prompt))\";\n",
       "                var nbb_formatted_code = \"type(strip_tags(prompt))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "type(strip_tags(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d64f3f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Completions([], {'num_piggies': Completion(text = '25', start = 72, stop = 74)})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 77;\n",
       "                var nbb_unformatted_code = \"prompt.completions\";\n",
       "                var nbb_formatted_code = \"prompt.completions\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt.completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b3ec4d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 78;\n",
       "                var nbb_unformatted_code = \"int(prompt.completions.num_piggies)\";\n",
       "                var nbb_formatted_code = \"int(prompt.completions.num_piggies)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "int(prompt.completions.num_piggies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798e723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5a374eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      " there\n",
      "!\n",
      " How\n",
      " can\n",
      " I\n",
      " assist\n",
      " you\n",
      " today\n",
      "?\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 62;\n",
       "                var nbb_unformatted_code = \"async for tok in oai.generate(\\\"Hello\\\", 15):\\n    print(tok)\";\n",
       "                var nbb_formatted_code = \"async for tok in oai.generate(\\\"Hello\\\", 15):\\n    print(tok)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "async for tok in oai.generate(\"Hello\", 15):\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cb8d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"messages\": messages,\n",
    "    \"logit_bias\": {str(token): 100 for token in selected_tokens},\n",
    "    \"model\": self.model_name,\n",
    "    \"max_tokens\": self.chunksize,\n",
    "    \"temperature\": self.temperature,\n",
    "}\n",
    "chat_completion = await openai.ChatCompletion.acreate(**payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b5b5fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 35;\n",
       "                var nbb_unformatted_code = \"oai = OpenAIChat(chunksize=1, validate_completion_buffer=True)\";\n",
       "                var nbb_formatted_code = \"oai = OpenAIChat(chunksize=1, validate_completion_buffer=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "oai = OpenAIChat(chunksize=1, validate_completion_buffer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f0db79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIChat(model_name='gpt-3.5-turbo', chunksize=1, temperature=0.0, role_tag_start='%', role_tag_end='%', default_role='assistant', allowed_roles={'user', 'system', 'assistant'}, use_completion_buffer=True, validate_completion_buffer=True, clear_used_validated_buffer=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 36;\n",
       "                var nbb_unformatted_code = \"oai\";\n",
       "                var nbb_formatted_code = \"oai\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "oai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ef212c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 37;\n",
       "                var nbb_unformatted_code = \"prompt = \\\"\\\"\\\"\\n%system%\\nYou are a friendly bot\\n%/system%\\n%user%\\nwhat day did napolean die?\\n%/user%\\n\\\"\\\"\\\"\";\n",
       "                var nbb_formatted_code = \"prompt = \\\"\\\"\\\"\\n%system%\\nYou are a friendly bot\\n%/system%\\n%user%\\nwhat day did napolean die?\\n%/user%\\n\\\"\\\"\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "%system%\n",
    "You are a friendly bot\n",
    "%/system%\n",
    "%user%\n",
    "what day did napolean die?\n",
    "%/user%\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c450dc5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%system%\\nYou are a friendly bot\\n%/system%\\n%user%\\nwhat day did napolean die?\\n%/user%\\nN'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 38;\n",
       "                var nbb_unformatted_code = \"completion = await oai.sample(prompt)\\nprompt += completion\\n\\nprompt\";\n",
       "                var nbb_formatted_code = \"completion = await oai.sample(prompt)\\nprompt += completion\\n\\nprompt\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "completion = await oai.sample(prompt)\n",
    "prompt += completion\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21cf8611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 39;\n",
       "                var nbb_unformatted_code = \"prompt = Prompt(\\\"There are (3-2) = \\\")\";\n",
       "                var nbb_formatted_code = \"prompt = Prompt(\\\"There are (3-2) = \\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = Prompt(\"There are (3-2) = \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37a2fc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 40;\n",
       "                var nbb_unformatted_code = \"completion = await prompt.complete(oai, RegexConstraint(\\\"[0-9]\\\"), \\\"npigs\\\")\";\n",
       "                var nbb_formatted_code = \"completion = await prompt.complete(oai, RegexConstraint(\\\"[0-9]\\\"), \\\"npigs\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "completion = await prompt.complete(oai, RegexConstraint(\"[0-9]\"), \"npigs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fe85ecae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt('There are (3-2) = 1')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 54;\n",
       "                var nbb_unformatted_code = \"completion\";\n",
       "                var nbb_formatted_code = \"completion\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f35b25e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'npigs': Completion(text='1', start=18, stop=19)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 41;\n",
       "                var nbb_unformatted_code = \"completion.completions\";\n",
       "                var nbb_formatted_code = \"completion.completions\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "completion.completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8948c8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 42;\n",
       "                var nbb_unformatted_code = \"grammar = \\\"\\\"\\\"\\n?start: value\\n\\n    ?value: object\\n\\n          | array\\n\\n          | string\\n\\n          | \\\"true\\\"             -> true\\n\\n          | \\\"false\\\"            -> false\\n\\n          | \\\"null\\\"             -> null\\n\\n    array  : \\\"[\\\" [value (\\\",\\\" value)*] \\\"]\\\"\\n\\n    object : \\\"{\\\" [pair (\\\",\\\" pair)*] \\\"}\\\"\\n\\n    pair   : string \\\":\\\" value\\n\\n    string : ESCAPED_STRING\\n\\n    %import common.ESCAPED_STRING\\n\\n    %import common.SIGNED_NUMBER\\n\\n    %import common.WS\\n\\n    %ignore WS\\n\\\"\\\"\\\".strip()\";\n",
       "                var nbb_formatted_code = \"grammar = \\\"\\\"\\\"\\n?start: value\\n\\n    ?value: object\\n\\n          | array\\n\\n          | string\\n\\n          | \\\"true\\\"             -> true\\n\\n          | \\\"false\\\"            -> false\\n\\n          | \\\"null\\\"             -> null\\n\\n    array  : \\\"[\\\" [value (\\\",\\\" value)*] \\\"]\\\"\\n\\n    object : \\\"{\\\" [pair (\\\",\\\" pair)*] \\\"}\\\"\\n\\n    pair   : string \\\":\\\" value\\n\\n    string : ESCAPED_STRING\\n\\n    %import common.ESCAPED_STRING\\n\\n    %import common.SIGNED_NUMBER\\n\\n    %import common.WS\\n\\n    %ignore WS\\n\\\"\\\"\\\".strip()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grammar = \"\"\"\n",
    "?start: value\n",
    "\n",
    "    ?value: object\n",
    "\n",
    "          | array\n",
    "\n",
    "          | string\n",
    "\n",
    "          | \"true\"             -> true\n",
    "\n",
    "          | \"false\"            -> false\n",
    "\n",
    "          | \"null\"             -> null\n",
    "\n",
    "    array  : \"[\" [value (\",\" value)*] \"]\"\n",
    "\n",
    "    object : \"{\" [pair (\",\" pair)*] \"}\"\n",
    "\n",
    "    pair   : string \":\" value\n",
    "\n",
    "    string : ESCAPED_STRING\n",
    "\n",
    "    %import common.ESCAPED_STRING\n",
    "\n",
    "    %import common.SIGNED_NUMBER\n",
    "\n",
    "    %import common.WS\n",
    "\n",
    "    %ignore WS\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad3d5fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 43;\n",
       "                var nbb_unformatted_code = \"from lark import Lark, UnexpectedInput\";\n",
       "                var nbb_formatted_code = \"from lark import Lark, UnexpectedInput\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lark import Lark, UnexpectedInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1b8aab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 44;\n",
       "                var nbb_unformatted_code = \"parser = Lark(grammar)\";\n",
       "                var nbb_formatted_code = \"parser = Lark(grammar)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parser = Lark(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3898cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_terminal_regex(parser, model: \"Model\"):\n",
    "    regex_map = {}\n",
    "    for term in parser.terminals:#type: ignore\n",
    "        if term.pattern:\n",
    "            regex_map[term.name] = re.compile(term.pattern.to_regexp())\n",
    "    return regex_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a4a96d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 61;\n",
       "                var nbb_unformatted_code = \"def next_lex(input_str, parser):\\n    try:\\n        parser.parse(input_str)  # type: ignore\\n    except UnexpectedInput as e:\\n        expected_tokens = e.expected\\n        parser.last_expected = expected_tokens\\n        return expected_tokens\\n\\n    return []\";\n",
       "                var nbb_formatted_code = \"def next_lex(input_str, parser):\\n    try:\\n        parser.parse(input_str)  # type: ignore\\n    except UnexpectedInput as e:\\n        expected_tokens = e.expected\\n        parser.last_expected = expected_tokens\\n        return expected_tokens\\n\\n    return []\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def next_lex(input_str: str, parser: Lark):\n",
    "    try:\n",
    "        parser.parse(input_str)  # type: ignore\n",
    "    except UnexpectedInput as e:\n",
    "        expected_tokens = e.expected\n",
    "        parser.last_expected = expected_tokens\n",
    "        return expected_tokens\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23510c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RBRACE', 'ESCAPED_STRING']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 62;\n",
       "                var nbb_unformatted_code = \"next_lex(\\\"{\\\", parser)\";\n",
       "                var nbb_formatted_code = \"next_lex(\\\"{\\\", parser)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "next_lex(\"{\", parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4364bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TerminalDef('ESCAPED_STRING', '\".*?(?<!\\\\\\\\)(\\\\\\\\\\\\\\\\)*?\"'),\n",
       " TerminalDef('WS', '(?:[ \\t\\x0c\\r\\n])+'),\n",
       " TerminalDef('TRUE', 'true'),\n",
       " TerminalDef('FALSE', 'false'),\n",
       " TerminalDef('NULL', 'null'),\n",
       " TerminalDef('COMMA', ','),\n",
       " TerminalDef('LSQB', '\\\\['),\n",
       " TerminalDef('RSQB', '\\\\]'),\n",
       " TerminalDef('LBRACE', '\\\\{'),\n",
       " TerminalDef('RBRACE', '\\\\}'),\n",
       " TerminalDef('COLON', ':')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 40;\n",
       "                var nbb_unformatted_code = \"parser.terminals\";\n",
       "                var nbb_formatted_code = \"parser.terminals\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parser.terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed7914bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lark==1.1.5 in /Users/nick/miniconda3/lib/python3.10/site-packages (1.1.5)\r\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 48;\n",
       "                var nbb_unformatted_code = \"!pip install \\\"lark==1.1.5\\\"\";\n",
       "                var nbb_formatted_code = \"!pip install \\\"lark==1.1.5\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install \"lark==1.1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9cc2ae80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 64;\n",
       "                var nbb_unformatted_code = \"[\\\"a\\\"] == [\\\"a\\\"]\";\n",
       "                var nbb_formatted_code = \"[\\\"a\\\"] == [\\\"a\\\"]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "[\"a\"] == [\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee01fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
