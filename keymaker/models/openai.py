"""Models to use with any openai compatible api endpoints"""
import warnings
from dataclasses import dataclass
from functools import lru_cache
from typing import Any, AsyncGenerator, Dict, FrozenSet, List, Optional, Tuple

import aiohttp
import openai
import tiktoken
from tiktoken import Encoding

from keymaker.models.base import ChatModel, Model
from keymaker.types import Decoder, DecodingStrategy, SelectedTokens, TokenIds, Tokens
from keymaker.utils.chat import split_tags


@lru_cache(10)
def openai_tokens(tokenizer: Encoding) -> Tokens:
    vocab_len = len(tokenizer.token_byte_values())
    tokens = {i: tokenizer.decode([i]) for i in range(vocab_len - 1)}
    for i in range(vocab_len, tokenizer.max_token_value):
        tokens[i] = f"<|special_{i}|>"
    return tokens


@dataclass
class OpenAIChat(ChatModel):
    model_name: str = "gpt-3.5-turbo"
    supported_decodings: FrozenSet[DecodingStrategy] = frozenset(  # type: ignore
        (
            DecodingStrategy.GREEDY,
            DecodingStrategy.SAMPLE,
        ),
    )
    role_tag_start: str = "%"
    role_tag_end: str = "%"
    default_role: str = "user"
    allowed_roles: FrozenSet[str] = frozenset(("system", "user", "assistant"))
    max_retries: int = 10
    retry_sleep_time: float = 1.0
    max_token_selection: int = 300
    max_total_tokens: int = 4000
    sample_chunk_size: int = 12  # tokens generated by `generate` on behalf of `sample`

    def __post_init__(self):
        self._tokenizer = tiktoken.encoding_for_model(self.model_name)
        self.tokens = openai_tokens(self._tokenizer)
        self._all_token_ids = set(self.tokens.keys())

    def encode(self, text: str) -> TokenIds:
        return self._tokenizer.encode(text)

    def decode(self, ids: TokenIds) -> str:
        return self._tokenizer.decode(ids)

    @property
    def eos_token_id(self) -> int:
        return self._tokenizer.eot_token

    @property
    def bos_token_id(self) -> int:
        return self._tokenizer.encode_single_token("<|endofprompt|>")

    async def _generate(
        self,
        text: str,
        max_tokens: int = 1,
        logit_bias: Optional[Dict[str, int]] = None,
        decoder: Optional[Decoder] = None,
        timeout: float = 10.0,
    ) -> AsyncGenerator[Any, None]:
        decoder = decoder or Decoder()
        if decoder.strategy not in self.supported_decodings:
            raise ValueError(f"Unsupported decoding strategy for {self.__class__} model `{decoder.strategy}`.")

        messages = split_tags(
            text,
            self.role_tag_start,
            self.role_tag_end,
            self.default_role,
            self.allowed_roles,
        )

        if messages[-1]['role'] != self.default_role:
            messages.append({'role': self.default_role, 'content': " "})
        gen_kwargs = {}
        if temperature := decoder.temperature:
            gen_kwargs['temperature'] = temperature
        if top_p := decoder.top_p:
            gen_kwargs['top_p'] = top_p
        if top_k := decoder.top_k:
            gen_kwargs['top_k'] = top_k
        if decoder.strategy == DecodingStrategy.GREEDY:
            # try to make the sampling as deterministic as possible
            # to select only the one top token
            gen_kwargs['temperature'] = 0
            gen_kwargs['top_p'] = (
                1 / self.vocab_size
            )  # select only n tokens to get over 1/self.vocab_size, should always be a single token
            if (
                'top_k' in gen_kwargs
            ):  # non-openai yet openai-compliant apis may also support topk while the official api does not
                gen_kwargs['top_k'] = 1

        payload = {
            "messages": messages,
            "logit_bias": logit_bias,
            "model": self.model_name,
            "max_tokens": max_tokens,
            **gen_kwargs,  # type: ignore
        }

        async with aiohttp.ClientSession() as session:
            openai.aiosession.set(session)
            completion_stream = await openai.ChatCompletion.acreate(**payload, stream=True)
            async for chat_completion in completion_stream:
                yield chat_completion

    async def generate(  # type: ignore
        self,
        text: str,
        max_tokens: int = 1,
        selected_tokens: Optional[SelectedTokens] = None,
        decoder: Optional[Decoder] = None,
        timeout: float = 10.0,
    ) -> AsyncGenerator[Tuple[str, List[None]], None]:
        bias = 100
        # if there are more tokens to keep than ignore, invert the bias
        if selected_tokens and len(selected_tokens) > (self.vocab_size / 2):
            selected_tokens = self._all_token_ids - selected_tokens
            bias = -100

        if selected_tokens and len(selected_tokens) > self.max_token_selection:
            warnings.warn(
                f"Trying to mask {len(selected_tokens)} tokens which "
                f"is more than {self.max_token_selection} mask limit "
                f"of {self}. Consider stricter constraints. Will select"
                "lowest token ids up to this limit.",
            )
            selected_tokens = sorted(list(selected_tokens))[: self.max_token_selection]  # type: ignore
        logit_bias = {}
        if selected_tokens:
            for i, idx in enumerate(selected_tokens):
                logit_bias[str(idx)] = bias

        def result_handler(response):
            delta = response["choices"][0]["delta"]
            return (
                "" if "content" not in delta else delta["content"],  # content
                "finish_reason" in delta and delta["finish_reason"] is not None,  # complete generation
            )

        eos_token = self.decode([self.eos_token_id])
        error = False
        retries = 0
        for retries in range(self.max_retries):
            async for chat_completion in self._generate(
                text=text,
                max_tokens=max_tokens,
                logit_bias=logit_bias,
                decoder=decoder,
                timeout=timeout,
            ):
                if "error" in chat_completion.keys():
                    message = chat_completion["error"]["message"]
                    retry = retries < self.max_retries
                    retries += 1
                    warnings.warn(
                        "OpenAI Chat Completion API raised an error: \n" f"MESSAGE: {message}\n" f"RETRYING {retries}"
                        if retry
                        else "",
                    )
                    error = True
                    break
                else:
                    error = False
                    content, done = result_handler(chat_completion)
                    if content.startswith(eos_token):
                        break
                    if content:
                        yield (content, [None])
                    if done:
                        break
            if not error:
                break


@dataclass
class OpenAICompletion(Model):
    model_name: str = "text-ada-001"
    supported_decodings: FrozenSet[DecodingStrategy] = frozenset(  # type: ignore
        (
            DecodingStrategy.GREEDY,
            DecodingStrategy.SAMPLE,
        ),
    )
    role_tag_start: str = "%"
    role_tag_end: str = "%"
    default_role: str = "assistant"
    allowed_roles: FrozenSet[str] = frozenset(("system", "user", "assistant"))
    max_retries: int = 10
    retry_sleep_time: float = 1.0
    max_token_selection: int = 300
    max_total_tokens: int = 4000

    def __post_init__(self):
        self._tokenizer = tiktoken.encoding_for_model(self.model_name)
        self.tokens = openai_tokens(self._tokenizer)
        self._all_token_ids = set(self.tokens.keys())

    def encode(self, text: str) -> TokenIds:
        return self._tokenizer.encode(text)

    def decode(self, ids: TokenIds) -> str:
        return self._tokenizer.decode(ids)

    @property
    def eos_token_id(self) -> int:
        return self._tokenizer.eot_token

    @property
    def bos_token_id(self) -> int:
        return self._tokenizer.encode_single_token("<|endofprompt|>")

    async def _generate(
        self,
        text: str,
        max_tokens: int = 1,
        logit_bias: Optional[Dict[str, int]] = None,
        decoder: Optional[Decoder] = None,
        timeout: float = 10.0,
    ) -> AsyncGenerator[str, None]:
        decoder = decoder or Decoder()
        if decoder.strategy not in self.supported_decodings:
            raise ValueError(f"Unsupported decoding strategy for {self.__class__} model `{decoder.strategy}`.")

        gen_kwargs = {}
        if temperature := decoder.temperature:
            gen_kwargs['temperature'] = temperature
        if top_p := decoder.top_p:
            gen_kwargs['top_p'] = top_p
        if top_k := decoder.top_k:
            gen_kwargs['top_k'] = top_k
        if decoder.strategy == DecodingStrategy.GREEDY:
            # try to make the sampling as deterministic as possible
            # to select only the one top token
            gen_kwargs['top_p'] = (
                1 / self.vocab_size
            )  # select only n tokens to get over 1/self.vocab_size, should always be a single token
            if 'top_k' in gen_kwargs:
                gen_kwargs['top_k'] = 1
        payload = {"prompt": text, "logit_bias": logit_bias, "model": self.model_name, "max_tokens": max_tokens, "logprobs": 5, **gen_kwargs}  # type: ignore

        async with aiohttp.ClientSession() as session:
            openai.aiosession.set(session)
            completion_stream = await openai.Completion.acreate(**payload, stream=True)

            async for completion in completion_stream:
                yield completion

    async def generate(  # type: ignore
        self,
        text: str,
        max_tokens: int = 1,
        selected_tokens: Optional[SelectedTokens] = None,
        decoder: Optional[Decoder] = None,
        timeout: float = 10.0,
    ) -> AsyncGenerator[Any, None]:
        bias = 100
        if selected_tokens and len(selected_tokens) > (self.vocab_size / 2):
            selected_tokens = self._all_token_ids - selected_tokens
            bias = -100

        if selected_tokens and len(selected_tokens) > self.max_token_selection:
            warnings.warn(
                f"Trying to mask {len(selected_tokens)} tokens which "
                f"is more than {self.max_token_selection} mask limit "
                f"of {self}. Consider stricter constraints. Will select"
                "lowest token ids up to this limit.",
            )
            selected_tokens = sorted(list(selected_tokens))[: self.max_token_selection]  # type: ignore
        logit_bias = {}
        if selected_tokens:
            for i, idx in enumerate(selected_tokens):
                logit_bias[str(idx)] = bias

        def result_handler(response):
            delta = response.choices[0]
            return (
                ("", 1) if "text" not in delta else (delta["text"], delta['logprobs']['token_logprobs']),  # content
                "finish_reason" in delta and delta["finish_reason"] is not None,  # complete generation
            )

        eos_token = self.decode([self.eos_token_id])
        error = False
        retries = 0
        for retries in range(self.max_retries):
            async for completion in self._generate(
                text=text,
                max_tokens=max_tokens,
                logit_bias=logit_bias,
                decoder=decoder,
                timeout=timeout,
            ):
                if "error" in completion.keys():  # type: ignore
                    message = completion["error"]["message"]  # type: ignore
                    retry = retries < self.max_retries
                    retries += 1
                    warnings.warn(
                        "OpenAI Completion API raised an error: \n" f"MESSAGE: {message}\n" f"RETRYING {retries}"
                        if retry
                        else "",
                    )
                    error = True
                    break
                else:
                    error = False
                    (content, logprob), done = result_handler(completion)
                    if content.startswith(eos_token):
                        break
                    if content:
                        yield (content, list(logprob))
                    if done:
                        break
            if not error:
                break
