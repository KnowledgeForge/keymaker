{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa39223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab802ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"from enum import Enum\\nimport openai\\nfrom keymaker.models import chatgpt, gpt4, LlamaCpp, Huggingface, OpenAICompletion\\nfrom keymaker.constraints import JsonConstraint, RegexConstraint\\nfrom keymaker import Prompt, Completion\\nfrom keymaker.utils import strip_tags\";\n",
       "                var nbb_formatted_code = \"from enum import Enum\\nimport openai\\nfrom keymaker.models import chatgpt, gpt4, LlamaCpp, Huggingface, OpenAICompletion\\nfrom keymaker.constraints import JsonConstraint, RegexConstraint\\nfrom keymaker import Prompt, Completion\\nfrom keymaker.utils import strip_tags\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from enum import Enum\n",
    "import openai\n",
    "from keymaker.models import chatgpt, gpt4, LlamaCpp, Huggingface, OpenAICompletion\n",
    "from keymaker.constraints import JsonConstraint, RegexConstraint\n",
    "from keymaker import Prompt, Completion\n",
    "from keymaker.utils import strip_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c659bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"import json\\nwith open('config.json') as f:\\n    openai.api_key=json.loads(f.read())['OPENAI_API_KEY']\";\n",
       "                var nbb_formatted_code = \"import json\\n\\nwith open(\\\"config.json\\\") as f:\\n    openai.api_key = json.loads(f.read())[\\\"OPENAI_API_KEY\\\"]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"config.json\") as f:\n",
    "    openai.api_key = json.loads(f.read())[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1ae84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"async def print_stream(completion: Completion):\\n    print(repr(completion))\";\n",
       "                var nbb_formatted_code = \"async def print_stream(completion: Completion):\\n    print(repr(completion))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "async def print_stream(completion: Completion):\n",
    "    print(repr(completion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b175656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"prompt = Prompt(\\\"\\\"\\\"%system%\\nYou are an expert in designing novel superheroes.\\n%/system%%user%Give me a json design of a superhero similar to this:\\n{\\n    \\\"name\\\": \\\"Superman\\\",\\n    \\\"aliases\\\": [\\\"Clark Kent\\\", \\\"Man of Steel\\\"],\\n    \\\"powers\\\": {\\n        \\\"strength\\\": 100,\\n        \\\"speed\\\": 75,\\n        \\\"flight\\\": true\\n    },\\n    \\\"weaknesses\\\": [\\\"kryptonite\\\", \\\"magic\\\"]\\n}\\n\\nI want a superhero that has something to do with pandas. \\nI want his name to be pandaman. He is stronger and faster than superman. \\nGive him 3 super cool aliases too!\\nsomething like {\\\"name\\\": \\\"pandaman\\\", \\\"aliases\\\":...}\\n\\n\\nGenerate me a json for pandaman.\\n%/user%Here is a JSON design for that:\\n\\\"\\\"\\\")\";\n",
       "                var nbb_formatted_code = \"prompt = Prompt(\\n    \\\"\\\"\\\"%system%\\nYou are an expert in designing novel superheroes.\\n%/system%%user%Give me a json design of a superhero similar to this:\\n{\\n    \\\"name\\\": \\\"Superman\\\",\\n    \\\"aliases\\\": [\\\"Clark Kent\\\", \\\"Man of Steel\\\"],\\n    \\\"powers\\\": {\\n        \\\"strength\\\": 100,\\n        \\\"speed\\\": 75,\\n        \\\"flight\\\": true\\n    },\\n    \\\"weaknesses\\\": [\\\"kryptonite\\\", \\\"magic\\\"]\\n}\\n\\nI want a superhero that has something to do with pandas. \\nI want his name to be pandaman. He is stronger and faster than superman. \\nGive him 3 super cool aliases too!\\nsomething like {\\\"name\\\": \\\"pandaman\\\", \\\"aliases\\\":...}\\n\\n\\nGenerate me a json for pandaman.\\n%/user%Here is a JSON design for that:\\n\\\"\\\"\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = Prompt(\n",
    "    \"\"\"%system%\n",
    "You are an expert in designing novel superheroes.\n",
    "%/system%%user%Give me a json design of a superhero similar to this:\n",
    "{\n",
    "    \"name\": \"Superman\",\n",
    "    \"aliases\": [\"Clark Kent\", \"Man of Steel\"],\n",
    "    \"powers\": {\n",
    "        \"strength\": 100,\n",
    "        \"speed\": 75,\n",
    "        \"flight\": true\n",
    "    },\n",
    "    \"weaknesses\": [\"kryptonite\", \"magic\"]\n",
    "}\n",
    "\n",
    "I want a superhero that has something to do with pandas. \n",
    "I want his name to be pandaman. He is stronger and faster than superman. \n",
    "Give him 3 super cool aliases too!\n",
    "something like {\"name\": \"pandaman\", \"aliases\":...}\n",
    "\n",
    "\n",
    "Generate me a json for pandaman.\n",
    "%/user%Here is a JSON design for that:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de573baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"chat_model = chatgpt(sample_chunk_size=64)\";\n",
       "                var nbb_formatted_code = \"chat_model = chatgpt(sample_chunk_size=64)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_model = chatgpt(sample_chunk_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "740f7bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='{\n",
      "', start=4, stop=6, name=None, chunk=True, score=None)\n",
      "Completion(text='   ', start=5, stop=8, name=None, chunk=True, score=None)\n",
      "Completion(text=' \"', start=7, stop=9, name=None, chunk=True, score=None)\n",
      "Completion(text='name', start=11, stop=15, name=None, chunk=True, score=None)\n",
      "Completion(text='\":', start=13, stop=15, name=None, chunk=True, score=None)\n",
      "Completion(text=' \"', start=15, stop=17, name=None, chunk=True, score=None)\n",
      "Completion(text='P', start=16, stop=17, name=None, chunk=True, score=None)\n",
      "Completion(text='and', start=19, stop=22, name=None, chunk=True, score=None)\n",
      "Completion(text='aman', start=23, stop=27, name=None, chunk=True, score=None)\n",
      "Completion(text='\",\n",
      "', start=26, stop=29, name=None, chunk=True, score=None)\n",
      "Completion(text='   ', start=29, stop=32, name=None, chunk=True, score=None)\n",
      "Completion(text=' \"', start=31, stop=33, name=None, chunk=True, score=None)\n",
      "Completion(text='aliases', start=38, stop=45, name=None, chunk=True, score=None)\n",
      "Completion(text='\":', start=40, stop=42, name=None, chunk=True, score=None)\n",
      "Completion(text=' [\"', start=43, stop=46, name=None, chunk=True, score=None)\n",
      "Completion(text='B', start=44, stop=45, name=None, chunk=True, score=None)\n",
      "Completion(text='am', start=46, stop=48, name=None, chunk=True, score=None)\n",
      "Completion(text='boo', start=49, stop=52, name=None, chunk=True, score=None)\n",
      "Completion(text=' Fury', start=54, stop=59, name=None, chunk=True, score=None)\n",
      "Completion(text='\",', start=56, stop=58, name=None, chunk=True, score=None)\n",
      "Completion(text=' \"', start=58, stop=60, name=None, chunk=True, score=None)\n",
      "Completion(text='P', start=59, stop=60, name=None, chunk=True, score=None)\n",
      "Completion(text='anda', start=63, stop=67, name=None, chunk=True, score=None)\n",
      "Completion(text=' Power', start=69, stop=75, name=None, chunk=True, score=None)\n",
      "Completion(text='house', start=74, stop=79, name=None, chunk=True, score=None)\n",
      "Completion(text='\",', start=76, stop=78, name=None, chunk=True, score=None)\n",
      "Completion(text=' \"', start=78, stop=80, name=None, chunk=True, score=None)\n",
      "Completion(text='Master', start=84, stop=90, name=None, chunk=True, score=None)\n",
      "Completion(text=' of', start=87, stop=90, name=None, chunk=True, score=None)\n",
      "Completion(text=' Pand', start=92, stop=97, name=None, chunk=True, score=None)\n",
      "Completion(text='emon', start=96, stop=100, name=None, chunk=True, score=None)\n",
      "Completion(text='ium', start=99, stop=102, name=None, chunk=True, score=None)\n",
      "Completion(text='\"],\n",
      "', start=103, stop=107, name=None, chunk=True, score=None)\n",
      "Completion(text='   ', start=106, stop=109, name=None, chunk=True, score=None)\n",
      "Completion(text=' \"', start=108, stop=110, name=None, chunk=True, score=None)\n",
      "Completion(text='powers', start=114, stop=120, name=None, chunk=True, score=None)\n",
      "Completion(text='\":', start=116, stop=118, name=None, chunk=True, score=None)\n",
      "Completion(text=' {\n",
      "', start=119, stop=122, name=None, chunk=True, score=None)\n",
      "Completion(text='       ', start=126, stop=133, name=None, chunk=True, score=None)\n",
      "Completion(text=' \"', start=128, stop=130, name=None, chunk=True, score=None)\n",
      "Completion(text='strength', start=136, stop=144, name=None, chunk=True, score=None)\n",
      "Completion(text='\":', start=138, stop=140, name=None, chunk=True, score=None)\n",
      "Completion(text=' ', start=139, stop=140, name=None, chunk=True, score=None)\n",
      "Completion(text='150', start=142, stop=145, name=None, chunk=True, score=None)\n",
      "Completion(text=',\n",
      "', start=144, stop=146, name=None, chunk=True, score=None)\n",
      "Completion(text='       ', start=151, stop=158, name=None, chunk=True, score=None)\n",
      "Completion(text=' \"', start=153, stop=155, name=None, chunk=True, score=None)\n",
      "Completion(text='speed', start=158, stop=163, name=None, chunk=True, score=None)\n",
      "Completion(text='\":', start=160, stop=162, name=None, chunk=True, score=None)\n",
      "Completion(text=' ', start=161, stop=162, name=None, chunk=True, score=None)\n",
      "Completion(text='100', start=164, stop=167, name=None, chunk=True, score=None)\n",
      "Completion(text=',\n",
      "', start=166, stop=168, name=None, chunk=True, score=None)\n",
      "Completion(text='       ', start=173, stop=180, name=None, chunk=True, score=None)\n",
      "Completion(text=' \"', start=175, stop=177, name=None, chunk=True, score=None)\n",
      "Completion(text='ag', start=177, stop=179, name=None, chunk=True, score=None)\n",
      "Completion(text='ility', start=182, stop=187, name=None, chunk=True, score=None)\n",
      "Completion(text='\":', start=184, stop=186, name=None, chunk=True, score=None)\n",
      "Completion(text=' ', start=185, stop=186, name=None, chunk=True, score=None)\n",
      "Completion(text='90', start=187, stop=189, name=None, chunk=True, score=None)\n",
      "Completion(text=',\n",
      "', start=189, stop=191, name=None, chunk=True, score=None)\n",
      "Completion(text='       ', start=196, stop=203, name=None, chunk=True, score=None)\n",
      "Completion(text=' \"', start=198, stop=200, name=None, chunk=True, score=None)\n",
      "Completion(text='flight', start=204, stop=210, name=None, chunk=True, score=None)\n",
      "Completion(text='\":', start=206, stop=208, name=None, chunk=True, score=None)\n",
      "Completion(text='false', start=216, stop=221, name=None, chunk=True, score=None)\n",
      "Completion(text='\n",
      "', start=212, stop=213, name=None, chunk=True, score=None)\n",
      "Completion(text='   ', start=215, stop=218, name=None, chunk=True, score=None)\n",
      "Completion(text=' },\n",
      "', start=219, stop=223, name=None, chunk=True, score=None)\n",
      "Completion(text='   ', start=222, stop=225, name=None, chunk=True, score=None)\n",
      "Completion(text=' \"', start=224, stop=226, name=None, chunk=True, score=None)\n",
      "Completion(text='weak', start=228, stop=232, name=None, chunk=True, score=None)\n",
      "Completion(text='ness', start=232, stop=236, name=None, chunk=True, score=None)\n",
      "Completion(text='es', start=234, stop=236, name=None, chunk=True, score=None)\n",
      "Completion(text='\":', start=236, stop=238, name=None, chunk=True, score=None)\n",
      "Completion(text=' [\"', start=239, stop=242, name=None, chunk=True, score=None)\n",
      "Completion(text='bam', start=242, stop=245, name=None, chunk=True, score=None)\n",
      "Completion(text='boo', start=245, stop=248, name=None, chunk=True, score=None)\n",
      "Completion(text=' shortage', start=254, stop=263, name=None, chunk=True, score=None)\n",
      "Completion(text='\",', start=256, stop=258, name=None, chunk=True, score=None)\n",
      "Completion(text=' \"', start=258, stop=260, name=None, chunk=True, score=None)\n",
      "Completion(text='fire', start=262, stop=266, name=None, chunk=True, score=None)\n",
      "Completion(text='\"]\n",
      "', start=265, stop=268, name=None, chunk=True, score=None)\n",
      "None\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"chat_completed_prompt = await prompt.complete(model=chat_model, constraint=JsonConstraint(), stream = print_stream)\";\n",
       "                var nbb_formatted_code = \"chat_completed_prompt = await prompt.complete(\\n    model=chat_model, constraint=JsonConstraint(), stream=print_stream\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_completed_prompt = await prompt.complete(\n",
    "    model=chat_model, constraint=JsonConstraint(), stream=print_stream\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00bd5699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%system%\n",
      "You are an expert in designing novel superheroes.\n",
      "%/system%%user%Give me a json design of a superhero similar to this:\n",
      "{\n",
      "    \"name\": \"Superman\",\n",
      "    \"aliases\": [\"Clark Kent\", \"Man of Steel\"],\n",
      "    \"powers\": {\n",
      "        \"strength\": 100,\n",
      "        \"speed\": 75,\n",
      "        \"flight\": true\n",
      "    },\n",
      "    \"weaknesses\": [\"kryptonite\", \"magic\"]\n",
      "}\n",
      "\n",
      "I want a superhero that has something to do with pandas. \n",
      "I want his name to be pandaman. He is stronger and faster than superman. \n",
      "Give him 3 super cool aliases too!\n",
      "something like {\"name\": \"pandaman\", \"aliases\":...}\n",
      "\n",
      "\n",
      "Generate me a json for pandaman.\n",
      "%/user%Here is a JSON design for that:\n",
      "{\n",
      "    \"name\": \"Pandaman\",\n",
      "    \"aliases\": [\"Bamboo Fury\", \"Panda Powerhouse\", \"Master of Pandemonium\"],\n",
      "    \"powers\": {\n",
      "        \"strength\": 150,\n",
      "        \"speed\": 100,\n",
      "        \"agility\": 90,\n",
      "        \"flight\":false\n",
      "    },\n",
      "    \"weaknesses\": [\"bamboo shortage\", \"fire\"]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"print(chat_completed_prompt)\";\n",
       "                var nbb_formatted_code = \"print(chat_completed_prompt)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(chat_completed_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "098ce850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/nick/Downloads/llama-2-7b-chat.ggmlv3.q3_K_S.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 11 (mostly Q3_K - Small)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 4603.09 MB (+ 1026.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"llama_model = LlamaCpp(model_path=\\\"/Users/nick/Downloads/llama-2-7b-chat.ggmlv3.q3_K_S.bin\\\")\";\n",
       "                var nbb_formatted_code = \"llama_model = LlamaCpp(\\n    model_path=\\\"/Users/nick/Downloads/llama-2-7b-chat.ggmlv3.q3_K_S.bin\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_model = LlamaCpp(\n",
    "    model_path=\"/Users/nick/Downloads/llama-2-7b-chat.ggmlv3.q3_K_S.bin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d8c75ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1466.28 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12535.39 ms /   212 tokens (   59.13 ms per token,    16.91 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time = 12803.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1449.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   130.39 ms /     1 runs   (  130.39 ms per token,     7.67 tokens per second)\n",
      "llama_print_timings:       total time =   134.55 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='{', start=2, stop=3, name=llama, chunk=True, score=9.999999999999996e-11)\n",
      "Completion(text='\n",
      "', start=3, stop=4, name=llama, chunk=True, score=5.236759847354896e-18)\n",
      "Completion(text='   ', start=8, stop=11, name=llama, chunk=True, score=5.236759847354875e-28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   107.42 ms /     1 runs   (  107.42 ms per token,     9.31 tokens per second)\n",
      "llama_print_timings:       total time =   111.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1447.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   114.23 ms /     1 runs   (  114.23 ms per token,     8.75 tokens per second)\n",
      "llama_print_timings:       total time =   118.24 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1455.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    81.78 ms /     1 runs   (   81.78 ms per token,    12.23 tokens per second)\n",
      "llama_print_timings:       total time =    87.08 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=9, stop=11, name=llama, chunk=True, score=5.236759847354854e-38)\n",
      "Completion(text='name', start=15, stop=19, name=llama, chunk=True, score=3.531428452546688e-46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1418.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   104.56 ms /     1 runs   (  104.56 ms per token,     9.56 tokens per second)\n",
      "llama_print_timings:       total time =   110.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\"', start=13, stop=14, name=llama, chunk=True, score=4.803124910196184e-55)\n",
      "Completion(text=' :', start=16, stop=18, name=llama, chunk=True, score=4.803124910196234e-65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1408.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   113.78 ms /     1 runs   (  113.78 ms per token,     8.79 tokens per second)\n",
      "llama_print_timings:       total time =   118.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1466.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    96.73 ms /     1 runs   (   96.73 ms per token,    10.34 tokens per second)\n",
      "llama_print_timings:       total time =   100.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1447.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    74.72 ms /     1 runs   (   74.72 ms per token,    13.38 tokens per second)\n",
      "llama_print_timings:       total time =    80.13 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=18, stop=20, name=llama, chunk=True, score=4.803124910196283e-75)\n",
      "Completion(text='P', start=18, stop=19, name=llama, chunk=True, score=7.208390736769787e-85)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1461.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    92.50 ms /     1 runs   (   92.50 ms per token,    10.81 tokens per second)\n",
      "llama_print_timings:       total time =    97.92 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='and', start=23, stop=26, name=llama, chunk=True, score=1.1538517924886474e-93)\n",
      "Completion(text='aman', start=28, stop=32, name=llama, chunk=True, score=1.1545860258891805e-103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1447.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    96.37 ms /     1 runs   (   96.37 ms per token,    10.38 tokens per second)\n",
      "llama_print_timings:       total time =   101.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1466.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.42 ms /     1 runs   (   82.42 ms per token,    12.13 tokens per second)\n",
      "llama_print_timings:       total time =    87.84 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\"', start=26, stop=27, name=llama, chunk=True, score=4.952248469014866e-112)\n",
      "Completion(text=' ,', start=29, stop=31, name=llama, chunk=True, score=4.952248469014776e-122)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   109.26 ms /     1 runs   (  109.26 ms per token,     9.15 tokens per second)\n",
      "llama_print_timings:       total time =   113.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1449.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    93.45 ms /     1 runs   (   93.45 ms per token,    10.70 tokens per second)\n",
      "llama_print_timings:       total time =    97.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1468.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    85.59 ms /     1 runs   (   85.59 ms per token,    11.68 tokens per second)\n",
      "llama_print_timings:       total time =    89.59 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=29, stop=30, name=llama, chunk=True, score=4.438063790523782e-129)\n",
      "Completion(text='   ', start=34, stop=37, name=llama, chunk=True, score=4.438063790523701e-139)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1459.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    85.72 ms /     1 runs   (   85.72 ms per token,    11.67 tokens per second)\n",
      "llama_print_timings:       total time =    89.85 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1449.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    87.33 ms /     1 runs   (   87.33 ms per token,    11.45 tokens per second)\n",
      "llama_print_timings:       total time =    92.70 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=35, stop=37, name=llama, chunk=True, score=4.4380637905236205e-149)\n",
      "Completion(text='ali', start=39, stop=42, name=llama, chunk=True, score=6.621441923261401e-157)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    84.92 ms /     1 runs   (   84.92 ms per token,    11.78 tokens per second)\n",
      "llama_print_timings:       total time =    90.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='ases', start=44, stop=48, name=llama, chunk=True, score=1.4249749239662437e-166)\n",
      "Completion(text='\"', start=42, stop=43, name=llama, chunk=True, score=1.8015369160955427e-176)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1459.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    80.74 ms /     1 runs   (   80.74 ms per token,    12.39 tokens per second)\n",
      "llama_print_timings:       total time =    86.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1472.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    89.78 ms /     1 runs   (   89.78 ms per token,    11.14 tokens per second)\n",
      "llama_print_timings:       total time =    93.82 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' :', start=45, stop=47, name=llama, chunk=True, score=1.80153691609551e-186)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1404.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.03 ms /     1 runs   (   88.03 ms per token,    11.36 tokens per second)\n",
      "llama_print_timings:       total time =    92.08 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' [', start=47, stop=49, name=llama, chunk=True, score=1.8015369160954773e-196)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1455.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    86.79 ms /     1 runs   (   86.79 ms per token,    11.52 tokens per second)\n",
      "llama_print_timings:       total time =    90.79 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=47, stop=48, name=llama, chunk=True, score=1.6943326034609792e-201)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    78.98 ms /     1 runs   (   78.98 ms per token,    12.66 tokens per second)\n",
      "llama_print_timings:       total time =    83.19 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='       ', start=60, stop=67, name=llama, chunk=True, score=1.6943326034609484e-211)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1459.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    79.80 ms /     1 runs   (   79.80 ms per token,    12.53 tokens per second)\n",
      "llama_print_timings:       total time =    83.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' {', start=57, stop=59, name=llama, chunk=True, score=1.6943326034609176e-221)\n",
      "Completion(text='\n",
      "', start=57, stop=58, name=llama, chunk=True, score=1.4243140931119115e-227)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.44 ms /     1 runs   (   88.44 ms per token,    11.31 tokens per second)\n",
      "llama_print_timings:       total time =    92.42 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1472.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    91.88 ms /     1 runs   (   91.88 ms per token,    10.88 tokens per second)\n",
      "llama_print_timings:       total time =    95.80 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='           ', start=78, stop=89, name=llama, chunk=True, score=1.4243140931118857e-237)\n",
      "Completion(text=' \"', start=71, stop=73, name=llama, chunk=True, score=1.4243140931118597e-247)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1451.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    89.56 ms /     1 runs   (   89.56 ms per token,    11.17 tokens per second)\n",
      "llama_print_timings:       total time =    93.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1455.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    97.25 ms /     1 runs   (   97.25 ms per token,    10.28 tokens per second)\n",
      "llama_print_timings:       total time =   102.63 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='name', start=77, stop=81, name=llama, chunk=True, score=1.673007003692675e-255)\n",
      "Completion(text='\"', start=75, stop=76, name=llama, chunk=True, score=1.8797001771348192e-265)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1461.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    91.23 ms /     1 runs   (   91.23 ms per token,    10.96 tokens per second)\n",
      "llama_print_timings:       total time =    96.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1404.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   114.98 ms /     1 runs   (  114.98 ms per token,     8.70 tokens per second)\n",
      "llama_print_timings:       total time =   118.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' :', start=78, stop=80, name=llama, chunk=True, score=1.879700177134785e-275)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1451.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    89.67 ms /     1 runs   (   89.67 ms per token,    11.15 tokens per second)\n",
      "llama_print_timings:       total time =    93.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1436.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    80.77 ms /     1 runs   (   80.77 ms per token,    12.38 tokens per second)\n",
      "llama_print_timings:       total time =    86.11 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=80, stop=82, name=llama, chunk=True, score=1.8797001771347508e-285)\n",
      "Completion(text='Pan', start=84, stop=87, name=llama, chunk=True, score=3.507580534824061e-294)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1438.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    89.84 ms /     1 runs   (   89.84 ms per token,    11.13 tokens per second)\n",
      "llama_print_timings:       total time =    95.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='ther', start=89, stop=93, name=llama, chunk=True, score=1.9519108103294316e-299)\n",
      "Completion(text=' P', start=89, stop=91, name=llama, chunk=True, score=3.0699936803036063e-304)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1443.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    81.40 ms /     1 runs   (   81.40 ms per token,    12.29 tokens per second)\n",
      "llama_print_timings:       total time =    86.89 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1445.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    99.12 ms /     1 runs   (   99.12 ms per token,    10.09 tokens per second)\n",
      "llama_print_timings:       total time =   104.47 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='aw', start=91, stop=93, name=llama, chunk=True, score=1.0194979342566e-310)\n",
      "Completion(text='z', start=91, stop=92, name=llama, chunk=True, score=8.334e-319)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.32 ms /     1 runs   (   82.32 ms per token,    12.15 tokens per second)\n",
      "llama_print_timings:       total time =    87.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    86.22 ms /     1 runs   (   86.22 ms per token,    11.60 tokens per second)\n",
      "llama_print_timings:       total time =    91.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    81.53 ms /     1 runs   (   81.53 ms per token,    12.27 tokens per second)\n",
      "llama_print_timings:       total time =    86.97 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='illa', start=98, stop=102, name=llama, chunk=True, score=5e-324)\n",
      "Completion(text='\"', start=96, stop=97, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    97.14 ms /     1 runs   (   97.14 ms per token,    10.29 tokens per second)\n",
      "llama_print_timings:       total time =   101.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' ,', start=99, stop=101, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\n",
      "', start=99, stop=100, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1443.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    90.37 ms /     1 runs   (   90.37 ms per token,    11.07 tokens per second)\n",
      "llama_print_timings:       total time =    94.51 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   100.06 ms /     1 runs   (  100.06 ms per token,     9.99 tokens per second)\n",
      "llama_print_timings:       total time =   104.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='           ', start=120, stop=131, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' \"', start=113, stop=115, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    92.08 ms /     1 runs   (   92.08 ms per token,    10.86 tokens per second)\n",
      "llama_print_timings:       total time =    96.08 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    92.79 ms /     1 runs   (   92.79 ms per token,    10.78 tokens per second)\n",
      "llama_print_timings:       total time =    98.24 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='description', start=133, stop=144, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\"', start=124, stop=125, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1451.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    91.56 ms /     1 runs   (   91.56 ms per token,    10.92 tokens per second)\n",
      "llama_print_timings:       total time =    97.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1443.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    89.61 ms /     1 runs   (   89.61 ms per token,    11.16 tokens per second)\n",
      "llama_print_timings:       total time =    93.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' :', start=127, stop=129, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1447.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    90.56 ms /     1 runs   (   90.56 ms per token,    11.04 tokens per second)\n",
      "llama_print_timings:       total time =    94.63 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1451.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.90 ms /     1 runs   (   88.90 ms per token,    11.25 tokens per second)\n",
      "llama_print_timings:       total time =    94.32 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=129, stop=131, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='P', start=129, stop=130, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1459.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    84.79 ms /     1 runs   (   84.79 ms per token,    11.79 tokens per second)\n",
      "llama_print_timings:       total time =    90.31 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='and', start=134, stop=137, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='aman', start=139, stop=143, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1459.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.86 ms /     1 runs   (   82.86 ms per token,    12.07 tokens per second)\n",
      "llama_print_timings:       total time =    88.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1459.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    77.45 ms /     1 runs   (   77.45 ms per token,    12.91 tokens per second)\n",
      "llama_print_timings:       total time =    82.91 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=''', start=137, stop=138, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='s', start=138, stop=139, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1468.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    84.14 ms /     1 runs   (   84.14 ms per token,    11.88 tokens per second)\n",
      "llama_print_timings:       total time =    89.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1449.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    90.39 ms /     1 runs   (   90.39 ms per token,    11.06 tokens per second)\n",
      "llama_print_timings:       total time =    95.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1436.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    92.90 ms /     1 runs   (   92.90 ms per token,    10.76 tokens per second)\n",
      "llama_print_timings:       total time =    98.40 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' alter', start=149, stop=155, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' e', start=147, stop=149, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    79.95 ms /     1 runs   (   79.95 ms per token,    12.51 tokens per second)\n",
      "llama_print_timings:       total time =    85.60 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='go', start=149, stop=151, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='.', start=149, stop=150, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    90.44 ms /     1 runs   (   90.44 ms per token,    11.06 tokens per second)\n",
      "llama_print_timings:       total time =    95.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1468.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    76.23 ms /     1 runs   (   76.23 ms per token,    13.12 tokens per second)\n",
      "llama_print_timings:       total time =    81.73 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' A', start=152, stop=154, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' sle', start=158, stop=162, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1447.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    81.25 ms /     1 runs   (   81.25 ms per token,    12.31 tokens per second)\n",
      "llama_print_timings:       total time =    86.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    95.27 ms /     1 runs   (   95.27 ms per token,    10.50 tokens per second)\n",
      "llama_print_timings:       total time =   100.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1464.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    79.74 ms /     1 runs   (   79.74 ms per token,    12.54 tokens per second)\n",
      "llama_print_timings:       total time =    85.14 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='ek', start=158, stop=160, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' and', start=164, stop=168, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.26 ms /     1 runs   (   88.26 ms per token,    11.33 tokens per second)\n",
      "llama_print_timings:       total time =    93.81 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' ste', start=168, stop=172, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='alth', start=172, stop=176, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1461.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    87.45 ms /     1 runs   (   87.45 ms per token,    11.43 tokens per second)\n",
      "llama_print_timings:       total time =    92.99 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1468.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    79.05 ms /     1 runs   (   79.05 ms per token,    12.65 tokens per second)\n",
      "llama_print_timings:       total time =    84.52 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='y', start=170, stop=171, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' hero', start=179, stop=184, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1443.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    90.84 ms /     1 runs   (   90.84 ms per token,    11.01 tokens per second)\n",
      "llama_print_timings:       total time =    96.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1440.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    92.35 ms /     1 runs   (   92.35 ms per token,    10.83 tokens per second)\n",
      "llama_print_timings:       total time =    97.85 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1461.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    81.00 ms /     1 runs   (   81.00 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:       total time =    86.48 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' who', start=182, stop=186, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' uses', start=188, stop=193, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1420.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   101.25 ms /     1 runs   (  101.25 ms per token,     9.88 tokens per second)\n",
      "llama_print_timings:       total time =   107.03 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' his', start=191, stop=195, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' quick', start=199, stop=205, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1447.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    90.54 ms /     1 runs   (   90.54 ms per token,    11.04 tokens per second)\n",
      "llama_print_timings:       total time =    96.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    79.72 ms /     1 runs   (   79.72 ms per token,    12.54 tokens per second)\n",
      "llama_print_timings:       total time =    85.20 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' ref', start=201, stop=205, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='lex', start=203, stop=206, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1455.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    90.52 ms /     1 runs   (   90.52 ms per token,    11.05 tokens per second)\n",
      "llama_print_timings:       total time =    96.02 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1459.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    84.93 ms /     1 runs   (   84.93 ms per token,    11.77 tokens per second)\n",
      "llama_print_timings:       total time =    90.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    84.69 ms /     1 runs   (   84.69 ms per token,    11.81 tokens per second)\n",
      "llama_print_timings:       total time =    90.21 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='es', start=204, stop=206, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' and', start=210, stop=214, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1455.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    81.88 ms /     1 runs   (   81.88 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =    87.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' ag', start=212, stop=215, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='ility', start=219, stop=224, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1451.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    94.25 ms /     1 runs   (   94.25 ms per token,    10.61 tokens per second)\n",
      "llama_print_timings:       total time =    99.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    89.82 ms /     1 runs   (   89.82 ms per token,    11.13 tokens per second)\n",
      "llama_print_timings:       total time =    95.38 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' to', start=220, stop=223, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' out', start=225, stop=229, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1443.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    78.33 ms /     1 runs   (   78.33 ms per token,    12.77 tokens per second)\n",
      "llama_print_timings:       total time =    83.89 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1449.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    89.24 ms /     1 runs   (   89.24 ms per token,    11.21 tokens per second)\n",
      "llama_print_timings:       total time =    94.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1459.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    81.84 ms /     1 runs   (   81.84 ms per token,    12.22 tokens per second)\n",
      "llama_print_timings:       total time =    87.35 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='man', start=227, stop=230, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='eu', start=228, stop=230, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1440.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    91.98 ms /     1 runs   (   91.98 ms per token,    10.87 tokens per second)\n",
      "llama_print_timings:       total time =    97.51 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='ver', start=232, stop=235, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' his', start=237, stop=241, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1461.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    98.71 ms /     1 runs   (   98.71 ms per token,    10.13 tokens per second)\n",
      "llama_print_timings:       total time =   104.19 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1451.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    77.31 ms /     1 runs   (   77.31 ms per token,    12.94 tokens per second)\n",
      "llama_print_timings:       total time =    82.74 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' enemies', start=249, stop=257, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\"', start=243, stop=244, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1455.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    91.51 ms /     1 runs   (   91.51 ms per token,    10.93 tokens per second)\n",
      "llama_print_timings:       total time =    97.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1449.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    99.29 ms /     1 runs   (   99.29 ms per token,    10.07 tokens per second)\n",
      "llama_print_timings:       total time =   103.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    92.21 ms /     1 runs   (   92.21 ms per token,    10.84 tokens per second)\n",
      "llama_print_timings:       total time =    96.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=244, stop=245, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='       ', start=257, stop=264, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    99.39 ms /     1 runs   (   99.39 ms per token,    10.06 tokens per second)\n",
      "llama_print_timings:       total time =   103.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1449.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.30 ms /     1 runs   (   88.30 ms per token,    11.32 tokens per second)\n",
      "llama_print_timings:       total time =    92.39 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' }', start=254, stop=256, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' ,', start=256, stop=258, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1464.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    90.27 ms /     1 runs   (   90.27 ms per token,    11.08 tokens per second)\n",
      "llama_print_timings:       total time =    94.44 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=256, stop=257, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1466.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    98.34 ms /     1 runs   (   98.34 ms per token,    10.17 tokens per second)\n",
      "llama_print_timings:       total time =   102.49 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='       ', start=269, stop=276, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1455.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    80.52 ms /     1 runs   (   80.52 ms per token,    12.42 tokens per second)\n",
      "llama_print_timings:       total time =    84.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' {', start=266, stop=268, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\n",
      "', start=266, stop=267, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1451.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   100.15 ms /     1 runs   (  100.15 ms per token,     9.98 tokens per second)\n",
      "llama_print_timings:       total time =   104.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.19 ms /     1 runs   (   88.19 ms per token,    11.34 tokens per second)\n",
      "llama_print_timings:       total time =    92.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='           ', start=287, stop=298, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' \"', start=280, stop=282, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    85.09 ms /     1 runs   (   85.09 ms per token,    11.75 tokens per second)\n",
      "llama_print_timings:       total time =    89.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1461.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    90.22 ms /     1 runs   (   90.22 ms per token,    11.08 tokens per second)\n",
      "llama_print_timings:       total time =    95.64 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='name', start=286, stop=290, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\"', start=284, stop=285, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1445.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.12 ms /     1 runs   (   82.12 ms per token,    12.18 tokens per second)\n",
      "llama_print_timings:       total time =    87.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1449.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   104.13 ms /     1 runs   (  104.13 ms per token,     9.60 tokens per second)\n",
      "llama_print_timings:       total time =   108.28 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' :', start=287, stop=289, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1455.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   100.10 ms /     1 runs   (  100.10 ms per token,     9.99 tokens per second)\n",
      "llama_print_timings:       total time =   104.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1451.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    79.68 ms /     1 runs   (   79.68 ms per token,    12.55 tokens per second)\n",
      "llama_print_timings:       total time =    85.19 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=289, stop=291, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='M', start=289, stop=290, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1455.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    92.27 ms /     1 runs   (   92.27 ms per token,    10.84 tokens per second)\n",
      "llama_print_timings:       total time =    97.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='oon', start=294, stop=297, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='light', start=301, stop=306, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1477.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   105.17 ms /     1 runs   (  105.17 ms per token,     9.51 tokens per second)\n",
      "llama_print_timings:       total time =   110.57 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    90.11 ms /     1 runs   (   90.11 ms per token,    11.10 tokens per second)\n",
      "llama_print_timings:       total time =    95.73 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' M', start=300, stop=302, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='ask', start=304, stop=307, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    81.09 ms /     1 runs   (   81.09 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:       total time =    86.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1455.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    89.32 ms /     1 runs   (   89.32 ms per token,    11.20 tokens per second)\n",
      "llama_print_timings:       total time =    94.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1449.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    83.29 ms /     1 runs   (   83.29 ms per token,    12.01 tokens per second)\n",
      "llama_print_timings:       total time =    88.83 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='er', start=305, stop=307, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\"', start=305, stop=306, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    93.40 ms /     1 runs   (   93.40 ms per token,    10.71 tokens per second)\n",
      "llama_print_timings:       total time =    97.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' ,', start=308, stop=310, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\n",
      "', start=308, stop=309, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1455.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    97.03 ms /     1 runs   (   97.03 ms per token,    10.31 tokens per second)\n",
      "llama_print_timings:       total time =   101.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1449.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    91.53 ms /     1 runs   (   91.53 ms per token,    10.92 tokens per second)\n",
      "llama_print_timings:       total time =    95.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='           ', start=329, stop=340, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' \"', start=322, stop=324, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   100.24 ms /     1 runs   (  100.24 ms per token,     9.98 tokens per second)\n",
      "llama_print_timings:       total time =   104.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1451.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    96.87 ms /     1 runs   (   96.87 ms per token,    10.32 tokens per second)\n",
      "llama_print_timings:       total time =   102.39 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='description', start=342, stop=353, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\"', start=333, stop=334, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    85.27 ms /     1 runs   (   85.27 ms per token,    11.73 tokens per second)\n",
      "llama_print_timings:       total time =    90.88 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1451.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    87.45 ms /     1 runs   (   87.45 ms per token,    11.44 tokens per second)\n",
      "llama_print_timings:       total time =    91.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' :', start=336, stop=338, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1459.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    99.67 ms /     1 runs   (   99.67 ms per token,    10.03 tokens per second)\n",
      "llama_print_timings:       total time =   103.77 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1455.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    77.89 ms /     1 runs   (   77.89 ms per token,    12.84 tokens per second)\n",
      "llama_print_timings:       total time =    83.44 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=338, stop=340, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='P', start=338, stop=339, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1461.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    96.29 ms /     1 runs   (   96.29 ms per token,    10.39 tokens per second)\n",
      "llama_print_timings:       total time =   101.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='and', start=343, stop=346, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1445.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   123.43 ms /     1 runs   (  123.43 ms per token,     8.10 tokens per second)\n",
      "llama_print_timings:       total time =   128.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1461.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    81.74 ms /     1 runs   (   81.74 ms per token,    12.23 tokens per second)\n",
      "llama_print_timings:       total time =    87.27 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='aman', start=348, stop=352, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=''', start=346, stop=347, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1468.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    96.20 ms /     1 runs   (   96.20 ms per token,    10.40 tokens per second)\n",
      "llama_print_timings:       total time =   101.76 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='s', start=347, stop=348, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' secret', start=360, stop=367, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1447.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    87.16 ms /     1 runs   (   87.16 ms per token,    11.47 tokens per second)\n",
      "llama_print_timings:       total time =    92.76 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1464.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.05 ms /     1 runs   (   88.05 ms per token,    11.36 tokens per second)\n",
      "llama_print_timings:       total time =    93.53 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' identity', start=371, stop=380, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='.', start=364, stop=365, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1464.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    86.71 ms /     1 runs   (   86.71 ms per token,    11.53 tokens per second)\n",
      "llama_print_timings:       total time =    92.26 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1464.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.95 ms /     1 runs   (   88.95 ms per token,    11.24 tokens per second)\n",
      "llama_print_timings:       total time =    94.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1406.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    87.23 ms /     1 runs   (   87.23 ms per token,    11.46 tokens per second)\n",
      "llama_print_timings:       total time =    92.82 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' A', start=367, stop=369, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' gentle', start=379, stop=386, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1420.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.10 ms /     1 runs   (   88.10 ms per token,    11.35 tokens per second)\n",
      "llama_print_timings:       total time =    93.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' giant', start=384, stop=390, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' with', start=388, stop=393, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    97.99 ms /     1 runs   (   97.99 ms per token,    10.21 tokens per second)\n",
      "llama_print_timings:       total time =   103.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1449.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    77.95 ms /     1 runs   (   77.95 ms per token,    12.83 tokens per second)\n",
      "llama_print_timings:       total time =    83.49 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' a', start=387, stop=389, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' soft', start=395, stop=400, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1400.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    78.82 ms /     1 runs   (   78.82 ms per token,    12.69 tokens per second)\n",
      "llama_print_timings:       total time =    84.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   101.91 ms /     1 runs   (  101.91 ms per token,     9.81 tokens per second)\n",
      "llama_print_timings:       total time =   107.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1436.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    80.67 ms /     1 runs   (   80.67 ms per token,    12.40 tokens per second)\n",
      "llama_print_timings:       total time =    86.32 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' spot', start=400, stop=405, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' for', start=403, stop=407, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    84.17 ms /     1 runs   (   84.17 ms per token,    11.88 tokens per second)\n",
      "llama_print_timings:       total time =    89.83 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' the', start=407, stop=411, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' vulner', start=417, stop=424, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    85.89 ms /     1 runs   (   85.89 ms per token,    11.64 tokens per second)\n",
      "llama_print_timings:       total time =    91.59 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    79.95 ms /     1 runs   (   79.95 ms per token,    12.51 tokens per second)\n",
      "llama_print_timings:       total time =    85.56 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='able', start=418, stop=422, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' and', start=422, stop=426, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.95 ms /     1 runs   (   82.95 ms per token,    12.06 tokens per second)\n",
      "llama_print_timings:       total time =    88.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.74 ms /     1 runs   (    0.74 ms per token,  1351.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   104.10 ms /     1 runs   (  104.10 ms per token,     9.61 tokens per second)\n",
      "llama_print_timings:       total time =   109.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.31 ms /     1 runs   (   88.31 ms per token,    11.32 tokens per second)\n",
      "llama_print_timings:       total time =    93.95 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' opp', start=426, stop=430, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='ress', start=430, stop=434, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1464.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    91.68 ms /     1 runs   (   91.68 ms per token,    10.91 tokens per second)\n",
      "llama_print_timings:       total time =    97.28 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='ed', start=430, stop=432, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\"', start=430, stop=431, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1466.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   100.10 ms /     1 runs   (  100.10 ms per token,     9.99 tokens per second)\n",
      "llama_print_timings:       total time =   105.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.45 ms /     1 runs   (   82.45 ms per token,    12.13 tokens per second)\n",
      "llama_print_timings:       total time =    86.59 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=431, stop=432, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='       ', start=444, stop=451, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    83.39 ms /     1 runs   (   83.39 ms per token,    11.99 tokens per second)\n",
      "llama_print_timings:       total time =    87.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    87.97 ms /     1 runs   (   87.97 ms per token,    11.37 tokens per second)\n",
      "llama_print_timings:       total time =    92.09 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' }', start=441, stop=443, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' ,', start=443, stop=445, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    83.17 ms /     1 runs   (   83.17 ms per token,    12.02 tokens per second)\n",
      "llama_print_timings:       total time =    87.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1440.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    83.76 ms /     1 runs   (   83.76 ms per token,    11.94 tokens per second)\n",
      "llama_print_timings:       total time =    87.95 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=443, stop=444, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1408.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    78.06 ms /     1 runs   (   78.06 ms per token,    12.81 tokens per second)\n",
      "llama_print_timings:       total time =    82.59 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='       ', start=456, stop=463, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1434.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.96 ms /     1 runs   (   88.96 ms per token,    11.24 tokens per second)\n",
      "llama_print_timings:       total time =    93.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' {', start=453, stop=455, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\n",
      "', start=453, stop=454, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   101.41 ms /     1 runs   (  101.41 ms per token,     9.86 tokens per second)\n",
      "llama_print_timings:       total time =   105.59 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1418.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    86.14 ms /     1 runs   (   86.14 ms per token,    11.61 tokens per second)\n",
      "llama_print_timings:       total time =    90.30 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='           ', start=474, stop=485, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' \"', start=467, stop=469, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    79.31 ms /     1 runs   (   79.31 ms per token,    12.61 tokens per second)\n",
      "llama_print_timings:       total time =    83.59 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1451.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   100.66 ms /     1 runs   (  100.66 ms per token,     9.93 tokens per second)\n",
      "llama_print_timings:       total time =   106.28 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='name', start=473, stop=477, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\"', start=471, stop=472, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1455.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.85 ms /     1 runs   (   82.85 ms per token,    12.07 tokens per second)\n",
      "llama_print_timings:       total time =    88.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1447.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   115.58 ms /     1 runs   (  115.58 ms per token,     8.65 tokens per second)\n",
      "llama_print_timings:       total time =   119.82 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' :', start=474, stop=476, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1459.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   107.77 ms /     1 runs   (  107.77 ms per token,     9.28 tokens per second)\n",
      "llama_print_timings:       total time =   111.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1416.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    90.39 ms /     1 runs   (   90.39 ms per token,    11.06 tokens per second)\n",
      "llama_print_timings:       total time =    95.98 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=476, stop=478, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='B', start=476, stop=477, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    79.80 ms /     1 runs   (   79.80 ms per token,    12.53 tokens per second)\n",
      "llama_print_timings:       total time =    85.40 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='am', start=479, stop=481, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='bo', start=481, stop=483, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1459.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   104.14 ms /     1 runs   (  104.14 ms per token,     9.60 tokens per second)\n",
      "llama_print_timings:       total time =   109.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1436.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    85.78 ms /     1 runs   (   85.78 ms per token,    11.66 tokens per second)\n",
      "llama_print_timings:       total time =    91.49 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='o', start=481, stop=482, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' Band', start=490, stop=495, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    77.36 ms /     1 runs   (   77.36 ms per token,    12.93 tokens per second)\n",
      "llama_print_timings:       total time =    83.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1408.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    98.53 ms /     1 runs   (   98.53 ms per token,    10.15 tokens per second)\n",
      "llama_print_timings:       total time =   104.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    86.72 ms /     1 runs   (   86.72 ms per token,    11.53 tokens per second)\n",
      "llama_print_timings:       total time =    92.41 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='it', start=489, stop=491, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\"', start=489, stop=490, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1443.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   103.39 ms /     1 runs   (  103.39 ms per token,     9.67 tokens per second)\n",
      "llama_print_timings:       total time =   107.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' ,', start=492, stop=494, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\n",
      "', start=492, stop=493, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    96.41 ms /     1 runs   (   96.41 ms per token,    10.37 tokens per second)\n",
      "llama_print_timings:       total time =   100.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1418.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   103.27 ms /     1 runs   (  103.27 ms per token,     9.68 tokens per second)\n",
      "llama_print_timings:       total time =   107.59 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='           ', start=513, stop=524, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' \"', start=506, stop=508, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   103.30 ms /     1 runs   (  103.30 ms per token,     9.68 tokens per second)\n",
      "llama_print_timings:       total time =   107.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    93.38 ms /     1 runs   (   93.38 ms per token,    10.71 tokens per second)\n",
      "llama_print_timings:       total time =    99.04 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='description', start=526, stop=537, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\"', start=517, stop=518, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    77.39 ms /     1 runs   (   77.39 ms per token,    12.92 tokens per second)\n",
      "llama_print_timings:       total time =    83.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1414.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   114.39 ms /     1 runs   (  114.39 ms per token,     8.74 tokens per second)\n",
      "llama_print_timings:       total time =   118.57 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' :', start=520, stop=522, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.72 ms /     1 runs   (    0.72 ms per token,  1392.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    87.00 ms /     1 runs   (   87.00 ms per token,    11.49 tokens per second)\n",
      "llama_print_timings:       total time =    91.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1451.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.98 ms /     1 runs   (   88.98 ms per token,    11.24 tokens per second)\n",
      "llama_print_timings:       total time =    94.56 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=522, stop=524, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='P', start=522, stop=523, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.23 ms /     1 runs   (   82.23 ms per token,    12.16 tokens per second)\n",
      "llama_print_timings:       total time =    87.87 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='and', start=527, stop=530, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='aman', start=532, stop=536, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1414.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   105.00 ms /     1 runs   (  105.00 ms per token,     9.52 tokens per second)\n",
      "llama_print_timings:       total time =   110.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    84.39 ms /     1 runs   (   84.39 ms per token,    11.85 tokens per second)\n",
      "llama_print_timings:       total time =    90.02 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=''', start=530, stop=531, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='s', start=531, stop=532, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    80.66 ms /     1 runs   (   80.66 ms per token,    12.40 tokens per second)\n",
      "llama_print_timings:       total time =    86.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    85.77 ms /     1 runs   (   85.77 ms per token,    11.66 tokens per second)\n",
      "llama_print_timings:       total time =    91.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    81.52 ms /     1 runs   (   81.52 ms per token,    12.27 tokens per second)\n",
      "llama_print_timings:       total time =    87.23 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' m', start=534, stop=536, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='isch', start=540, stop=544, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.73 ms /     1 runs   (    0.73 ms per token,  1366.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    99.45 ms /     1 runs   (   99.45 ms per token,    10.05 tokens per second)\n",
      "llama_print_timings:       total time =   105.35 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='iev', start=542, stop=545, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='ous', start=545, stop=548, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1434.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    87.78 ms /     1 runs   (   87.78 ms per token,    11.39 tokens per second)\n",
      "llama_print_timings:       total time =    93.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    86.60 ms /     1 runs   (   86.60 ms per token,    11.55 tokens per second)\n",
      "llama_print_timings:       total time =    92.33 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' alter', start=554, stop=560, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' e', start=552, stop=554, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    80.24 ms /     1 runs   (   80.24 ms per token,    12.46 tokens per second)\n",
      "llama_print_timings:       total time =    85.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.10 ms /     1 runs   (   88.10 ms per token,    11.35 tokens per second)\n",
      "llama_print_timings:       total time =    93.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    78.75 ms /     1 runs   (   78.75 ms per token,    12.70 tokens per second)\n",
      "llama_print_timings:       total time =    84.41 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='go', start=554, stop=556, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='.', start=554, stop=555, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    80.33 ms /     1 runs   (   80.33 ms per token,    12.45 tokens per second)\n",
      "llama_print_timings:       total time =    85.99 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' A', start=557, stop=559, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' play', start=565, stop=570, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    96.39 ms /     1 runs   (   96.39 ms per token,    10.37 tokens per second)\n",
      "llama_print_timings:       total time =   102.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1420.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    96.42 ms /     1 runs   (   96.42 ms per token,    10.37 tokens per second)\n",
      "llama_print_timings:       total time =   102.17 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='ful', start=566, stop=569, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' pr', start=569, stop=572, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    85.27 ms /     1 runs   (   85.27 ms per token,    11.73 tokens per second)\n",
      "llama_print_timings:       total time =    91.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1418.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    84.37 ms /     1 runs   (   84.37 ms per token,    11.85 tokens per second)\n",
      "llama_print_timings:       total time =    90.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1436.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    89.16 ms /     1 runs   (   89.16 ms per token,    11.22 tokens per second)\n",
      "llama_print_timings:       total time =    94.85 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='ank', start=572, stop=575, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='ster', start=577, stop=581, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    99.72 ms /     1 runs   (   99.72 ms per token,    10.03 tokens per second)\n",
      "llama_print_timings:       total time =   105.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' who', start=581, stop=585, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' uses', start=587, stop=592, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    96.80 ms /     1 runs   (   96.80 ms per token,    10.33 tokens per second)\n",
      "llama_print_timings:       total time =   102.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    81.38 ms /     1 runs   (   81.38 ms per token,    12.29 tokens per second)\n",
      "llama_print_timings:       total time =    87.00 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' his', start=590, stop=594, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' c', start=590, stop=592, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    93.16 ms /     1 runs   (   93.16 ms per token,    10.73 tokens per second)\n",
      "llama_print_timings:       total time =    98.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.72 ms /     1 runs   (    0.72 ms per token,  1394.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    93.59 ms /     1 runs   (   93.59 ms per token,    10.69 tokens per second)\n",
      "llama_print_timings:       total time =    99.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    99.57 ms /     1 runs   (   99.57 ms per token,    10.04 tokens per second)\n",
      "llama_print_timings:       total time =   105.13 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='unning', start=600, stop=606, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' and', start=602, stop=606, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1451.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   108.26 ms /     1 runs   (  108.26 ms per token,     9.24 tokens per second)\n",
      "llama_print_timings:       total time =   113.87 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' quick', start=610, stop=616, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' thinking', start=622, stop=631, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1418.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    92.41 ms /     1 runs   (   92.41 ms per token,    10.82 tokens per second)\n",
      "llama_print_timings:       total time =    98.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1438.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    92.81 ms /     1 runs   (   92.81 ms per token,    10.77 tokens per second)\n",
      "llama_print_timings:       total time =    98.47 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' to', start=619, stop=622, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' out', start=624, stop=628, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1416.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    79.64 ms /     1 runs   (   79.64 ms per token,    12.56 tokens per second)\n",
      "llama_print_timings:       total time =    85.47 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1414.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   100.87 ms /     1 runs   (  100.87 ms per token,     9.91 tokens per second)\n",
      "llama_print_timings:       total time =   106.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.83 ms /     1 runs   (    0.83 ms per token,  1199.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    94.51 ms /     1 runs   (   94.51 ms per token,    10.58 tokens per second)\n",
      "llama_print_timings:       total time =   100.63 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='w', start=622, stop=623, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='it', start=625, stop=627, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.83 ms /     1 runs   (   82.83 ms per token,    12.07 tokens per second)\n",
      "llama_print_timings:       total time =    88.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' his', start=631, stop=635, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' fo', start=633, stop=636, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    99.33 ms /     1 runs   (   99.33 ms per token,    10.07 tokens per second)\n",
      "llama_print_timings:       total time =   105.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    94.01 ms /     1 runs   (   94.01 ms per token,    10.64 tokens per second)\n",
      "llama_print_timings:       total time =    99.77 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='es', start=634, stop=636, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\"', start=634, stop=635, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1438.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    77.46 ms /     1 runs   (   77.46 ms per token,    12.91 tokens per second)\n",
      "llama_print_timings:       total time =    83.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    93.48 ms /     1 runs   (   93.48 ms per token,    10.70 tokens per second)\n",
      "llama_print_timings:       total time =    97.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.03 ms /     1 runs   (   88.03 ms per token,    11.36 tokens per second)\n",
      "llama_print_timings:       total time =    92.28 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=635, stop=636, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='       ', start=648, stop=655, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.74 ms /     1 runs   (    0.74 ms per token,  1351.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   102.23 ms /     1 runs   (  102.23 ms per token,     9.78 tokens per second)\n",
      "llama_print_timings:       total time =   106.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    79.11 ms /     1 runs   (   79.11 ms per token,    12.64 tokens per second)\n",
      "llama_print_timings:       total time =    83.38 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' }', start=645, stop=647, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\n",
      "', start=645, stop=646, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   110.36 ms /     1 runs   (  110.36 ms per token,     9.06 tokens per second)\n",
      "llama_print_timings:       total time =   114.68 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1420.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    95.76 ms /     1 runs   (   95.76 ms per token,    10.44 tokens per second)\n",
      "llama_print_timings:       total time =    99.99 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='   ', start=650, stop=653, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' ]', start=651, stop=653, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    84.63 ms /     1 runs   (   84.63 ms per token,    11.82 tokens per second)\n",
      "llama_print_timings:       total time =    88.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' ,', start=653, stop=655, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\n",
      "', start=653, stop=654, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1443.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    90.22 ms /     1 runs   (   90.22 ms per token,    11.08 tokens per second)\n",
      "llama_print_timings:       total time =    94.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1412.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   111.26 ms /     1 runs   (  111.26 ms per token,     8.99 tokens per second)\n",
      "llama_print_timings:       total time =   115.50 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='   ', start=658, stop=661, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' \"', start=659, stop=661, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1434.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    90.01 ms /     1 runs   (   90.01 ms per token,    11.11 tokens per second)\n",
      "llama_print_timings:       total time =    94.29 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1438.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    81.75 ms /     1 runs   (   81.75 ms per token,    12.23 tokens per second)\n",
      "llama_print_timings:       total time =    87.38 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='pow', start=663, stop=666, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='ers', start=666, stop=669, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    91.03 ms /     1 runs   (   91.03 ms per token,    10.99 tokens per second)\n",
      "llama_print_timings:       total time =    96.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.34 ms /     1 runs   (   88.34 ms per token,    11.32 tokens per second)\n",
      "llama_print_timings:       total time =    94.03 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    80.51 ms /     1 runs   (   80.51 ms per token,    12.42 tokens per second)\n",
      "llama_print_timings:       total time =    84.79 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\"', start=665, stop=666, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' :', start=668, stop=670, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1436.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    77.66 ms /     1 runs   (   77.66 ms per token,    12.88 tokens per second)\n",
      "llama_print_timings:       total time =    81.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' {', start=670, stop=672, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\n",
      "', start=670, stop=671, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1416.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    81.25 ms /     1 runs   (   81.25 ms per token,    12.31 tokens per second)\n",
      "llama_print_timings:       total time =    85.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    86.58 ms /     1 runs   (   86.58 ms per token,    11.55 tokens per second)\n",
      "llama_print_timings:       total time =    90.82 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='       ', start=683, stop=690, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' \"', start=680, stop=682, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.92 ms /     1 runs   (   88.92 ms per token,    11.25 tokens per second)\n",
      "llama_print_timings:       total time =    93.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.05 ms /     1 runs   (   88.05 ms per token,    11.36 tokens per second)\n",
      "llama_print_timings:       total time =    93.74 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='str', start=684, stop=687, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='ength', start=691, stop=696, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    78.41 ms /     1 runs   (   78.41 ms per token,    12.75 tokens per second)\n",
      "llama_print_timings:       total time =    84.12 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1451.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   199.67 ms /     1 runs   (  199.67 ms per token,     5.01 tokens per second)\n",
      "llama_print_timings:       total time =   205.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1416.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   148.65 ms /     1 runs   (  148.65 ms per token,     6.73 tokens per second)\n",
      "llama_print_timings:       total time =   152.93 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\"', start=688, stop=689, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' :', start=691, stop=693, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1447.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.97 ms /     1 runs   (   88.97 ms per token,    11.24 tokens per second)\n",
      "llama_print_timings:       total time =    93.27 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' ', start=691, stop=692, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1416.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.78 ms /     1 runs   (   82.78 ms per token,    12.08 tokens per second)\n",
      "llama_print_timings:       total time =    87.14 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='1', start=692, stop=693, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=',', start=693, stop=694, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   108.93 ms /     1 runs   (  108.93 ms per token,     9.18 tokens per second)\n",
      "llama_print_timings:       total time =   113.21 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    77.55 ms /     1 runs   (   77.55 ms per token,    12.89 tokens per second)\n",
      "llama_print_timings:       total time =    81.80 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=694, stop=695, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='       ', start=707, stop=714, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    97.22 ms /     1 runs   (   97.22 ms per token,    10.29 tokens per second)\n",
      "llama_print_timings:       total time =   101.51 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    86.56 ms /     1 runs   (   86.56 ms per token,    11.55 tokens per second)\n",
      "llama_print_timings:       total time =    90.90 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=704, stop=706, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='speed', start=712, stop=717, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    94.15 ms /     1 runs   (   94.15 ms per token,    10.62 tokens per second)\n",
      "llama_print_timings:       total time =    99.86 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    81.77 ms /     1 runs   (   81.77 ms per token,    12.23 tokens per second)\n",
      "llama_print_timings:       total time =    87.45 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\"', start=709, stop=710, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' :', start=712, stop=714, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    92.93 ms /     1 runs   (   92.93 ms per token,    10.76 tokens per second)\n",
      "llama_print_timings:       total time =    97.31 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1434.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    85.38 ms /     1 runs   (   85.38 ms per token,    11.71 tokens per second)\n",
      "llama_print_timings:       total time =    89.70 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' ', start=712, stop=713, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1436.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    86.75 ms /     1 runs   (   86.75 ms per token,    11.53 tokens per second)\n",
      "llama_print_timings:       total time =    91.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='2', start=713, stop=714, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=',', start=714, stop=715, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.19 ms /     1 runs   (   88.19 ms per token,    11.34 tokens per second)\n",
      "llama_print_timings:       total time =    92.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1459.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   103.69 ms /     1 runs   (  103.69 ms per token,     9.64 tokens per second)\n",
      "llama_print_timings:       total time =   107.87 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=715, stop=716, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='       ', start=728, stop=735, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    89.31 ms /     1 runs   (   89.31 ms per token,    11.20 tokens per second)\n",
      "llama_print_timings:       total time =    93.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1440.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   105.56 ms /     1 runs   (  105.56 ms per token,     9.47 tokens per second)\n",
      "llama_print_timings:       total time =   109.80 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=725, stop=727, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='fl', start=727, stop=729, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1436.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.63 ms /     1 runs   (   82.63 ms per token,    12.10 tokens per second)\n",
      "llama_print_timings:       total time =    88.26 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    92.17 ms /     1 runs   (   92.17 ms per token,    10.85 tokens per second)\n",
      "llama_print_timings:       total time =    97.92 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='ight', start=733, stop=737, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\"', start=731, stop=732, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    80.11 ms /     1 runs   (   80.11 ms per token,    12.48 tokens per second)\n",
      "llama_print_timings:       total time =    85.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   100.14 ms /     1 runs   (  100.14 ms per token,     9.99 tokens per second)\n",
      "llama_print_timings:       total time =   104.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' :', start=734, stop=736, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1443.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   101.30 ms /     1 runs   (  101.30 ms per token,     9.87 tokens per second)\n",
      "llama_print_timings:       total time =   105.64 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1443.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.99 ms /     1 runs   (   82.99 ms per token,    12.05 tokens per second)\n",
      "llama_print_timings:       total time =    87.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' false', start=744, stop=750, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\n",
      "', start=740, stop=741, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.73 ms /     1 runs   (    0.73 ms per token,  1373.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    86.73 ms /     1 runs   (   86.73 ms per token,    11.53 tokens per second)\n",
      "llama_print_timings:       total time =    91.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1443.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   117.41 ms /     1 runs   (  117.41 ms per token,     8.52 tokens per second)\n",
      "llama_print_timings:       total time =   121.77 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='   ', start=745, stop=748, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' }', start=746, stop=748, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.39 ms /     1 runs   (   82.39 ms per token,    12.14 tokens per second)\n",
      "llama_print_timings:       total time =    86.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' ,', start=748, stop=750, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\n",
      "', start=748, stop=749, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   106.86 ms /     1 runs   (  106.86 ms per token,     9.36 tokens per second)\n",
      "llama_print_timings:       total time =   111.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   103.08 ms /     1 runs   (  103.08 ms per token,     9.70 tokens per second)\n",
      "llama_print_timings:       total time =   107.37 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='   ', start=753, stop=756, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' \"', start=754, stop=756, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1420.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    90.88 ms /     1 runs   (   90.88 ms per token,    11.00 tokens per second)\n",
      "llama_print_timings:       total time =    95.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   133.41 ms /     1 runs   (  133.41 ms per token,     7.50 tokens per second)\n",
      "llama_print_timings:       total time =   139.08 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='weak', start=760, stop=764, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='ness', start=764, stop=768, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.39 ms /     1 runs   (   82.39 ms per token,    12.14 tokens per second)\n",
      "llama_print_timings:       total time =    88.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    84.72 ms /     1 runs   (   84.72 ms per token,    11.80 tokens per second)\n",
      "llama_print_timings:       total time =    90.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    91.19 ms /     1 runs   (   91.19 ms per token,    10.97 tokens per second)\n",
      "llama_print_timings:       total time =    96.83 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='es', start=764, stop=766, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\"', start=764, stop=765, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    86.42 ms /     1 runs   (   86.42 ms per token,    11.57 tokens per second)\n",
      "llama_print_timings:       total time =    90.84 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' :', start=767, stop=769, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    87.04 ms /     1 runs   (   87.04 ms per token,    11.49 tokens per second)\n",
      "llama_print_timings:       total time =    91.35 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' [', start=769, stop=771, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    87.89 ms /     1 runs   (   87.89 ms per token,    11.38 tokens per second)\n",
      "llama_print_timings:       total time =    92.21 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=769, stop=770, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    84.46 ms /     1 runs   (   84.46 ms per token,    11.84 tokens per second)\n",
      "llama_print_timings:       total time =    88.72 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='       ', start=782, stop=789, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1434.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    79.69 ms /     1 runs   (   79.69 ms per token,    12.55 tokens per second)\n",
      "llama_print_timings:       total time =    83.99 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' {', start=779, stop=781, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\n",
      "', start=779, stop=780, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1436.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.62 ms /     1 runs   (   82.62 ms per token,    12.10 tokens per second)\n",
      "llama_print_timings:       total time =    86.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   102.82 ms /     1 runs   (  102.82 ms per token,     9.73 tokens per second)\n",
      "llama_print_timings:       total time =   107.04 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='           ', start=800, stop=811, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' \"', start=793, stop=795, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    86.27 ms /     1 runs   (   86.27 ms per token,    11.59 tokens per second)\n",
      "llama_print_timings:       total time =    90.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1420.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    96.17 ms /     1 runs   (   96.17 ms per token,    10.40 tokens per second)\n",
      "llama_print_timings:       total time =   101.87 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    96.06 ms /     1 runs   (   96.06 ms per token,    10.41 tokens per second)\n",
      "llama_print_timings:       total time =   101.70 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='name', start=799, stop=803, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\"', start=797, stop=798, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    95.32 ms /     1 runs   (   95.32 ms per token,    10.49 tokens per second)\n",
      "llama_print_timings:       total time =    99.65 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' :', start=800, stop=802, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    98.80 ms /     1 runs   (   98.80 ms per token,    10.12 tokens per second)\n",
      "llama_print_timings:       total time =   103.14 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=802, stop=804, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='K', start=802, stop=803, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1447.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   102.99 ms /     1 runs   (  102.99 ms per token,     9.71 tokens per second)\n",
      "llama_print_timings:       total time =   108.68 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.55 ms /     1 runs   (   88.55 ms per token,    11.29 tokens per second)\n",
      "llama_print_timings:       total time =    94.19 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='rypt', start=809, stop=813, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='on', start=809, stop=811, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    83.12 ms /     1 runs   (   83.12 ms per token,    12.03 tokens per second)\n",
      "llama_print_timings:       total time =    88.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    91.66 ms /     1 runs   (   91.66 ms per token,    10.91 tokens per second)\n",
      "llama_print_timings:       total time =    97.46 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    87.25 ms /     1 runs   (   87.25 ms per token,    11.46 tokens per second)\n",
      "llama_print_timings:       total time =    92.90 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='ite', start=813, stop=816, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\"', start=812, stop=813, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    95.61 ms /     1 runs   (   95.61 ms per token,    10.46 tokens per second)\n",
      "llama_print_timings:       total time =   100.00 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' ,', start=815, stop=817, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\n",
      "', start=815, stop=816, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    85.11 ms /     1 runs   (   85.11 ms per token,    11.75 tokens per second)\n",
      "llama_print_timings:       total time =    89.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   105.11 ms /     1 runs   (  105.11 ms per token,     9.51 tokens per second)\n",
      "llama_print_timings:       total time =   109.46 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='           ', start=836, stop=847, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' \"', start=829, stop=831, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   127.92 ms /     1 runs   (  127.92 ms per token,     7.82 tokens per second)\n",
      "llama_print_timings:       total time =   132.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.76 ms /     1 runs   (    0.76 ms per token,  1310.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    85.53 ms /     1 runs   (   85.53 ms per token,    11.69 tokens per second)\n",
      "llama_print_timings:       total time =    91.23 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='description', start=849, stop=860, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='\"', start=840, stop=841, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    86.79 ms /     1 runs   (   86.79 ms per token,    11.52 tokens per second)\n",
      "llama_print_timings:       total time =    92.58 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   103.21 ms /     1 runs   (  103.21 ms per token,     9.69 tokens per second)\n",
      "llama_print_timings:       total time =   107.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' :', start=843, stop=845, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1402.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   102.51 ms /     1 runs   (  102.51 ms per token,     9.75 tokens per second)\n",
      "llama_print_timings:       total time =   106.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1418.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    86.68 ms /     1 runs   (   86.68 ms per token,    11.54 tokens per second)\n",
      "llama_print_timings:       total time =    93.12 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=845, stop=847, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='P', start=845, stop=846, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    80.22 ms /     1 runs   (   80.22 ms per token,    12.47 tokens per second)\n",
      "llama_print_timings:       total time =    85.94 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='and', start=850, stop=853, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='aman', start=855, stop=859, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    85.97 ms /     1 runs   (   85.97 ms per token,    11.63 tokens per second)\n",
      "llama_print_timings:       total time =    91.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    96.47 ms /     1 runs   (   96.47 ms per token,    10.37 tokens per second)\n",
      "llama_print_timings:       total time =   102.18 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=''', start=853, stop=854, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='s', start=854, stop=855, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   105.52 ms /     1 runs   (  105.52 ms per token,     9.48 tokens per second)\n",
      "llama_print_timings:       total time =   111.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.73 ms /     1 runs   (    0.73 ms per token,  1371.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    98.30 ms /     1 runs   (   98.30 ms per token,    10.17 tokens per second)\n",
      "llama_print_timings:       total time =   104.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.94 ms /     1 runs   (   82.94 ms per token,    12.06 tokens per second)\n",
      "llama_print_timings:       total time =    88.65 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' arch', start=863, stop=868, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='-', start=860, stop=861, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1418.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   104.56 ms /     1 runs   (  104.56 ms per token,     9.56 tokens per second)\n",
      "llama_print_timings:       total time =   110.28 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='nem', start=865, stop=868, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='esis', start=870, stop=874, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1418.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    99.09 ms /     1 runs   (   99.09 ms per token,    10.09 tokens per second)\n",
      "llama_print_timings:       total time =   104.78 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.41 ms /     1 runs   (   88.41 ms per token,    11.31 tokens per second)\n",
      "llama_print_timings:       total time =    94.07 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='.', start=868, stop=869, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' A', start=871, stop=873, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    78.29 ms /     1 runs   (   78.29 ms per token,    12.77 tokens per second)\n",
      "llama_print_timings:       total time =    84.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1414.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.40 ms /     1 runs   (   88.40 ms per token,    11.31 tokens per second)\n",
      "llama_print_timings:       total time =    94.06 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1418.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.06 ms /     1 runs   (   82.06 ms per token,    12.19 tokens per second)\n",
      "llama_print_timings:       total time =    87.84 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' radio', start=881, stop=887, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='active', start=887, stop=893, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    99.64 ms /     1 runs   (   99.64 ms per token,    10.04 tokens per second)\n",
      "llama_print_timings:       total time =   105.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' min', start=889, stop=893, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='eral', start=893, stop=897, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    84.06 ms /     1 runs   (   84.06 ms per token,    11.90 tokens per second)\n",
      "llama_print_timings:       total time =    89.76 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1420.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    93.07 ms /     1 runs   (   93.07 ms per token,    10.74 tokens per second)\n",
      "llama_print_timings:       total time =    98.80 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' that', start=899, stop=904, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' can', start=902, stop=906, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1438.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    79.81 ms /     1 runs   (   79.81 ms per token,    12.53 tokens per second)\n",
      "llama_print_timings:       total time =    85.63 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    92.80 ms /     1 runs   (   92.80 ms per token,    10.78 tokens per second)\n",
      "llama_print_timings:       total time =    98.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1434.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.96 ms /     1 runs   (   82.96 ms per token,    12.05 tokens per second)\n",
      "llama_print_timings:       total time =    88.77 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' weak', start=908, stop=913, name=llama, chunk=True, score=0.0)\n",
      "Completion(text='en', start=907, stop=909, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1414.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    80.85 ms /     1 runs   (   80.85 ms per token,    12.37 tokens per second)\n",
      "llama_print_timings:       total time =    86.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' and', start=913, stop=917, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' rep', start=917, stop=921, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1438.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    87.99 ms /     1 runs   (   87.99 ms per token,    11.36 tokens per second)\n",
      "llama_print_timings:       total time =    93.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    93.69 ms /     1 runs   (   93.69 ms per token,    10.67 tokens per second)\n",
      "llama_print_timings:       total time =    99.50 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='el', start=917, stop=919, name=llama, chunk=True, score=0.0)\n",
      "Completion(text=' him', start=923, stop=927, name=llama, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1438.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    81.48 ms /     1 runs   (   81.48 ms per token,    12.27 tokens per second)\n",
      "llama_print_timings:       total time =    87.30 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\"', start=921, stop=922, name=llama, chunk=True, score=0.0)\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   108.72 ms /     1 runs   (  108.72 ms per token,     9.20 tokens per second)\n",
      "llama_print_timings:       total time =   114.50 ms\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"llama_model_completion = await strip_tags(prompt).complete(model=llama_model, constraint=JsonConstraint(), stream = print_stream, max_tokens=300, name='llama')\";\n",
       "                var nbb_formatted_code = \"llama_model_completion = await strip_tags(prompt).complete(\\n    model=llama_model,\\n    constraint=JsonConstraint(),\\n    stream=print_stream,\\n    max_tokens=300,\\n    name=\\\"llama\\\",\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_model_completion = await strip_tags(prompt).complete(\n",
    "    model=llama_model,\n",
    "    constraint=JsonConstraint(),\n",
    "    stream=print_stream,\n",
    "    max_tokens=300,\n",
    "    name=\"llama\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae2f54e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Completion(text='{\n",
       "    \"name\" : \"Pandaman\" ,\n",
       "    \"aliases\" : [\n",
       "        {\n",
       "            \"name\" : \"Panther Pawzilla\" ,\n",
       "            \"description\" : \"Pandaman's alter ego. A sleek and stealthy hero who uses his quick reflexes and agility to outmaneuver his enemies\"\n",
       "        } ,\n",
       "        {\n",
       "            \"name\" : \"Moonlight Masker\" ,\n",
       "            \"description\" : \"Pandaman's secret identity. A gentle giant with a soft spot for the vulnerable and oppressed\"\n",
       "        } ,\n",
       "        {\n",
       "            \"name\" : \"Bamboo Bandit\" ,\n",
       "            \"description\" : \"Pandaman's mischievous alter ego. A playful prankster who uses his cunning and quick thinking to outwit his foes\"\n",
       "        }\n",
       "    ] ,\n",
       "    \"powers\" : {\n",
       "        \"strength\" : 1,\n",
       "        \"speed\" : 2,\n",
       "        \"flight\" : false\n",
       "    } ,\n",
       "    \"weaknesses\" : [\n",
       "        {\n",
       "            \"name\" : \"Kryptonite\" ,\n",
       "            \"description\" : \"Pandaman's arch-nemesis. A radioactive mineral that can weaken and repel him\"', start=619, stop=1539, name=llama, chunk=False, score=0.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"llama_model_completion.completions.llama\";\n",
       "                var nbb_formatted_code = \"llama_model_completion.completions.llama\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_model_completion.completions.llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a602cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"chat_prompt = Prompt(\\\"\\\"\\\"%system%You are an expert in designing novel superheroes.\\n%/system%%user%Give me a json design of a superhero similar to this:\\n{\\n    \\\"name\\\": \\\"Superman\\\",\\n    \\\"aliases\\\": [\\\"Clark Kent\\\", \\\"Man of Steel\\\"],\\n    \\\"powers\\\": {\\n        \\\"strength\\\": 100,\\n        \\\"speed\\\": 75,\\n        \\\"flight\\\": true\\n    },\\n    \\\"weaknesses\\\": [\\\"kryptonite\\\", \\\"magic\\\"]\\n}\\n\\nI want a superhero that has something to do with pandas. \\nI want his name to be pandaman. He is stronger and faster than superman. \\nGive him 3 super cool aliases too!\\n%/user%\\n\\\"\\\"\\\")\";\n",
       "                var nbb_formatted_code = \"chat_prompt = Prompt(\\n    \\\"\\\"\\\"%system%You are an expert in designing novel superheroes.\\n%/system%%user%Give me a json design of a superhero similar to this:\\n{\\n    \\\"name\\\": \\\"Superman\\\",\\n    \\\"aliases\\\": [\\\"Clark Kent\\\", \\\"Man of Steel\\\"],\\n    \\\"powers\\\": {\\n        \\\"strength\\\": 100,\\n        \\\"speed\\\": 75,\\n        \\\"flight\\\": true\\n    },\\n    \\\"weaknesses\\\": [\\\"kryptonite\\\", \\\"magic\\\"]\\n}\\n\\nI want a superhero that has something to do with pandas. \\nI want his name to be pandaman. He is stronger and faster than superman. \\nGive him 3 super cool aliases too!\\n%/user%\\n\\\"\\\"\\\"\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_prompt = Prompt(\n",
    "    \"\"\"%system%You are an expert in designing novel superheroes.\n",
    "%/system%%user%Give me a json design of a superhero similar to this:\n",
    "{\n",
    "    \"name\": \"Superman\",\n",
    "    \"aliases\": [\"Clark Kent\", \"Man of Steel\"],\n",
    "    \"powers\": {\n",
    "        \"strength\": 100,\n",
    "        \"speed\": 75,\n",
    "        \"flight\": true\n",
    "    },\n",
    "    \"weaknesses\": [\"kryptonite\", \"magic\"]\n",
    "}\n",
    "\n",
    "I want a superhero that has something to do with pandas. \n",
    "I want his name to be pandaman. He is stronger and faster than superman. \n",
    "Give him 3 super cool aliases too!\n",
    "%/user%\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a003bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"regular_prompt = strip_tags(prompt, roles_seps={'system': 'System: ', 'user': 'User: ', 'assistant': 'Assistant: '})\";\n",
       "                var nbb_formatted_code = \"regular_prompt = strip_tags(\\n    prompt,\\n    roles_seps={\\\"system\\\": \\\"System: \\\", \\\"user\\\": \\\"User: \\\", \\\"assistant\\\": \\\"Assistant: \\\"},\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regular_prompt = strip_tags(\n",
    "    prompt,\n",
    "    roles_seps={\"system\": \"System: \", \"user\": \"User: \", \"assistant\": \"Assistant: \"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb15f6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: \n",
      "You are an expert in designing novel superheroes.\n",
      "\n",
      "User: Give me a json design of a superhero similar to this:\n",
      "{\n",
      "    \"name\": \"Superman\",\n",
      "    \"aliases\": [\"Clark Kent\", \"Man of Steel\"],\n",
      "    \"powers\": {\n",
      "        \"strength\": 100,\n",
      "        \"speed\": 75,\n",
      "        \"flight\": true\n",
      "    },\n",
      "    \"weaknesses\": [\"kryptonite\", \"magic\"]\n",
      "}\n",
      "\n",
      "I want a superhero that has something to do with pandas. \n",
      "I want his name to be pandaman. He is stronger and faster than superman. \n",
      "Give him 3 super cool aliases too!\n",
      "something like {\"name\": \"pandaman\", \"aliases\":...}\n",
      "\n",
      "\n",
      "Generate me a json for pandaman.\n",
      "\n",
      "Assistant: Here is a JSON design for that:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"print(regular_prompt)\";\n",
       "                var nbb_formatted_code = \"print(regular_prompt)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(regular_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30fe1c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"superhero_pattern=r\\\"\\\"\\\"{\\n    \\\\\\\"name\\\\\\\": \\\\\\\"[a-zA-Z]+\\\\\\\",\\n    \\\\\\\"aliases\\\\\\\": \\\\[(\\\\\\\"[A-Za-z]+(\\\\s[A-Za-z]+)*\\\\\\\",\\\\s)*\\\\\\\"[A-Za-z]+(\\\\s[A-Za-z]+)*\\\\\\\"\\\\],\\n    \\\\\\\"powers\\\\\\\": {\\n        \\\\\\\"strength\\\\\\\": [0-9]+,\\n        \\\\\\\"speed\\\\\\\": [0-9]+,\\n        \\\\\\\"flight\\\\\\\": (true|false)\\n    },\\n    \\\\\\\"weaknesses\\\\\\\": \\\\[(\\\\\\\"[A-Za-z]+(\\\\s[A-Za-z]+)*\\\\\\\",\\\\s)*\\\\\\\"[A-Za-z]+(\\\\s[A-Za-z]+)*\\\\\\\"\\\\]\\n}\\\"\\\"\\\"\\n\\nsuperhero_constraint = RegexConstraint(superhero_pattern)\";\n",
       "                var nbb_formatted_code = \"superhero_pattern = r\\\"\\\"\\\"{\\n    \\\\\\\"name\\\\\\\": \\\\\\\"[a-zA-Z]+\\\\\\\",\\n    \\\\\\\"aliases\\\\\\\": \\\\[(\\\\\\\"[A-Za-z]+(\\\\s[A-Za-z]+)*\\\\\\\",\\\\s)*\\\\\\\"[A-Za-z]+(\\\\s[A-Za-z]+)*\\\\\\\"\\\\],\\n    \\\\\\\"powers\\\\\\\": {\\n        \\\\\\\"strength\\\\\\\": [0-9]+,\\n        \\\\\\\"speed\\\\\\\": [0-9]+,\\n        \\\\\\\"flight\\\\\\\": (true|false)\\n    },\\n    \\\\\\\"weaknesses\\\\\\\": \\\\[(\\\\\\\"[A-Za-z]+(\\\\s[A-Za-z]+)*\\\\\\\",\\\\s)*\\\\\\\"[A-Za-z]+(\\\\s[A-Za-z]+)*\\\\\\\"\\\\]\\n}\\\"\\\"\\\"\\n\\nsuperhero_constraint = RegexConstraint(superhero_pattern)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "superhero_pattern = r\"\"\"{\n",
    "    \\\"name\\\": \\\"[a-zA-Z]+\\\",\n",
    "    \\\"aliases\\\": \\[(\\\"[A-Za-z]+(\\s[A-Za-z]+)*\\\",\\s)*\\\"[A-Za-z]+(\\s[A-Za-z]+)*\\\"\\],\n",
    "    \\\"powers\\\": {\n",
    "        \\\"strength\\\": [0-9]+,\n",
    "        \\\"speed\\\": [0-9]+,\n",
    "        \\\"flight\\\": (true|false)\n",
    "    },\n",
    "    \\\"weaknesses\\\": \\[(\\\"[A-Za-z]+(\\s[A-Za-z]+)*\\\",\\s)*\\\"[A-Za-z]+(\\s[A-Za-z]+)*\\\"\\]\n",
    "}\"\"\"\n",
    "\n",
    "superhero_constraint = RegexConstraint(superhero_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45c71ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1451.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6795.53 ms /   213 tokens (   31.90 ms per token,    31.34 tokens per second)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =  7092.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='{', start=2, stop=3, name=None, chunk=True, score=9.999999999999996e-11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1418.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   243.07 ms /     1 runs   (  243.07 ms per token,     4.11 tokens per second)\n",
      "llama_print_timings:       total time =   247.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1420.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   100.32 ms /     1 runs   (  100.32 ms per token,     9.97 tokens per second)\n",
      "llama_print_timings:       total time =   104.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=3, stop=4, name=None, chunk=True, score=9.999999999999992e-21)\n",
      "Completion(text='   ', start=8, stop=11, name=None, chunk=True, score=1.0000000000000024e-30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   136.56 ms /     1 runs   (  136.56 ms per token,     7.32 tokens per second)\n",
      "llama_print_timings:       total time =   140.95 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    99.96 ms /     1 runs   (   99.96 ms per token,    10.00 tokens per second)\n",
      "llama_print_timings:       total time =   104.13 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=9, stop=11, name=None, chunk=True, score=9.999999999999985e-41)\n",
      "Completion(text='name', start=15, stop=19, name=None, chunk=True, score=9.999999999999944e-51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1457.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   119.50 ms /     1 runs   (  119.50 ms per token,     8.37 tokens per second)\n",
      "llama_print_timings:       total time =   123.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    84.45 ms /     1 runs   (   84.45 ms per token,    11.84 tokens per second)\n",
      "llama_print_timings:       total time =    88.59 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\":', start=15, stop=17, name=None, chunk=True, score=1.0000000000000048e-60)\n",
      "Completion(text=' \"', start=17, stop=19, name=None, chunk=True, score=1.000000000000015e-70)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1445.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    99.95 ms /     1 runs   (   99.95 ms per token,    10.01 tokens per second)\n",
      "llama_print_timings:       total time =   104.64 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1416.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    78.20 ms /     1 runs   (   78.20 ms per token,    12.79 tokens per second)\n",
      "llama_print_timings:       total time =    82.91 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='P', start=17, stop=18, name=None, chunk=True, score=1.0000000000000252e-80)\n",
      "Completion(text='and', start=22, stop=25, name=None, chunk=True, score=1.0558783676419703e-89)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   104.86 ms /     1 runs   (  104.86 ms per token,     9.54 tokens per second)\n",
      "llama_print_timings:       total time =   109.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   104.63 ms /     1 runs   (  104.63 ms per token,     9.56 tokens per second)\n",
      "llama_print_timings:       total time =   109.34 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='aman', start=27, stop=31, name=None, chunk=True, score=1.055878367641981e-99)\n",
      "Completion(text='\",', start=27, stop=29, name=None, chunk=True, score=8.520709452512616e-109)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1418.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   119.30 ms /     1 runs   (  119.30 ms per token,     8.38 tokens per second)\n",
      "llama_print_timings:       total time =   123.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    93.47 ms /     1 runs   (   93.47 ms per token,    10.70 tokens per second)\n",
      "llama_print_timings:       total time =    97.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=27, stop=28, name=None, chunk=True, score=8.520709452512461e-119)\n",
      "Completion(text='   ', start=32, stop=35, name=None, chunk=True, score=8.520709452512306e-129)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    99.53 ms /     1 runs   (   99.53 ms per token,    10.05 tokens per second)\n",
      "llama_print_timings:       total time =   103.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.26 ms /     1 runs   (   88.26 ms per token,    11.33 tokens per second)\n",
      "llama_print_timings:       total time =    92.42 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=33, stop=35, name=None, chunk=True, score=8.520709452512151e-139)\n",
      "Completion(text='ali', start=37, stop=40, name=None, chunk=True, score=8.520709452511997e-149)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1453.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    94.28 ms /     1 runs   (   94.28 ms per token,    10.61 tokens per second)\n",
      "llama_print_timings:       total time =    98.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    83.72 ms /     1 runs   (   83.72 ms per token,    11.95 tokens per second)\n",
      "llama_print_timings:       total time =    87.88 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='ases', start=42, stop=46, name=None, chunk=True, score=8.520709452511842e-159)\n",
      "Completion(text='\":', start=42, stop=44, name=None, chunk=True, score=8.520709452511687e-169)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    93.65 ms /     1 runs   (   93.65 ms per token,    10.68 tokens per second)\n",
      "llama_print_timings:       total time =    97.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    78.02 ms /     1 runs   (   78.02 ms per token,    12.82 tokens per second)\n",
      "llama_print_timings:       total time =    82.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' [\"', start=46, stop=49, name=None, chunk=True, score=8.520709452511533e-179)\n",
      "Completion(text='P', start=45, stop=46, name=None, chunk=True, score=8.520709452511378e-189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    97.63 ms /     1 runs   (   97.63 ms per token,    10.24 tokens per second)\n",
      "llama_print_timings:       total time =   102.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    92.90 ms /     1 runs   (   92.90 ms per token,    10.76 tokens per second)\n",
      "llama_print_timings:       total time =    98.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='anda', start=52, stop=56, name=None, chunk=True, score=1.3512710251028783e-197)\n",
      "Completion(text=' Power', start=60, stop=66, name=None, chunk=True, score=1.6832607856506282e-204)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   123.10 ms /     1 runs   (  123.10 ms per token,     8.12 tokens per second)\n",
      "llama_print_timings:       total time =   128.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    96.75 ms /     1 runs   (   96.75 ms per token,    10.34 tokens per second)\n",
      "llama_print_timings:       total time =   101.01 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\",', start=58, stop=60, name=None, chunk=True, score=1.2695234127267207e-212)\n",
      "Completion(text=' \"', start=60, stop=62, name=None, chunk=True, score=1.2695234127266978e-222)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    94.92 ms /     1 runs   (   94.92 ms per token,    10.54 tokens per second)\n",
      "llama_print_timings:       total time =    99.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='M', start=60, stop=61, name=None, chunk=True, score=1.2695234127267469e-232)\n",
      "Completion(text='ister', start=69, stop=74, name=None, chunk=True, score=4.416973370432893e-238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1464.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    93.27 ms /     1 runs   (   93.27 ms per token,    10.72 tokens per second)\n",
      "llama_print_timings:       total time =    98.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1412.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    86.21 ms /     1 runs   (   86.21 ms per token,    11.60 tokens per second)\n",
      "llama_print_timings:       total time =    91.54 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' P', start=68, stop=70, name=None, chunk=True, score=8.486896169033431e-243)\n",
      "Completion(text='aws', start=72, stop=75, name=None, chunk=True, score=1.0182619606468493e-250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    89.96 ms /     1 runs   (   89.96 ms per token,    11.12 tokens per second)\n",
      "llama_print_timings:       total time =    95.29 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1414.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   155.72 ms /     1 runs   (  155.72 ms per token,     6.42 tokens per second)\n",
      "llama_print_timings:       total time =   161.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    86.62 ms /     1 runs   (   86.62 ms per token,    11.54 tokens per second)\n",
      "llama_print_timings:       total time =    91.97 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='itive', start=79, stop=84, name=None, chunk=True, score=1.108598603501113e-258)\n",
      "Completion(text='\",', start=78, stop=80, name=None, chunk=True, score=1.344016623343356e-267)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1414.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    77.34 ms /     1 runs   (   77.34 ms per token,    12.93 tokens per second)\n",
      "llama_print_timings:       total time =    81.67 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=80, stop=82, name=None, chunk=True, score=1.3440166233433317e-277)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.72 ms /     1 runs   (    0.72 ms per token,  1385.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   101.85 ms /     1 runs   (  101.85 ms per token,     9.82 tokens per second)\n",
      "llama_print_timings:       total time =   106.85 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1420.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    85.10 ms /     1 runs   (   85.10 ms per token,    11.75 tokens per second)\n",
      "llama_print_timings:       total time =    90.38 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='The', start=84, stop=87, name=None, chunk=True, score=4.0223605358579495e-284)\n",
      "Completion(text=' Great', start=93, stop=99, name=None, chunk=True, score=6.560096211155011e-292)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    95.04 ms /     1 runs   (   95.04 ms per token,    10.52 tokens per second)\n",
      "llama_print_timings:       total time =   100.32 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' P', start=91, stop=93, name=None, chunk=True, score=5.546225858513812e-298)\n",
      "Completion(text='anda', start=97, stop=101, name=None, chunk=True, score=2.5905699430421324e-305)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    93.89 ms /     1 runs   (   93.89 ms per token,    10.65 tokens per second)\n",
      "llama_print_timings:       total time =    99.17 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1455.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    83.49 ms /     1 runs   (   83.49 ms per token,    11.98 tokens per second)\n",
      "llama_print_timings:       total time =    88.84 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\"],', start=99, stop=102, name=None, chunk=True, score=1.151086148015e-312)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    93.33 ms /     1 runs   (   93.33 ms per token,    10.71 tokens per second)\n",
      "llama_print_timings:       total time =    97.59 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=98, stop=99, name=None, chunk=True, score=1.14e-322)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1420.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    92.01 ms /     1 runs   (   92.01 ms per token,    10.87 tokens per second)\n",
      "llama_print_timings:       total time =    96.31 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='   ', start=103, stop=106, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1445.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    85.76 ms /     1 runs   (   85.76 ms per token,    11.66 tokens per second)\n",
      "llama_print_timings:       total time =    89.98 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=104, stop=106, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.78 ms /     1 runs   (    0.78 ms per token,  1287.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   270.63 ms /     1 runs   (  270.63 ms per token,     3.70 tokens per second)\n",
      "llama_print_timings:       total time =   274.99 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='pow', start=108, stop=111, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   150.93 ms /     1 runs   (  150.93 ms per token,     6.63 tokens per second)\n",
      "llama_print_timings:       total time =   155.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='ers', start=111, stop=114, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1416.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   152.55 ms /     1 runs   (  152.55 ms per token,     6.56 tokens per second)\n",
      "llama_print_timings:       total time =   157.04 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\":', start=112, stop=114, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1400.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   113.25 ms /     1 runs   (  113.25 ms per token,     8.83 tokens per second)\n",
      "llama_print_timings:       total time =   117.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' {', start=114, stop=116, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1420.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   150.15 ms /     1 runs   (  150.15 ms per token,     6.66 tokens per second)\n",
      "llama_print_timings:       total time =   154.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=114, stop=115, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   109.17 ms /     1 runs   (  109.17 ms per token,     9.16 tokens per second)\n",
      "llama_print_timings:       total time =   113.34 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='       ', start=127, stop=134, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    94.90 ms /     1 runs   (   94.90 ms per token,    10.54 tokens per second)\n",
      "llama_print_timings:       total time =    99.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=124, stop=126, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1416.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    99.93 ms /     1 runs   (   99.93 ms per token,    10.01 tokens per second)\n",
      "llama_print_timings:       total time =   104.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='str', start=128, stop=131, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1434.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   152.52 ms /     1 runs   (  152.52 ms per token,     6.56 tokens per second)\n",
      "llama_print_timings:       total time =   156.70 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='ength', start=135, stop=140, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   172.13 ms /     1 runs   (  172.13 ms per token,     5.81 tokens per second)\n",
      "llama_print_timings:       total time =   176.30 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\":', start=134, stop=136, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1459.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   107.17 ms /     1 runs   (  107.17 ms per token,     9.33 tokens per second)\n",
      "llama_print_timings:       total time =   111.21 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' ', start=134, stop=135, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1408.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    91.51 ms /     1 runs   (   91.51 ms per token,    10.93 tokens per second)\n",
      "llama_print_timings:       total time =    95.64 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='1', start=135, stop=136, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.68 ms /     1 runs   (    0.68 ms per token,  1461.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   105.70 ms /     1 runs   (  105.70 ms per token,     9.46 tokens per second)\n",
      "llama_print_timings:       total time =   109.88 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='5', start=136, stop=137, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.72 ms /     1 runs   (    0.72 ms per token,  1396.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   137.05 ms /     1 runs   (  137.05 ms per token,     7.30 tokens per second)\n",
      "llama_print_timings:       total time =   141.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='0', start=137, stop=138, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   146.11 ms /     1 runs   (  146.11 ms per token,     6.84 tokens per second)\n",
      "llama_print_timings:       total time =   150.30 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=',', start=138, stop=139, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   105.18 ms /     1 runs   (  105.18 ms per token,     9.51 tokens per second)\n",
      "llama_print_timings:       total time =   109.33 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=139, stop=140, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1416.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    99.84 ms /     1 runs   (   99.84 ms per token,    10.02 tokens per second)\n",
      "llama_print_timings:       total time =   104.00 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='       ', start=152, stop=159, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1445.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   123.88 ms /     1 runs   (  123.88 ms per token,     8.07 tokens per second)\n",
      "llama_print_timings:       total time =   128.06 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=149, stop=151, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    97.40 ms /     1 runs   (   97.40 ms per token,    10.27 tokens per second)\n",
      "llama_print_timings:       total time =   101.63 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='speed', start=157, stop=162, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    87.07 ms /     1 runs   (   87.07 ms per token,    11.48 tokens per second)\n",
      "llama_print_timings:       total time =    91.32 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\":', start=156, stop=158, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1434.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    81.77 ms /     1 runs   (   81.77 ms per token,    12.23 tokens per second)\n",
      "llama_print_timings:       total time =    85.96 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' ', start=156, stop=157, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    81.49 ms /     1 runs   (   81.49 ms per token,    12.27 tokens per second)\n",
      "llama_print_timings:       total time =    85.66 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='9', start=157, stop=158, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    93.75 ms /     1 runs   (   93.75 ms per token,    10.67 tokens per second)\n",
      "llama_print_timings:       total time =    97.95 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='0', start=158, stop=159, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1440.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    76.86 ms /     1 runs   (   76.86 ms per token,    13.01 tokens per second)\n",
      "llama_print_timings:       total time =    80.99 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=',', start=159, stop=160, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.85 ms /     1 runs   (   82.85 ms per token,    12.07 tokens per second)\n",
      "llama_print_timings:       total time =    87.01 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=160, stop=161, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   104.31 ms /     1 runs   (  104.31 ms per token,     9.59 tokens per second)\n",
      "llama_print_timings:       total time =   108.52 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='       ', start=173, stop=180, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   103.13 ms /     1 runs   (  103.13 ms per token,     9.70 tokens per second)\n",
      "llama_print_timings:       total time =   107.31 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=170, stop=172, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   145.42 ms /     1 runs   (  145.42 ms per token,     6.88 tokens per second)\n",
      "llama_print_timings:       total time =   149.66 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='fl', start=172, stop=174, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1416.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    93.92 ms /     1 runs   (   93.92 ms per token,    10.65 tokens per second)\n",
      "llama_print_timings:       total time =    98.23 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='ight', start=178, stop=182, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    95.12 ms /     1 runs   (   95.12 ms per token,    10.51 tokens per second)\n",
      "llama_print_timings:       total time =    99.38 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\":', start=178, stop=180, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    84.62 ms /     1 runs   (   84.62 ms per token,    11.82 tokens per second)\n",
      "llama_print_timings:       total time =    88.86 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' true', start=186, stop=191, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1445.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   117.01 ms /     1 runs   (  117.01 ms per token,     8.55 tokens per second)\n",
      "llama_print_timings:       total time =   121.23 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=183, stop=184, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1398.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    84.29 ms /     1 runs   (   84.29 ms per token,    11.86 tokens per second)\n",
      "llama_print_timings:       total time =    88.70 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='   ', start=188, stop=191, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    83.68 ms /     1 runs   (   83.68 ms per token,    11.95 tokens per second)\n",
      "llama_print_timings:       total time =    87.92 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' },', start=191, stop=194, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1434.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    85.89 ms /     1 runs   (   85.89 ms per token,    11.64 tokens per second)\n",
      "llama_print_timings:       total time =    90.20 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=190, stop=191, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1434.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    90.81 ms /     1 runs   (   90.81 ms per token,    11.01 tokens per second)\n",
      "llama_print_timings:       total time =    95.07 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='   ', start=195, stop=198, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1436.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   132.42 ms /     1 runs   (  132.42 ms per token,     7.55 tokens per second)\n",
      "llama_print_timings:       total time =   136.71 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=196, stop=198, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    87.05 ms /     1 runs   (   87.05 ms per token,    11.49 tokens per second)\n",
      "llama_print_timings:       total time =    91.30 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='weak', start=202, stop=206, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.73 ms /     1 runs   (    0.73 ms per token,  1369.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    89.81 ms /     1 runs   (   89.81 ms per token,    11.13 tokens per second)\n",
      "llama_print_timings:       total time =    94.05 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='ness', start=206, stop=210, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    79.54 ms /     1 runs   (   79.54 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:       total time =    83.75 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='es', start=206, stop=208, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    88.10 ms /     1 runs   (   88.10 ms per token,    11.35 tokens per second)\n",
      "llama_print_timings:       total time =    92.41 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\":', start=208, stop=210, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    99.46 ms /     1 runs   (   99.46 ms per token,    10.05 tokens per second)\n",
      "llama_print_timings:       total time =   103.73 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' [', start=210, stop=212, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.74 ms /     1 runs   (    0.74 ms per token,  1356.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   210.55 ms /     1 runs   (  210.55 ms per token,     4.75 tokens per second)\n",
      "llama_print_timings:       total time =   215.37 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\"', start=210, stop=211, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1434.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    82.75 ms /     1 runs   (   82.75 ms per token,    12.08 tokens per second)\n",
      "llama_print_timings:       total time =    87.50 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='fire', start=217, stop=221, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1436.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    90.63 ms /     1 runs   (   90.63 ms per token,    11.03 tokens per second)\n",
      "llama_print_timings:       total time =    96.09 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\",', start=217, stop=219, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1416.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    83.78 ms /     1 runs   (   83.78 ms per token,    11.94 tokens per second)\n",
      "llama_print_timings:       total time =    87.97 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=219, stop=221, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1420.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    80.14 ms /     1 runs   (   80.14 ms per token,    12.48 tokens per second)\n",
      "llama_print_timings:       total time =    84.92 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='po', start=221, stop=223, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1438.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    84.40 ms /     1 runs   (   84.40 ms per token,    11.85 tokens per second)\n",
      "llama_print_timings:       total time =    89.73 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='ison', start=227, stop=231, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    86.86 ms /     1 runs   (   86.86 ms per token,    11.51 tokens per second)\n",
      "llama_print_timings:       total time =    92.32 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\",', start=227, stop=229, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1434.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    80.13 ms /     1 runs   (   80.13 ms per token,    12.48 tokens per second)\n",
      "llama_print_timings:       total time =    84.42 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' \"', start=229, stop=231, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1438.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    81.22 ms /     1 runs   (   81.22 ms per token,    12.31 tokens per second)\n",
      "llama_print_timings:       total time =    86.03 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='p', start=229, stop=230, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    78.37 ms /     1 runs   (   78.37 ms per token,    12.76 tokens per second)\n",
      "llama_print_timings:       total time =    83.86 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='un', start=232, stop=234, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1428.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    87.71 ms /     1 runs   (   87.71 ms per token,    11.40 tokens per second)\n",
      "llama_print_timings:       total time =    93.10 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='y', start=232, stop=233, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1418.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    97.61 ms /     1 runs   (   97.61 ms per token,    10.24 tokens per second)\n",
      "llama_print_timings:       total time =   103.07 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text=' humans', start=245, stop=252, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.71 ms /     1 runs   (    0.71 ms per token,  1416.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    98.64 ms /     1 runs   (   98.64 ms per token,    10.14 tokens per second)\n",
      "llama_print_timings:       total time =   104.04 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\"]', start=242, stop=244, name=None, chunk=True, score=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.70 ms /     1 runs   (    0.70 ms per token,  1426.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    79.79 ms /     1 runs   (   79.79 ms per token,    12.53 tokens per second)\n",
      "llama_print_timings:       total time =    84.23 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion(text='\n",
      "', start=242, stop=243, name=None, chunk=True, score=0.0)\n",
      "Completion(text='}', start=243, stop=244, name=None, chunk=True, score=0.0)\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12535.43 ms\n",
      "llama_print_timings:      sample time =     0.69 ms /     1 runs   (    0.69 ms per token,  1445.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   115.16 ms /     1 runs   (  115.16 ms per token,     8.68 tokens per second)\n",
      "llama_print_timings:       total time =   119.33 ms\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"regular_prompt = await regular_prompt.complete(llama_model, constraint=superhero_constraint, stream=print_stream)\";\n",
       "                var nbb_formatted_code = \"regular_prompt = await regular_prompt.complete(\\n    llama_model, constraint=superhero_constraint, stream=print_stream\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regular_prompt = await regular_prompt.complete(\n",
    "    llama_model, constraint=superhero_constraint, stream=print_stream\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd29f578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompt('System: \n",
       "You are an expert in designing novel superheroes.\n",
       "\n",
       "User: Give me a json design of a superhero similar to this:\n",
       "{\n",
       "    \"name\": \"Superman\",\n",
       "    \"aliases\": [\"Clark Kent\", \"Man of Steel\"],\n",
       "    \"powers\": {\n",
       "        \"strength\": 100,\n",
       "        \"speed\": 75,\n",
       "        \"flight\": true\n",
       "    },\n",
       "    \"weaknesses\": [\"kryptonite\", \"magic\"]\n",
       "}\n",
       "\n",
       "I want a superhero that has something to do with pandas. \n",
       "I want his name to be pandaman. He is stronger and faster than superman. \n",
       "Give him 3 super cool aliases too!\n",
       "something like {\"name\": \"pandaman\", \"aliases\":...}\n",
       "\n",
       "\n",
       "Generate me a json for pandaman.\n",
       "\n",
       "Assistant: Here is a JSON design for that:\n",
       "{\n",
       "    \"name\": \"Pandaman\",\n",
       "    \"aliases\": [\"Panda Power\", \"Mister Pawsitive\", \"The Great Panda\"],\n",
       "    \"powers\": {\n",
       "        \"strength\": 150,\n",
       "        \"speed\": 90,\n",
       "        \"flight\": true\n",
       "    },\n",
       "    \"weaknesses\": [\"fire\", \"poison\", \"puny humans\"]\n",
       "}')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"regular_prompt\";\n",
       "                var nbb_formatted_code = \"regular_prompt\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regular_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff04bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
